<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;aaaaaaron.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;}}</script>
<meta property="og:type" content="website">
<meta property="og:title" content="Jiatao Tao&#39;s blog">
<meta property="og:url" content="https://aaaaaaron.github.io/index.html">
<meta property="og:site_name" content="Jiatao Tao&#39;s blog">
<meta property="og:locale">
<meta property="article:author" content="陶加涛">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://aaaaaaron.github.io/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:true,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;zh-Hans&quot;,&quot;comments&quot;:&quot;&quot;,&quot;permalink&quot;:&quot;&quot;,&quot;path&quot;:&quot;index.html&quot;,&quot;title&quot;:&quot;&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Jiatao Tao's blog</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Jiatao Tao's blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">λ</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">陶加涛</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2021/05/01/SQL-CTE-optimize/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/05/01/SQL-CTE-optimize/" class="post-title-link" itemprop="url">SQL CTE 优化</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-05-01 20:49:53" itemprop="dateCreated datePublished" datetime="2021-05-01T20:49:53+08:00">2021-05-01</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 00:37:24" itemprop="dateModified" datetime="2021-05-08T00:37:24+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p><strong>CTE</strong>: Common Table Expressions, 也就是我们常见的 With clause.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> v <span class="keyword">AS</span> (<span class="keyword">SELECT</span> i<span class="operator">-</span>brand, i_current_price, <span class="built_in">MAX</span> (i_units) m</span><br><span class="line">           <span class="keyword">FROM</span> item <span class="keyword">WHERE</span> i_color <span class="operator">=</span> <span class="string">&#x27;red&#x27;</span></span><br><span class="line">           <span class="keyword">GROUP</span> <span class="keyword">BY</span> ibrand, i_current_price)</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> v <span class="keyword">WHERE</span> m <span class="operator">&lt;</span> <span class="number">100</span></span><br></pre></td></tr></table></figure>

<p><strong>CTE 优点:</strong></p>
<ol>
<li>简化查询, 使其更可读</li>
<li>性能提升(本文主要介绍非递归 CTE 带来的性能提升)</li>
</ol>
<h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><h3 id="解决死锁"><a href="#解决死锁" class="headerlink" title="解决死锁"></a>解决死锁</h3><ul>
<li>复杂的查询可能会有嵌套的 CTE, 优化器需要保证没有同时多个 process 在等对方结束</li>
</ul>
<h3 id="如何枚举-选取可能的计划"><a href="#如何枚举-选取可能的计划" class="headerlink" title="如何枚举/选取可能的计划"></a>如何枚举/选取可能的计划</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL2</span></span><br><span class="line"><span class="comment">-- index on i_color</span></span><br><span class="line"><span class="keyword">WITH</span> v <span class="keyword">AS</span> (<span class="keyword">SELECT</span> i_brand, i_color <span class="keyword">FROM</span> item </span><br><span class="line">           <span class="keyword">WHERE</span> i_CURRENT_price <span class="operator">&lt;</span> <span class="number">1000</span>)</span><br><span class="line"><span class="keyword">SELECT</span> v1.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> y vl, v v2, v v3</span><br><span class="line"><span class="keyword">WHERE</span> vl.i_brand <span class="operator">=</span> v2.i_brand</span><br><span class="line">  <span class="keyword">AND</span> v2.i_brand <span class="operator">=</span> v3.i_brand</span><br><span class="line">  <span class="keyword">AND</span> v3.i_color <span class="operator">=</span> <span class="string">&#x27;red&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>对于 <strong>SQL2</strong>, 有几种 CTE 展开的形式</p>
<p>a. 不展开: CTE 只执行一次, 但是没有用到 i_color 上的 index</p>
<p>b. 全部展开: 可以使用 i_color 上的 index, 但是展开的部分重复计算了三次</p>
<p>c. 部分展开</p>
<p><img src="/2021/05/01/SQL-CTE-optimize/image-20210426183446419-20210507210541900.png" alt="image-20210426183446419"></p>
<h3 id="如何根据上下文优化"><a href="#如何根据上下文优化" class="headerlink" title="如何根据上下文优化"></a>如何根据上下文优化</h3><ol>
<li>如果查询对所有的 CTE 都有 filter, 可以直接把 filter 下推到 CTE 的定义上</li>
<li>分布式计划中的物理属性的 enforce, 减少对同一份数据的re-partition/re-ordering</li>
</ol>
<h1 id="优化流程"><a href="#优化流程" class="headerlink" title="优化流程"></a>优化流程</h1><h2 id="名词定义"><a href="#名词定义" class="headerlink" title="名词定义"></a>名词定义</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL3</span></span><br><span class="line"><span class="keyword">WITH</span> v <span class="keyword">AS</span> (<span class="keyword">SELECT</span> i_brand <span class="keyword">FROM</span> item <span class="keyword">WHERE</span> i_color <span class="operator">=</span> ’red’) </span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> v <span class="keyword">as</span> v1, v <span class="keyword">as</span> v2 <span class="keyword">WHERE</span> v1.i_brand <span class="operator">=</span> v2.i_brand;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/05/01/SQL-CTE-optimize/image-20210426205405612-20210507210542053.png" alt="image-20210426205405612"></p>
<ol>
<li><strong>CTEProducer</strong>: CTE 定义的树的根节点, 有唯一的 ID.</li>
<li><strong>CTEConsumer</strong>: query 里用到 CTE 的地方, ID 与其对应的 CTEProducer 一致.</li>
<li><strong>CTEAnchor</strong>: 定义了 CTE 的作用域, 一个 CTE 仅可以在被其相对应的 CTEAnchor 下被引用.</li>
<li><strong>Sequence</strong>: 二元操作符, 按顺序执行其 children (从左到右), 然后返回其 right child.</li>
</ol>
<p>图三展示了 SQL3 的两种可能的 plan.</p>
<ul>
<li>a. 所有 CTEs 都被内联了, CTEAnchor 被去除, 所有的 CTEConsumer 都被替换成 CTE 展开的定义</li>
<li>b. 无内联, CTEAnchor 被替换成 Sequence, CTEProducer 作为其 left child, CTEAnchor 的 child 作为其 right child<ul>
<li><strong>Sequence</strong> 保证了特定的执行顺序, 可以保证 CTEProducer 在 CTEConsumer 之前执行. 这个机制可以保证生成的计划没有死锁</li>
</ul>
</li>
</ul>
<p><img src="/2021/05/01/SQL-CTE-optimize/image-20210427111008104-20210507210542029.png" alt="image-20210427111008104"></p>
<h3 id="嵌套-CTE-例子"><a href="#嵌套-CTE-例子" class="headerlink" title="嵌套 CTE 例子"></a>嵌套 CTE 例子</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL4</span></span><br><span class="line"><span class="keyword">WITH</span> v <span class="keyword">AS</span> (<span class="keyword">SELECT</span> i_current_price <span class="keyword">FROM</span> item <span class="keyword">WHERE</span> i_color <span class="operator">=</span> ’red’),</span><br><span class="line">     w <span class="keyword">AS</span> (<span class="keyword">SELECT</span> v1.p <span class="keyword">FROM</span> v <span class="keyword">AS</span> v1, v <span class="keyword">AS</span> v2 <span class="keyword">WHERE</span> v1.p <span class="operator">&lt;</span> v2.p)</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> v <span class="keyword">AS</span> v3, w <span class="keyword">AS</span> w1, w <span class="keyword">AS</span> w2</span><br><span class="line"><span class="keyword">WHERE</span> v3.p <span class="operator">&lt;</span> w1.p <span class="operator">+</span> w2.p;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/05/01/SQL-CTE-optimize/image-20210427142643234-20210507210542091.png" alt="image-20210427142643234"></p>
<p>注意, 图 b 有两个 CTEAnchor, 其顺序和在 with 定义里的一致</p>
<h2 id="PLAN-ENUMERATION"><a href="#PLAN-ENUMERATION" class="headerlink" title="PLAN ENUMERATION"></a>PLAN ENUMERATION</h2><h3 id="Memo-介绍"><a href="#Memo-介绍" class="headerlink" title="Memo 介绍"></a>Memo 介绍</h3><ul>
<li>MEMO 的定义：是一种数据结构，用于管理一个组，每个组代表一个查询计划的不同子目标。</li>
<li>MEMO 结构的目标：是通过尽可能的公用相同的子树使得内存的使用最小。</li>
<li>MEMO 的主要思想：通过使用共享的副本来避免子树的重复使用。</li>
</ul>
<p>Memo 中两个最基本的概念就是 Expression Group（简称 Group） 以及 Group Expression（对应关系代数算子）</p>
<ul>
<li>每个 Group 中保存的是逻辑等价的 Group Expression</li>
<li>Group Expression 的子节点是由 Group 组成</li>
</ul>
<h4 id="Init-Memo"><a href="#Init-Memo" class="headerlink" title="Init Memo"></a>Init Memo</h4><p><img src="/2021/05/01/SQL-CTE-optimize/image-20200208221706543-20210507210542129.png" alt="image-20200208221706543"></p>
<p>一旦最初的计划复制到了MEMO结构中以后，就可以对逻辑操作符做一些转换以生成物理操作符。<br>一个转换规则可以生成：</p>
<ol>
<li>同一组中的一个逻辑操作符: 如 join( A, B) -&gt; join( B, A)</li>
<li>同一组中的一个物理操作符: 如 join -&gt; Hash Join</li>
</ol>
<h4 id="Apply-transformation-implement-rule"><a href="#Apply-transformation-implement-rule" class="headerlink" title="Apply transformation/implement rule"></a>Apply transformation/implement rule</h4><p><img src="/2021/05/01/SQL-CTE-optimize/image-20200208221734549-20210507210542169.png" alt="image-20200208221734549"></p>
<p>一组逻辑操作符组成一个子计划。根仍保留在原来的组中，而其他操作符分配到其他的组中，必要的时候可以建立新组，如 join( A, join(B,C)) -&gt; join( join(A,B), C), 这两个最外面的 Join 是等价的, 所以是同一个根节点, 但是前后两次里面的 join 不一样, 所以在不同的组</p>
<h4 id="Find-best-plan"><a href="#Find-best-plan" class="headerlink" title="Find best plan"></a>Find best plan</h4><p><img src="/2021/05/01/SQL-CTE-optimize/image-20200208221741219-20210507210542188.png" alt="image-20200208221741219"></p>
<h3 id="CTE-Transformation"><a href="#CTE-Transformation" class="headerlink" title="CTE Transformation"></a>CTE Transformation</h3><p>使用 Memo 代表不同的候选者使得这个过程是 CBO 的, 我们可以在一条查询中内联某些 CTE, 其他的不内联.</p>
<p><img src="/2021/05/01/SQL-CTE-optimize/image-20210427144511944-20210507210542294.png" alt="image-20210427144511944"></p>
<ol>
<li>首先生成一个初始的 memo (F.6)</li>
<li>第一条 rule 应用在 CTEAnchor 上, 生成一个 Sequence 节点(group 0), 同时把 CTEProducer 作为其 left child 展开, 生成了 group 4/5/6, 其 right child 是CTEAnchor 的 child</li>
<li>第二条 rule 还是应用在 CTEAnchor, 生成一个 NoOp 算子, child 是 CTEAnchor 的 child</li>
<li>第三条 rule 应用在 CTEConsumer 上, 生成一个展开后的 CTE 定义, 添加到 CTEConsumer 的同一个 group 中</li>
</ol>
<p>F.8 展示了一些可能的 plan, 这些 plan 都有一些问题:</p>
<ul>
<li>8(a)/8(b) 是非法的, 他们只有 CTEConsumer, 却没有 CTEProducer, 所以 CTEConsumer 永远也读不到他们想要的数据. 我们会避免产生这样的计划</li>
<li>8(c)/8(d) 的计划明显不是最优的, c 展开了所有的 CTE, 但是保留了 CTEProducer, 同理 d 也是.</li>
</ul>
<p><img src="/2021/05/01/SQL-CTE-optimize/image-20210427145718543-20210507210542229.png" alt="image-20210427145718543"></p>
<h3 id="Predicate-Pushdown"><a href="#Predicate-Pushdown" class="headerlink" title="Predicate Pushdown"></a>Predicate Pushdown</h3><p>在 Orca, 我们引入了一种方法, 可以不展开 CTE 也可以做谓词下推.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL5</span></span><br><span class="line"><span class="keyword">WITH</span> v <span class="keyword">AS</span> (<span class="keyword">SELECT</span> i_brand, i_color <span class="keyword">FROM</span> item <span class="keyword">WHERE</span> i_CURRENT price <span class="operator">&lt;</span> <span class="number">50</span>)</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> v v1, v v2</span><br><span class="line"><span class="keyword">WHERE</span> v1.i_brand <span class="operator">=</span> v2.i_brand</span><br><span class="line">  <span class="keyword">AND</span> v1.i_color <span class="operator">=</span> ’red’</span><br><span class="line">  <span class="keyword">AND</span> v2.i_color <span class="operator">=</span> ’blue’;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/05/01/SQL-CTE-optimize/image-20210427153800222-20210507210542245.png" alt="image-20210427153800222"></p>
<h3 id="Always-Inlining-Single-use-CTEs"><a href="#Always-Inlining-Single-use-CTEs" class="headerlink" title="Always Inlining Single-use CTEs"></a>Always Inlining Single-use CTEs</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL6</span></span><br><span class="line"><span class="keyword">WITH</span> v <span class="keyword">AS</span> (<span class="keyword">SELECT</span> i_color <span class="keyword">FROM</span> item <span class="keyword">WHERE</span> i_current_price <span class="operator">&lt;</span> <span class="number">50</span>)</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> v <span class="keyword">WHERE</span> v.i_color <span class="operator">=</span> ’red’;</span><br></pre></td></tr></table></figure>

<p>由于这类 CTE 只被使用了一次, 所以可以无脑 inline</p>
<h3 id="Elimination-of-unused-CTEs"><a href="#Elimination-of-unused-CTEs" class="headerlink" title="Elimination of unused CTEs"></a>Elimination of unused CTEs</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> v <span class="keyword">AS</span> (<span class="keyword">SELECT</span> i_color <span class="keyword">FROM</span> item <span class="keyword">WHERE</span> i_current_price <span class="operator">&lt;</span> <span class="number">50</span>)</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> item <span class="keyword">WHERE</span> item.i_color <span class="operator">=</span> <span class="string">&#x27;red&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>当 CTE 没有被使用到的时候, 直接消除掉. 当 CTE 有嵌套定义时, 可以先消除最外层定义的 CTE, 外层的 CTE 消除之后, 内层的 CTE 也无引用, 也可以消除.</p>
<h2 id="CONTEXTUALIZED-OPTIMIZATION"><a href="#CONTEXTUALIZED-OPTIMIZATION" class="headerlink" title="CONTEXTUALIZED OPTIMIZATION"></a>CONTEXTUALIZED OPTIMIZATION</h2><h3 id="Enforcing-Physical-Properties"><a href="#Enforcing-Physical-Properties" class="headerlink" title="Enforcing Physical Properties"></a>Enforcing Physical Properties</h3><p><img src="/2021/05/01/SQL-CTE-optimize/image-20210427161134653-20210507210542580.png" alt="image-20210427161134653"></p>
<h3 id="Producer-Context"><a href="#Producer-Context" class="headerlink" title="Producer Context"></a>Producer Context</h3><p>CTEProducer 侧的计划生成比较独立, 可以不考虑 Consumer 侧. 如图 11(a), CTEProducer 侧并不需要任何分布, 所以没有添加任何 distribution. 但是 Consumer 测有个 hash join, 所以需要其底下的算子提供 HashDistribution, 但这不是最优的计划.</p>
<h3 id="Consumer-Context"><a href="#Consumer-Context" class="headerlink" title="Consumer Context"></a>Consumer Context</h3><p>先从 Consumer 侧来考虑数据分布, 可以产生的更优的计划. 如图 11(b). 这种方式把 distribution 下推到了 CTE 中, 这样在 consumer 测就不用做 distribution.</p>
<h2 id="CTE-BASED-OPTIMIZATIONS"><a href="#CTE-BASED-OPTIMIZATIONS" class="headerlink" title="CTE-BASED OPTIMIZATIONS"></a>CTE-BASED OPTIMIZATIONS</h2><h3 id="CTE-Generating-Transformations"><a href="#CTE-Generating-Transformations" class="headerlink" title="CTE-Generating Transformations"></a>CTE-Generating Transformations</h3><p>在以下场景 window function, full outer joins, distinct aggregates , Orca 会自动生成 CTE.</p>
<p>例如下面的 SQL9, distinct 了两个不同的列, 可以把 P-F-T 的部分提成一个 CTE, 减少读取的 input.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL9</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cs_item_sk), <span class="built_in">AVG</span>(<span class="keyword">DISTINCT</span> cs_qty)</span><br><span class="line"><span class="keyword">FROM</span> CATALOG sales <span class="keyword">WHERE</span> cs_net profit <span class="operator">&gt;</span> <span class="number">1000</span></span><br></pre></td></tr></table></figure>

<p><img src="/2021/05/01/SQL-CTE-optimize/image-20210427200049532-20210507210542308.png" alt="image-20210427200049532"></p>
<h3 id="Common-Subexpression-Elimination"><a href="#Common-Subexpression-Elimination" class="headerlink" title="Common Subexpression Elimination"></a>Common Subexpression Elimination</h3><p>Orca可以提取公共子表达式为 CTE.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL10</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span></span><br><span class="line">  (<span class="keyword">SELECT</span> i_brand, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">AS</span> b <span class="keyword">FROM</span> item <span class="keyword">GROUP</span> <span class="keyword">BY</span> i_brand <span class="keyword">HAVING</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="operator">&gt;</span> <span class="number">10</span>) t1,</span><br><span class="line">  (<span class="keyword">SELECT</span> i_brand, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">AS</span> b <span class="keyword">FROM</span> item <span class="keyword">GROUP</span> <span class="keyword">BY</span> i_brand <span class="keyword">HAVING</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="operator">&gt;</span> <span class="number">20</span>) t2</span><br><span class="line"><span class="keyword">WHERE</span> t1.i_brand <span class="operator">&lt;&gt;</span> t2.i_brand;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/05/01/SQL-CTE-optimize/image-20210427202305622-20210507210542337.png" alt="image-20210427202305622"></p>
<p>具体算法如下:</p>
<ul>
<li>输入: 原始表达式</li>
<li>输出: common subexpression 已经被替换过的表达式</li>
<li>DetectMatches() 用于找出公共的表达式, 论文中又引用了其他两篇论文<ul>
<li>Exploiting Common Subexpressions for Cloud Query Processing.</li>
<li>Efﬁcient Exploitation of Similar Subexpressions for Query Processing.</li>
</ul>
</li>
<li>InsertCTEConsumers() 把表达式中的公共子表达式替换为对应的 CTEConsumer</li>
<li>最后, 在每组公共子表达式的 LCA(最近公共祖先) 处插入一个CTEAnchor</li>
</ul>
<p><img src="/2021/05/01/SQL-CTE-optimize/image-20210427202537374-20210507210542586.png" alt="image-20210427202537374"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2021/04/25/Calcite-Not-in-opt/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/04/25/Calcite-Not-in-opt/" class="post-title-link" itemprop="url">Calcite Not in 转 cross join</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-04-25 20:49:53" itemprop="dateCreated datePublished" datetime="2021-04-25T20:49:53+08:00">2021-04-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 00:40:06" itemprop="dateModified" datetime="2021-05-08T00:40:06+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>SQL:</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> users</span><br><span class="line"><span class="keyword">WHERE</span> age <span class="keyword">NOT</span> <span class="keyword">IN</span></span><br><span class="line">    ( <span class="keyword">SELECT</span> id</span><br><span class="line">     <span class="keyword">FROM</span> jobs )</span><br></pre></td></tr></table></figure>

<p><strong>Plan:</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">EnumerableProject(ID<span class="operator">=</span>[$<span class="number">0</span>], NAME<span class="operator">=</span>[$<span class="number">1</span>], AGE<span class="operator">=</span>[$<span class="number">2</span>])</span><br><span class="line">  EnumerableFilter(<span class="keyword">condition</span><span class="operator">=</span>[<span class="keyword">OR</span>(<span class="operator">=</span>($<span class="number">3</span>, <span class="number">0</span>), <span class="keyword">AND</span>(<span class="keyword">IS</span> <span class="keyword">NULL</span>($<span class="number">5</span>), <span class="operator">&gt;=</span>($<span class="number">4</span>, $<span class="number">3</span>), <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>($<span class="number">2</span>)))])</span><br><span class="line">    EnumerableProject(ID<span class="operator">=</span>[$<span class="number">0</span>], NAME<span class="operator">=</span>[$<span class="number">1</span>], AGE<span class="operator">=</span>[$<span class="number">2</span>], c<span class="operator">=</span>[$<span class="number">3</span>], ck<span class="operator">=</span>[$<span class="number">4</span>], i<span class="operator">=</span>[$<span class="number">6</span>])</span><br><span class="line">      EnumerableHashJoin(<span class="keyword">condition</span><span class="operator">=</span>[<span class="operator">=</span>($<span class="number">2</span>, $<span class="number">5</span>)], joinType<span class="operator">=</span>[<span class="keyword">left</span>])</span><br><span class="line">        EnumerableNestedLoopJoin(<span class="keyword">condition</span><span class="operator">=</span>[<span class="literal">true</span>], joinType<span class="operator">=</span>[<span class="keyword">inner</span>])</span><br><span class="line">          EnumerableTableScan(<span class="keyword">table</span><span class="operator">=</span>[users])</span><br><span class="line">          EnumerableHashAggregate(<span class="keyword">group</span><span class="operator">=</span>[&#123;&#125;], c<span class="operator">=</span>[<span class="built_in">COUNT</span>()], ck<span class="operator">=</span>[<span class="built_in">COUNT</span>($<span class="number">0</span>)])</span><br><span class="line">            EnumerableTableScan(<span class="keyword">table</span><span class="operator">=</span>[jobs])</span><br><span class="line">        EnumerableProject(id<span class="operator">=</span>[$<span class="number">0</span>], i<span class="operator">=</span>[<span class="literal">true</span>])</span><br><span class="line">          EnumerableHashAggregate(<span class="keyword">group</span><span class="operator">=</span>[&#123;<span class="number">0</span>&#125;])</span><br><span class="line">            EnumerableTableScan(<span class="keyword">table</span><span class="operator">=</span>[jobs])</span><br></pre></td></tr></table></figure>

<p><strong>Plan 转回 SQL</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> ID,</span><br><span class="line">       NAME,</span><br><span class="line">       AGE</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (<span class="keyword">SELECT</span> users.ID,</span><br><span class="line">          users.NAME,</span><br><span class="line">          users.AGE</span><br><span class="line">          t.c,</span><br><span class="line">          t.ck,</span><br><span class="line">          t1.i</span><br><span class="line">   <span class="keyword">FROM</span> default.users</span><br><span class="line">   <span class="keyword">CROSS</span> <span class="keyword">JOIN</span></span><br><span class="line">     (<span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="number">1</span>) <span class="keyword">AS</span> c,</span><br><span class="line">             <span class="built_in">COUNT</span>(ID) <span class="keyword">AS</span> ck</span><br><span class="line">      <span class="keyword">FROM</span> default.jobs) <span class="keyword">AS</span> t</span><br><span class="line">   <span class="keyword">LEFT</span> <span class="keyword">JOIN</span></span><br><span class="line">     (<span class="keyword">SELECT</span> ID <span class="keyword">AS</span> id,</span><br><span class="line">             <span class="literal">TRUE</span> <span class="keyword">AS</span> i</span><br><span class="line">      <span class="keyword">FROM</span> default.jobs</span><br><span class="line">      <span class="keyword">GROUP</span> <span class="keyword">BY</span> ID) <span class="keyword">AS</span> t1 <span class="keyword">ON</span> users.AGE <span class="operator">=</span> t1.id) <span class="keyword">AS</span> t2</span><br><span class="line"><span class="keyword">WHERE</span> t2.c <span class="operator">=</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">OR</span> t2.i <span class="keyword">IS</span> <span class="keyword">NULL</span></span><br><span class="line">  <span class="keyword">AND</span> (t2.ck <span class="operator">&gt;=</span> t2.c</span><br><span class="line">       <span class="keyword">AND</span> t2.AGE <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>)</span><br></pre></td></tr></table></figure>

<p>NOT IN 转换成了两个 join, 一个是 cross join, 另一个是 left join, 下面对这俩 join 进行分析</p>
<h3 id="cross-join"><a href="#cross-join" class="headerlink" title="cross join"></a>cross join</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">EnumerableNestedLoopJoin(<span class="keyword">condition</span><span class="operator">=</span>[<span class="literal">true</span>], joinType<span class="operator">=</span>[<span class="keyword">inner</span>])</span><br><span class="line">  EnumerableTableScan(<span class="keyword">table</span><span class="operator">=</span>[users])</span><br><span class="line">  EnumerableHashAggregate(<span class="keyword">group</span><span class="operator">=</span>[&#123;&#125;], c<span class="operator">=</span>[<span class="built_in">COUNT</span>()], ck<span class="operator">=</span>[<span class="built_in">COUNT</span>($<span class="number">0</span>)])</span><br><span class="line">    EnumerableTableScan(<span class="keyword">table</span><span class="operator">=</span>[jobs])</span><br></pre></td></tr></table></figure>

<p>这里先对 jobs 求了 count(*) 和 count(id), 如果 id 没有 null 值, c 和 ck 的值应该是一样的, 注意这个语句只会出来一行数据, 然后这个 cross join 相当于users 拼上了两列(c, ck)</p>
<h3 id="left-join"><a href="#left-join" class="headerlink" title="left join"></a>left join</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">EnumerableHashJoin(<span class="keyword">condition</span><span class="operator">=</span>[<span class="operator">=</span>($<span class="number">2</span>, $<span class="number">5</span>)], joinType<span class="operator">=</span>[<span class="keyword">left</span>])</span><br><span class="line">  EnumerableNestedLoopJoin(<span class="keyword">condition</span><span class="operator">=</span>[<span class="literal">true</span>], joinType<span class="operator">=</span>[<span class="keyword">inner</span>])</span><br><span class="line">  EnumerableProject(id<span class="operator">=</span>[$<span class="number">0</span>], i<span class="operator">=</span>[<span class="literal">true</span>])</span><br><span class="line">    EnumerableHashAggregate(<span class="keyword">group</span><span class="operator">=</span>[&#123;<span class="number">0</span>&#125;])</span><br><span class="line">      EnumerableTableScan(<span class="keyword">table</span><span class="operator">=</span>[jobs])</span><br></pre></td></tr></table></figure>

<p>把上面的 cross join 作为输入, 并且再次 join 了一次 jobs 表(on age = id), 注意, 这里是把 age 和 id 相等的值 join 了起来, 由于是 left join, 右表无匹配则为 null</p>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>小知识点: AND 级别高于 OR, 并不是左结合</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> t2.c <span class="operator">=</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">OR</span> t2.i <span class="keyword">IS</span> <span class="keyword">NULL</span></span><br><span class="line">  <span class="keyword">AND</span> (t2.ck <span class="operator">&gt;=</span> t2.c</span><br><span class="line">       <span class="keyword">AND</span> t2.AGE <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>)</span><br></pre></td></tr></table></figure>

<p>最后看下这个 filter, 这个 filter 比较复杂, 还是直接举例把:</p>
<p>原表:</p>
<p><img src="/2021/04/25/Calcite-Not-in-opt/image-20210414163537859.png" alt="image-20210414163537859"></p>
<p>经过两轮 join 之后的表(USERS.ID=JOBS.AGE):</p>
<p><img src="/2021/04/25/Calcite-Not-in-opt/image-20210414171920321.png" alt="image-20210414171920321"></p>
<hr>
<p>首先来断下句: <code>t2.c = 0 OR (t2.i IS NULL AND (t2.ck &gt;= t2.c AND t2.AGE IS NOT NULL))</code></p>
<ol>
<li><code>t2.c = 0</code>:当 JOBS 表为空时的条件, 直接把 USERS 表所有数据选出即可.</li>
<li><code>t2.i IS NULL AND (t2.ck &gt;= t2.c AND t2.AGE IS NOT NULL)</code></li>
</ol>
<ul>
<li><code>t2.i IS NULL</code>: 是当 JOBS 表不为空, i 为 null 代表没有 join 上的值, 符合 not in 语义, 上面例子选出这俩条</li>
</ul>
<table>
<thead>
<tr>
<th align="left">ID</th>
<th align="left">NAME</th>
<th align="left">AGE</th>
<th align="left">JOBS.ID</th>
<th align="left">C</th>
<th align="left">CK</th>
<th align="left">I</th>
</tr>
</thead>
<tbody><tr>
<td align="left">NULL</td>
<td align="left">NULL</td>
<td align="left">NULL</td>
<td align="left">NULL</td>
<td align="left">NULL</td>
<td align="left">NULL</td>
<td align="left">NULL</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">TOM</td>
<td align="left">1</td>
<td align="left">NULL</td>
<td align="left">NULL</td>
<td align="left">NULL</td>
<td align="left">NULL</td>
</tr>
</tbody></table>
<ul>
<li><code>t2.ck &gt;= t2.c</code>: 当 JOBS 表ID有 NULL 值时, 整个查询返回空. 举个例子更容易理解: <code>select &#39;true&#39; where 3 not in (1, 2, null)</code>, 这个返回是空, 直接看 not in 比较难理解, 可以改成=, &lt;&gt;来看: <code>select &#39;true&#39; where 3 &lt;&gt; 1 and 3 &lt;&gt; 2 and 3 &lt;&gt; null</code>, <code>3 &lt;&gt; null</code> 为UNKNOWN, 既不是 true 也不是 false, 所以这里没有数据返回.</li>
<li><code>t2.AGE IS NOT NULL</code>: 当 JOBS 表不为空, 需要剔除 USERS 表 age 为 NULL 的行, NULL not in tbl 无法出来值(当 tbl 表有数据时), 再剔除第一条为 null 的, 得到结果</li>
</ul>
<table>
<thead>
<tr>
<th align="left">ID</th>
<th align="left">NAME</th>
<th align="left">AGE</th>
<th align="left">JOBS.ID</th>
<th align="left">C</th>
<th align="left">CK</th>
<th align="left">I</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1</td>
<td align="left">TOM</td>
<td align="left">1</td>
<td align="left">NULL</td>
<td align="left">NULL</td>
<td align="left">NULL</td>
<td align="left">NULL</td>
</tr>
</tbody></table>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>为什么不直接把 <code>a.id not in(select id from b)</code> 转换成 <code>left join on a.id = b.id where b.id IS NULL</code>, 需要增加一个 cross join? 答案是当 a.id 存在 null 且 b 表有数据, 会把 null 这行多选出来. 如果再加上 <code>a.id IS NOT NULL</code>可行吗? 答案还是不行的, 因为当 b 表无数据, 需要把 a.id null 这行取出来(令人头疼).所以这里加了个 cross join 是非常巧妙的.</p>
<h3 id="p-s"><a href="#p-s" class="headerlink" title="p.s."></a>p.s.</h3><p>SQL 是个三值系统 TRUE/FALSE/UNKNOWN, 其中 NULL 和所有值比较都是 UNKNOWN, 所以你不能写 col = null, 即使 col 真的有 null 值, 也出不来true, 只会出来 UNKNOWN, 正确的 NULL 值判断是 IS NULL/IS NOT NULL</p>
<p>所以这里 xxx NOT IN 也不会把这条NULL值选出来, NULL NOT IN/IN 都不会选出任何值, 他们的结果既不为 TRUE, 也不为 FALSE.</p>
<p>可以看到 jobs 插入了一个 users 不存在的值, 选出的结果会把 null 这行排除掉</p>
<p><img src="/2021/04/25/Calcite-Not-in-opt/image-20210414165506690.png" alt="image-20210414165506690"></p>
<p>但是有一种例外, 当 A not in B, B表是个空表时, 可以选出 A 表所有数据, 可以看到 null 这行数据选了出来</p>
<p><img src="/2021/04/25/Calcite-Not-in-opt/image-20210414165201722.png" alt="image-20210414165201722"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2020/02/09/Calcite-Volcano-Planner/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/02/09/Calcite-Volcano-Planner/" class="post-title-link" itemprop="url">Calcite Volcano Planner</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-02-09 20:49:53" itemprop="dateCreated datePublished" datetime="2020-02-09T20:49:53+08:00">2020-02-09</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 00:38:40" itemprop="dateModified" datetime="2021-05-08T00:38:40+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="优化器模型介绍"><a href="#优化器模型介绍" class="headerlink" title="优化器模型介绍"></a>优化器模型介绍</h1><h2 id="Cascade-Volcano"><a href="#Cascade-Volcano" class="headerlink" title="Cascade/Volcano"></a>Cascade/Volcano</h2><p>Calcite Volcano Planner 的思想来自 Cascade/Volcano<br>采用自顶向下的动态规划算法（记忆化搜索）</p>
<p>Volcano Optimizer 将搜索分为两个阶段，在第一个阶段枚举所有逻辑等价的 Logical Algebra，而在第二阶段运用动态规划的方法自顶向下地搜索代价最小的 Physical Algebra</p>
<p>Cascades Optimizer 则将这两个阶段融合在一起，通过提供一个 Guidance 来指导 Rule 的执行顺序，在枚举逻辑等价算子的同时也进行物理算子的生成，这样做可以避免枚举所有的逻辑执行计划</p>
<p><strong>Memo</strong></p>
<p>Cascades Optimizer 在搜索的过程中，其搜索的空间</p>
<ul>
<li>MEMO 的定义：是一种数据结构，用于管理一个组，每个组代表一个查询计划的不同子目标。</li>
<li>MEMO 结构的目标：是通过尽可能的公用相同的子树使得内存的使用最小。</li>
<li>MEMO 的主要思想：通过使用共享的副本来避免子树的重复使用。</li>
</ul>
<p><strong>Rule</strong><br>Volcano/Cascade Optimizer 中的变化都使用 Rule 来描述:</p>
<ul>
<li>Logical Algebra 之间的转换使用 Transformation Rule；</li>
<li>Logical Algebra 到 Physical Algebra 之间的转换使用 Implementation Rule</li>
<li>Physical Property 可以从 Physical Algebra 中提取，表示算子所产生的数的具有的物理属性，比如按照某个 Key 排序、按照某个 Key 分布在集群中等</li>
</ul>
<p>Memo 中两个最基本的概念就是 Expression Group（简称 Group） 以及 Group Expression（对应关系代数算子）</p>
<ul>
<li>每个 Group 中保存的是逻辑等价的 Group Expression</li>
<li>Group Expression 的子节点是由 Group 组成</li>
</ul>
<h3 id="Init-Memo"><a href="#Init-Memo" class="headerlink" title="Init Memo"></a>Init Memo</h3><p><img src="/2020/02/09/Calcite-Volcano-Planner/image-20200208221706543.png" alt="image-20200208221706543"></p>
<p>一旦最初的计划复制到了MEMO结构中以后，就可以对逻辑操作符做一些转换以生成物理操作符。</p>
<p>一个转换规则可以生成：</p>
<ol>
<li>同一组中的一个逻辑操作符: 如 join( A, B) -&gt; join( B, A)</li>
<li>同一组中的一个物理操作符: 如 join -&gt; Hash Join</li>
</ol>
<p>一组逻辑操作符组成一个子计划。根仍保留在原来的组中，而其他操作符分配到其他的组中，必要的时候可以建立新组，如 join( A, join(B,C)) -&gt; join( join(A,B), C), 这两个最外面的 Join 是等价的, 所以是同一个根节点, 但是前后两次里面的 join 不一样, 所以在不同的组</p>
<h3 id="Apply-transformation-implement-rule"><a href="#Apply-transformation-implement-rule" class="headerlink" title="Apply transformation/implement rule"></a>Apply transformation/implement rule</h3><p>由于物理属性的不同，同一组中的某些操作符可作为孩子节点，而另外一些操作符则不能</p>
<p><img src="/2020/02/09/Calcite-Volcano-Planner/image-20200208221734549.png" alt="image-20200208221734549"></p>
<h3 id="Find-best-plan"><a href="#Find-best-plan" class="headerlink" title="Find best plan"></a>Find best plan</h3><p><img src="/2020/02/09/Calcite-Volcano-Planner/image-20200208221741219.png" alt="image-20200208221741219"></p>
<p><strong>Ref:</strong></p>
<p>[1]. [图片] Counting, Enumerating, and Sampling of Execution Plans in a Cost-Based Query Optimizer</p>
<p>[2]. The Cascades Framework for Query Optimization</p>
<p>[3]. Orca: A Modular Query Optimizer Architecture for Big Data</p>
<h1 id="Calcite-Volcano-Planner"><a href="#Calcite-Volcano-Planner" class="headerlink" title="Calcite Volcano Planner"></a>Calcite Volcano Planner</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><ul>
<li><p><strong>RelNode</strong>: Relation Expression, 逻辑执行计划的节点.</p>
<ul>
<li><p>继承自 RelOptNode, 代表可以被优化器优化</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 每个 RelNode 都有对应的 RelTraitSet 来描述其物理特性</span></span><br><span class="line">  <span class="keyword">protected</span> RelTraitSet traitSet;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 其 children</span></span><br><span class="line">  <span class="function">List&lt;RelNode&gt; <span class="title">getInputs</span><span class="params">()</span></span>;</span><br><span class="line">&lt;!--￼<span class="number">0</span>--&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>RelTrait:</p>
<ul>
<li>RelTrait 是对应 RelTraitDef 的具体实例, 需要关注的是可以 register 方法来添加注册相关的 Rule(但是只有 JdbcConvention 一个实现了这个方法)</li>
<li>代表 Convention/排序/分布, Convention下面会详细解释.</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">register</span><span class="params">(RelOptPlanner planner)</span></span>;</span><br><span class="line"><span class="function">RelTraitDef <span class="title">getTraitDef</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">satisfies</span><span class="params">(RelTrait trait)</span></span>;</span><br></pre></td></tr></table></figure></li>
<li><p>RelTraitDef:</p>
<ul>
<li><p>类定义: <code>public abstract class RelTraitDef&lt;T extends RelTrait&gt;</code></p>
</li>
<li><p>定义一类 <code>RelTrait</code>, 有三类 (ConventionTraitDef/RelCollationTraitDef/RelDistributionTraitDef)</p>
</li>
<li><p>主要有两个方法 <code>convert()</code>/ <code>canConvert()</code>:</p>
<ul>
<li><p><code>canConvert</code> 和 <code>RelTrait#satisfies</code>, 没搞懂为啥要搞两套.</p>
<ul>
<li>e.g. Collation 的 <code>convert</code> 实现则是添加一个 <code>LogicalSort</code> 的 Rel 来替换原始 Rel 达到替换输出 trait 的目的</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol>
<li>ConventionTraitDef<ul>
<li><strong>代表一个物理实现, 为了方便异构数据源(database backend)混合查询, .</strong></li>
</ul>
</li>
<li>RelCollationTraitDef (排序)</li>
<li>RelDistributionTraitDef (分布)</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>RelSet</strong>: 描述一组逻辑上相等的 RelNode 的集合</p>
<ul>
<li>没有父类, 不是 **RelNode **!</li>
<li>所有的等价的 RelNode 会记录在 <code>rels</code> 中</li>
<li>一组等价关系表达式的集合, 语义相同, 但是其中的 RelNode 可以有不同的 Trait.</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> List&lt;RelNode&gt; rels = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">final</span> List&lt;RelNode&gt; parents = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">final</span> List&lt;RelSubset&gt; subsets = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * List of &#123;<span class="doctag">@link</span> AbstractConverter&#125; objects which have not yet been</span></span><br><span class="line"><span class="comment"> * satisfied.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">final</span> List&lt;AbstractConverter&gt; abstractConverters = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure></li>
<li><p><strong>RelSubset</strong>: 描述一组物理上相等的 Relation Expression，即具有相同的 Physical Properties</p>
<ul>
<li>也是一种 <strong>RelNode, 不是 RelSet !!!</strong></li>
<li>在一个 RelSet 中相同的 RelTraitSet 的 RelNode 会在同一个 RelSubSet 内</li>
<li>添加一个 Rel 到 RelSubset 会添加 rel 到对应的 RelSet 中</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * cost of best known plan (it may have improved since)</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> RelOptCost bestCost;</span><br><span class="line">  </span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * The set this subset belongs to.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="keyword">final</span> RelSet set;</span><br><span class="line">  </span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * best known plan</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"> RelNode best;</span><br><span class="line">  </span><br><span class="line"> <span class="keyword">protected</span> RelTraitSet traitSet;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> Iterable&lt;RelNode&gt; <span class="title">getRels</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> () -&gt; Linq4j.asEnumerable(set.rels)</span><br><span class="line">       .where(v1 -&gt; v1.getTraitSet().satisfies(traitSet))</span><br><span class="line">       .iterator();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>一个 RelSet 有一组 RelSubset, 而一个 RelSubset 引用一个 RelSet.</li>
<li>一个 RelSet 可能有多种 trait (因为一组 RelNode 逻辑上等价, 物理上(trait) 不等价), 比如 在 [X] , [Y, Z] 上都进行了排序, 那么对于这个 RelSet 有两个 RelSubset</li>
<li>通过 getRels 符合自己 trait 的 rels.</li>
</ul>
</li>
<li><p><strong>VolcanoRuleMatch/RuleCall</strong> :描述一次成功规则的匹配，包含 Rule 和被匹配的节点</p>
</li>
<li><p><strong>RuleQueue</strong>:是一个优先队列，包含当前所有可行的 RuleMatch</p>
</li>
<li><p><strong>Importance</strong> :描述 RuleMatch 的重要程度，importance 大的优先处理, 每一轮迭代都会实时调整.</p>
<ul>
<li><ul>
<li>尽量对代价大的节点先做优化，从而尽可能在有限的优化次数内获得更大的收益, ost 越大、importance 也越大</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Program</strong>: 用来组装优化的流程, 类似 pipeline 的感觉.</p>
</li>
</ul>
<h4 id="调用约定概念梳理"><a href="#调用约定概念梳理" class="headerlink" title="调用约定概念梳理"></a>调用约定概念梳理</h4><p>Calite 的特有概念, 为了支持多数据源(异构数据源)</p>
<h5 id="Convention"><a href="#Convention" class="headerlink" title="Convention"></a>Convention</h5><ul>
<li><p>一种 RelTrait, <strong>表一个物理实现</strong></p>
</li>
<li><p>几种实现</p>
<ol>
<li>Convention#Impl: 默认的 Convention, 只是提供了接口信息和名称信息 e.g.</li>
</ol>
<ul>
<li><code>Convention NONE = new Impl(&quot;NONE&quot;, RelNode.class);</code></li>
<li><code>SparkRel.CONVENTION: new Convention.Impl(&quot;SPARK&quot;, SparkRel.class);</code></li>
</ul>
<ol>
<li><p>EnumerableConvention: 能返回 <code>linq4j.Enumerable</code> 的 Convention 实现, 默认一般用这个, 会使用 codegen.</p>
</li>
<li><p>InterpretableConvention: 也是返回 Enumerable, 同样会实现 EnumerableRel 接口, 不通过 codegen 执行</p>
</li>
<li><p>JdbcConvention,</p>
</li>
<li><p>BindableConvention</p>
</li>
</ol>
</li>
</ul>
<h5 id="ConverterRule"><a href="#ConverterRule" class="headerlink" title="ConverterRule:"></a>ConverterRule:</h5><ul>
<li>是 RelOptRule 的子类, 专门用来做数据源之间的转换</li>
<li>ConverterRule 一般会调用对应的 Converter 来完成工作, 比如说 JdbcToSparkConverterRule 调用 JdbcToSparkConverter 来完成对 JDBC Table 到 Spark RDD 的转换</li>
<li>Abstract base class for a rule which converts from one calling convention to another without changing semantics.</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Planner rule that converts from Spark to enumerable convention. */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkToEnumerableConverterRule</span> <span class="keyword">extends</span> <span class="title">ConverterRule</span> </span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> SparkToEnumerableConverterRule INSTANCE =</span><br><span class="line">      <span class="keyword">new</span> SparkToEnumerableConverterRule();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="title">SparkToEnumerableConverterRule</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(</span><br><span class="line">        RelNode.class, SparkRel.CONVENTION, EnumerableConvention.INSTANCE,</span><br><span class="line">        <span class="string">&quot;SparkToEnumerableConverterRule&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> RelNode <span class="title">convert</span><span class="params">(RelNode rel)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> SparkToEnumerableConverter(rel.getCluster(),</span><br><span class="line">        rel.getTraitSet().replace(EnumerableConvention.INSTANCE), rel);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Converter"><a href="#Converter" class="headerlink" title="Converter:"></a>Converter:</h5><ul>
<li>一种特殊的 RelNode (<strong>这是一个 RelNode</strong>)</li>
<li>后续用 ExpandConversionRule 来调用 TraitDef 的 convert 来做转换, 关于 convention, 需要 isGuaranteed 为 true, 但是在 calcite 代码里并没有这个转换, 其他使用 Calcite 的引擎有, 如 Drill.</li>
<li>由 Converter Rule 转化而成</li>
<li>举个例子: <code>SparkToEnumerableConverterRule</code> 实现了 <code>SparkRel.CONVENTION</code> 到 <code>EnumerableConvention</code> 的转换，生成了<code>SparkToEnumerableConverter</code> , 对应没有这一套是会生成一个<code>AbstractConverter</code> 在根节点, 所以他们的子节点的 trait (Convention) 会以他们为准(ENUMERABLE/SPARK).</li>
<li>再次强调这个 <code>SparkToEnumerableConverter</code> 是一个 RelNode, 可以看看这个节点怎么生成物理执行计划:</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Result <span class="title">implement</span><span class="params">(EnumerableRelImplementor implementor, Prefer pref)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Generate:</span></span><br><span class="line">  <span class="comment">//   RDD rdd = ...;</span></span><br><span class="line">  <span class="comment">//   return SparkRuntime.asEnumerable(rdd);</span></span><br><span class="line">  <span class="keyword">final</span> BlockBuilder list = <span class="keyword">new</span> BlockBuilder();</span><br><span class="line">  <span class="keyword">final</span> SparkRel child = (SparkRel) getInput();</span><br><span class="line">  <span class="keyword">final</span> PhysType physType =</span><br><span class="line">      PhysTypeImpl.of(implementor.getTypeFactory(),</span><br><span class="line">          getRowType(),</span><br><span class="line">          JavaRowFormat.CUSTOM);</span><br><span class="line">  SparkRel.Implementor sparkImplementor =</span><br><span class="line">      <span class="keyword">new</span> SparkImplementorImpl(implementor);</span><br><span class="line">  <span class="keyword">final</span> SparkRel.Result result = child.implementSpark(sparkImplementor);</span><br><span class="line">  <span class="keyword">final</span> Expression rdd = list.append(<span class="string">&quot;rdd&quot;</span>, result.block);</span><br><span class="line">  <span class="keyword">final</span> Expression enumerable =</span><br><span class="line">      list.append(</span><br><span class="line">          <span class="string">&quot;enumerable&quot;</span>,</span><br><span class="line">          Expressions.call(</span><br><span class="line">              SparkMethod.AS_ENUMERABLE.method,</span><br><span class="line">              rdd));</span><br><span class="line">  list.add(Expressions.return_(<span class="keyword">null</span>, enumerable));</span><br><span class="line">  <span class="keyword">return</span> implementor.result(physType, list.toBlock());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>SparkMethod.AS_ENUMERABLE.method</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AS_ENUMERABLE(SparkRuntime.class, <span class="string">&quot;asEnumerable&quot;</span>, JavaRDD.class)</span><br></pre></td></tr></table></figure>

<p>在 SparkRuntime 类中可以找到 asEnumerable 方法:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Converts an RDD into an enumerable. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">Enumerable&lt;T&gt; <span class="title">asEnumerable</span><span class="params">(JavaRDD&lt;T&gt; rdd)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> Linq4j.asEnumerable(rdd.collect());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/09/Calcite-Volcano-Planner/image-20200208225453064-20210507205155192.png" alt="image-20200208225453064"></p>
<h2 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h2><h3 id="入口点"><a href="#入口点" class="headerlink" title="入口点"></a>入口点</h3><p><strong>测试 SQL: select * from emps where name = ‘John’</strong></p>
<p><strong>1. Prepare</strong></p>
<p>看代码中最后的 return 语句, 加进来了一系列的优化:</p>
<ol>
<li>Hep Planner 做一些子查询的优化 (SubQueryRemoveRule.FILTER, SubQueryRemoveRule.PROJECT, SubQueryRemoveRule.JOIN)</li>
<li>DecorrelateProgram/TrimFieldsProgram</li>
<li><strong>Volcano Planner</strong>, 默认会注册一些 rule, 在 RelOptUtil#registerDefaultRules</li>
<li>Hep Planner# RelOptRules.CALC_RULES</li>
</ol>
<p>对 VolcanoPlanner 的使用就是在第3步, 我们主要关注这一步</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> RelRoot <span class="title">optimize</span><span class="params">(RelRoot root)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">final</span> Program program = getProgram();</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">final</span> RelNode rootRel4 = program.run(planner, root.rel, desiredTraits, materializationList, latticeList);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Program <span class="title">standard</span><span class="params">(RelMetadataProvider metadataProvider)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> Program program1 =</span><br><span class="line">      (planner, rel, requiredOutputTraits, materializations, lattices) -&gt; &#123;</span><br><span class="line">        planner.setRoot(rel);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (RelOptMaterialization materialization : materializations) &#123;</span><br><span class="line">          planner.addMaterialization(materialization);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (RelOptLattice lattice : lattices) &#123;</span><br><span class="line">          planner.addLattice(lattice);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> RelNode rootRel2 =</span><br><span class="line">            rel.getTraitSet().equals(requiredOutputTraits)</span><br><span class="line">                ? rel</span><br><span class="line">                : planner.changeTraits(rel, requiredOutputTraits);</span><br><span class="line">        <span class="keyword">assert</span> rootRel2 != <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        planner.setRoot(rootRel2);</span><br><span class="line">        <span class="keyword">final</span> RelOptPlanner planner2 = planner.chooseDelegate();</span><br><span class="line">        <span class="keyword">final</span> RelNode rootRel3 = planner2.findBestExp();</span><br><span class="line">        <span class="keyword">assert</span> rootRel3 != <span class="keyword">null</span> : <span class="string">&quot;could not implement exp&quot;</span>;</span><br><span class="line">        <span class="keyword">return</span> rootRel3;</span><br><span class="line">      &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> sequence(subQuery(metadataProvider),</span><br><span class="line">      <span class="keyword">new</span> DecorrelateProgram(),</span><br><span class="line">      <span class="keyword">new</span> TrimFieldsProgram(),</span><br><span class="line">      program1,</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Second planner pass to do physical &quot;tweaks&quot;. This the first time</span></span><br><span class="line">      <span class="comment">// that EnumerableCalcRel is introduced.</span></span><br><span class="line">      calc(metadataProvider));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="set-default-rule-trait"><a href="#set-default-rule-trait" class="headerlink" title="set default rule/trait"></a>set default rule/trait</h3><p><strong>CalcitePrepareImpl#createPlanner</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> VolcanoPlanner planner = <span class="keyword">new</span> VolcanoPlanner(costFactory, externalContext);</span><br><span class="line"></span><br><span class="line">planner.addRelTraitDef(ConventionTraitDef.INSTANCE);</span><br><span class="line"><span class="keyword">if</span> (CalciteSystemProperty.ENABLE_COLLATION_TRAIT.value()) &#123;</span><br><span class="line">  planner.addRelTraitDef(RelCollationTraitDef.INSTANCE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">RelOptUtil.registerDefaultRules(planner,prepareContext.config().materializationsEnabled(), enableBindable);</span><br></pre></td></tr></table></figure>

<p>VolcanoPlanner#addRule</p>
<ul>
<li>RelOptRuleOperand<ul>
<li>e.g. Join(Filter, Any)</li>
</ul>
</li>
</ul>
<p>在优化时，哪个 RelNode 可以应用哪些 Rule 都已经提前记录好了, 是一个 MultiMap，一个<strong>RelNode</strong>可以对应于多个 operands.</p>
<p>后面还会调用 rule 的 onMatch 进行筛选, 这里只是很粗粒度的</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Each of this rule&#x27;s operands is an &#x27;entry point&#x27; for a rule call.</span></span><br><span class="line"><span class="comment">// Register each operand against all concrete sub-classes that could match it.</span></span><br><span class="line"><span class="keyword">for</span> (RelOptRuleOperand operand : rule.getOperands()) &#123;</span><br><span class="line">  <span class="keyword">for</span> (Class&lt;? extends RelNode&gt; subClass: subClasses(operand.getMatchedClass())) &#123;</span><br><span class="line">    classOperands.put(subClass, operand);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>match 的具体位置在:</p>
<p>org.apache.calcite.plan.volcano.VolcanoRuleCall#matchRecurse</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">if</span> (getRule().matches(<span class="keyword">this</span>)) &#123;</span><br><span class="line">  onMatch();</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="setRoot-1st"><a href="#setRoot-1st" class="headerlink" title="setRoot (1st)"></a>setRoot (1st)</h3><p>初始进入 optimizer 时的执行计划的 dump plan:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LogicalProject(EMPNO<span class="operator">=</span>[$<span class="number">0</span>], NAME<span class="operator">=</span>[$<span class="number">1</span>], DEPTNO<span class="operator">=</span>[$<span class="number">2</span>], GENDER<span class="operator">=</span>[$<span class="number">3</span>], CITY<span class="operator">=</span>[$<span class="number">4</span>], EMPID<span class="operator">=</span>[$<span class="number">5</span>], AGE<span class="operator">=</span>[$<span class="number">6</span>], SLACKER<span class="operator">=</span>[$<span class="number">7</span>], MANAGER<span class="operator">=</span>[$<span class="number">8</span>], JOINEDAT<span class="operator">=</span>[$<span class="number">9</span>])</span><br><span class="line">  LogicalFilter(<span class="keyword">condition</span><span class="operator">=</span>[<span class="operator">=</span>($<span class="number">1</span>, <span class="string">&#x27;John&#x27;</span>)])</span><br><span class="line">    LogicalTableScan(<span class="keyword">table</span><span class="operator">=</span>[[SALES, EMPS]])</span><br></pre></td></tr></table></figure>

<p>核心方法是 <code>registerImpl</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRoot</span><span class="params">(RelNode rel)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.root = registerImpl(rel, <span class="keyword">null</span>);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>.originalRoot == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>.originalRoot = rel;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Making a node the root changes its importance.</span></span><br><span class="line">  <span class="keyword">this</span>.ruleQueue.recompute(<span class="keyword">this</span>.root);</span><br><span class="line">  ensureRootConverters();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> RelSubset <span class="title">registerImpl</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      RelNode rel,</span></span></span><br><span class="line"><span class="params"><span class="function">      RelSet set)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">  <span class="keyword">if</span> (rel <span class="keyword">instanceof</span> RelSubset) &#123;</span><br><span class="line">    <span class="keyword">return</span> registerSubset(set, (RelSubset) rel);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 递归调用</span></span><br><span class="line">  rel = rel.onRegister(<span class="keyword">this</span>);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Place the expression in the appropriate equivalence set.</span></span><br><span class="line">  <span class="keyword">if</span> (set == <span class="keyword">null</span>) &#123;</span><br><span class="line">    set = <span class="keyword">new</span> RelSet(</span><br><span class="line">        nextSetId++,</span><br><span class="line">        Util.minus(</span><br><span class="line">            RelOptUtil.getVariablesSet(rel),</span><br><span class="line">            rel.getVariablesSet()),</span><br><span class="line">        RelOptUtil.getVariablesUsed(rel));</span><br><span class="line">    <span class="keyword">this</span>.allSets.add(set);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// **把 RelNode 加到 RelSet 中, 并且返回一个 RelSubset, 因为新加入的 rel 可能有之前没有过的 trait, 所以可能生成一个新的 RelSubset**</span></span><br><span class="line">  RelSubset subset = addRelToSet(rel, set);</span><br><span class="line">  </span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// Queue up all rules triggered by this relexp&#x27;s creation.</span></span><br><span class="line">  fireRules(rel, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// It&#x27;s a new subset.</span></span><br><span class="line">  <span class="keyword">if</span> (set.subsets.size() &gt; subsetBeforeCount) &#123;</span><br><span class="line">    fireRules(subset, <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> subset;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> RelSubset <span class="title">addRelToSet</span><span class="params">(RelNode rel, RelSet set)</span> </span>&#123;</span><br><span class="line">  RelSubset subset = set.add(rel);</span><br><span class="line">  mapRel2Subset.put(rel, subset);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// While a tree of RelNodes is being registered, sometimes nodes&#x27; costs</span></span><br><span class="line">  <span class="comment">// improve and the subset doesn&#x27;t hear about it. You can end up with</span></span><br><span class="line">  <span class="comment">// a subset with a single rel of cost 99 which thinks its best cost is</span></span><br><span class="line">  <span class="comment">// 100. We think this happens because the back-links to parents are</span></span><br><span class="line">  <span class="comment">// not established. So, give the subset another chance to figure out</span></span><br><span class="line">  <span class="comment">// its cost.</span></span><br><span class="line">  <span class="keyword">final</span> RelMetadataQuery mq = rel.getCluster().getMetadataQuery();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    subset.propagateCostImprovements(<span class="keyword">this</span>, mq, rel, <span class="keyword">new</span> HashSet&lt;&gt;());</span><br><span class="line">  &#125; <span class="keyword">catch</span> (CyclicMetadataException e) &#123;</span><br><span class="line">    <span class="comment">// ignore</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> subset;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>onRegister</strong></p>
<p>注意, 这里更新了 RelNode 的 input 为 RelSubSet(ensureRegistered).</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RelNode <span class="title">onRegister</span><span class="params">(RelOptPlanner planner)</span> </span>&#123;</span><br><span class="line">  List&lt;RelNode&gt; oldInputs = getInputs();</span><br><span class="line">  List&lt;RelNode&gt; inputs = <span class="keyword">new</span> ArrayList&lt;&gt;(oldInputs.size());</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">final</span> RelNode input : oldInputs) &#123;</span><br><span class="line">    RelNode e = planner.ensureRegistered(input, <span class="keyword">null</span>);</span><br><span class="line">        </span><br><span class="line">    <span class="comment">// 更新 input 为 RelSubset</span></span><br><span class="line">    inputs.add(e);</span><br><span class="line">  &#125;</span><br><span class="line">  RelNode r = <span class="keyword">this</span>;</span><br><span class="line">  <span class="keyword">if</span> (!Util.equalShallow(oldInputs, inputs)) &#123;</span><br><span class="line">    r = copy(getTraitSet(), inputs);</span><br><span class="line">  &#125;</span><br><span class="line">  r.recomputeDigest();</span><br><span class="line">  <span class="keyword">assert</span> r.isValid(Litmus.THROW, <span class="keyword">null</span>);</span><br><span class="line">  <span class="keyword">return</span> r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>ensureRegistered</strong></p>
<p>输入是两个等价的 RelNode, 返回一个 RelSubset</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RelSubset <span class="title">ensureRegistered</span><span class="params">(RelNode rel, RelNode equivRel)</span> </span>&#123;</span><br><span class="line">  RelSubset result;</span><br><span class="line">  <span class="keyword">final</span> RelSubset subset = getSubset(rel);</span><br><span class="line">  <span class="keyword">if</span> (subset != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (equivRel != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">final</span> RelSubset equivSubset = getSubset(equivRel);</span><br><span class="line">      <span class="keyword">if</span> (subset.set != equivSubset.set) &#123;</span><br><span class="line">        merge(equivSubset.set, subset.set);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    result = subset;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    result = register(rel, equivRel);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Checking if tree is valid considerably slows down planning</span></span><br><span class="line">  <span class="comment">// Only doing it if logger level is debug or finer</span></span><br><span class="line">  <span class="keyword">if</span> (LOGGER.isDebugEnabled()) &#123;</span><br><span class="line">    <span class="function"><span class="keyword">assert</span> <span class="title">isValid</span><span class="params">(Litmus.THROW)</span></span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>fireRules</strong></p>
<p>上面的 fireRules deferred 为是 true, 意思就是不立即执行 rule, 可以看到DeferringRuleCall#onMatch 把 VolcanoRuleMatch 扔到了 rule queue 中, 等之后执行.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fireRules</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    RelNode rel,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">boolean</span> deferred)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (RelOptRuleOperand operand : classOperands.get(rel.getClass())) &#123;</span><br><span class="line">    <span class="keyword">if</span> (operand.matches(rel)) &#123;</span><br><span class="line">      <span class="keyword">final</span> VolcanoRuleCall ruleCall;</span><br><span class="line">      <span class="keyword">if</span> (deferred) &#123;</span><br><span class="line">        ruleCall = <span class="keyword">new</span> DeferringRuleCall(<span class="keyword">this</span>, operand);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        ruleCall = <span class="keyword">new</span> VolcanoRuleCall(<span class="keyword">this</span>, operand);</span><br><span class="line">      &#125;</span><br><span class="line">      ruleCall.match(rel);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DeferringRuleCall</span> <span class="keyword">extends</span> <span class="title">VolcanoRuleCall</span> </span>&#123;</span><br><span class="line">  DeferringRuleCall(</span><br><span class="line">      VolcanoPlanner planner,</span><br><span class="line">      RelOptRuleOperand operand) &#123;</span><br><span class="line">    <span class="keyword">super</span>(planner, operand);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Rather than invoking the rule (as the base method does), creates a</span></span><br><span class="line"><span class="comment">   * &#123;<span class="doctag">@link</span> VolcanoRuleMatch&#125; which can be invoked later.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onMatch</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> VolcanoRuleMatch match =</span><br><span class="line">        <span class="keyword">new</span> VolcanoRuleMatch(</span><br><span class="line">            volcanoPlanner,</span><br><span class="line">            getOperand0(),</span><br><span class="line">            rels,</span><br><span class="line">            nodeInputs);</span><br><span class="line">    volcanoPlanner.ruleQueue.addMatch(match);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>总结:</strong></p>
<p>输入是一颗 RelNode 的树， setRoot 是深度遍历, 将每个 RelNode 创建一个 RelSet 和一个包含初始 RelNode 的 RelSubSet, 并且把节点所能执行的 Rule 作为一个 VolcanoRuleCall 放入 RuleQueue. 最终生成的是一颗 RelSet 组成的树(子树在 RelSet.rels (RelNode#getInput), input 是 RelSubSet, RelSubSet 又记录了 RelSet, 以此类推, 构成一棵树)</p>
<img src="/2020/02/09/Calcite-Volcano-Planner/image-20200201142854598.png" alt="image-20200201142854598" style="zoom:50%;">

<h3 id="setRoot-2st"><a href="#setRoot-2st" class="headerlink" title="setRoot (2st)"></a>setRoot (2st)</h3><p>首先会进行 <code>RelNode rootRel2 = planner.changeTraits(rel, requiredOutputTraits)</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RelNode <span class="title">changeTraits</span><span class="params">(<span class="keyword">final</span> RelNode rel, RelTraitSet toTraits)</span> </span>&#123;</span><br><span class="line">    RelSubset rel2 = ensureRegistered(rel, <span class="keyword">null</span>);</span><br><span class="line">  <span class="keyword">if</span> (rel2.getTraitSet().equals(toTraits)) &#123;</span><br><span class="line">    <span class="keyword">return</span> rel2;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> rel2.set.getOrCreateSubset(rel.getCluster(), toTraits.simplify());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在 changeTraits 里会创建一个新的 RelSubset(<code>rel#16:Subset#2.ENUMERABLE.[]</code>) 作为根节点(rootRel2), 但是两者还是同一个 RelSet (因为逻辑语义上没有变化)</p>
<p><img src="/2020/02/09/Calcite-Volcano-Planner/image-20200130143902228.png" alt="image-20200130143902228"></p>
<p>然后会再用这个 rootRel2 再 set 一次 root: <code>planner.setRoot(rootRel2);</code> 由于这次的根节点是 RelSubset, registerImpl 会走到 <code>registerSubset</code> 中去. 不过会直接返回出来.</p>
<p>接着会执行 <code>ensureRootConverters</code> , 这里发现当root 的traitSet 和 root 的 subset 的 traitSet 不想等的时候, 会添加一个 AbstractConverter 节点. 注意这个 AbstractConverter 是一个 RelNode.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ensureRootConverters</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> Set&lt;RelSubset&gt; subsets = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">  <span class="keyword">for</span> (RelSubset subset : root.set.subsets) &#123;</span><br><span class="line">    <span class="keyword">final</span> ImmutableList&lt;RelTrait&gt; difference = root.getTraitSet().difference(subset.getTraitSet());</span><br><span class="line">    <span class="keyword">if</span> (difference.size() == <span class="number">1</span> &amp;&amp; subsets.add(subset)) &#123;</span><br><span class="line">      register(<span class="keyword">new</span> AbstractConverter(subset.getCluster(), subset,</span><br><span class="line">              difference.get(<span class="number">0</span>).getTraitDef(), root.getTraitSet()), root);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在这里会再一次的调用到 registerImpl, 不过这次 RelSet 已经不为空了.</p>
<p>在 registerImpl 中, 当发现节点是 Converter 时, 会尝试把 Converter merge 到其 child 所在是 RelSet 中(<strong>Converters are in the same set as their children.</strong>).</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> RelSubset <span class="title">registerImpl</span><span class="params">(RelNode rel, RelSet set)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Converters are in the same set as their children.</span></span><br><span class="line">  <span class="keyword">if</span> (rel <span class="keyword">instanceof</span> Converter) &#123;</span><br><span class="line">    <span class="keyword">final</span> RelNode input = ((Converter) rel).getInput();</span><br><span class="line">    <span class="keyword">final</span> RelSet childSet = getSet(input);</span><br><span class="line">    <span class="keyword">if</span> ((set != <span class="keyword">null</span>) &amp;&amp; (set != childSet) &amp;&amp; (set.equivalentSet == <span class="keyword">null</span>)) &#123;</span><br><span class="line">      LOGGER.trace(<span class="string">&quot;Register #&#123;&#125; &#123;&#125; (and merge sets, because it is a conversion)&quot;</span>, rel.getId(), rel.getDigest());</span><br><span class="line">      merge(set, childSet);</span><br><span class="line">      registerCount++;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      set = childSet;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这是经过第二次 set root 之后的树的关系</p>
<img src="/2020/02/09/Calcite-Volcano-Planner/image-20200201143344730.png" alt="image-20200201143344730" style="zoom:50%;">

<p>整个过程 的 log:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">TRACE - new RelSubset#9</span><br><span class="line">TRACE - Register rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS]) in rel#9:Subset#0.NONE.[]</span><br><span class="line">TRACE - Importance of [rel#9:Subset#0.NONE.[]] is 0.0</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [EnumerableTableScanRule] rels [rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS])]</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [BindableTableScanRule] rels [rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS])]</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [TableScanRule] rels [rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS])]</span><br><span class="line">TRACE - new LogicalFilter#10</span><br><span class="line">TRACE - new RelSubset#11</span><br><span class="line">TRACE - Register rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;)) in rel#11:Subset#1.NONE.[]</span><br><span class="line">TRACE - Importance of [rel#9:Subset#0.NONE.[]] to its parent [rel#11:Subset#1.NONE.[]] is 0.0 (parent importance=0.0, child cost=1.0E30, parent cost=1.0E30)</span><br><span class="line">TRACE - Importance of [rel#9:Subset#0.NONE.[]] is 0.0</span><br><span class="line">TRACE - Importance of [rel#11:Subset#1.NONE.[]] is 0.0</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [EnumerableFilterRule] rels [rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;))]</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [FilterTableScanRule] rels [rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;)), rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS])]</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [MaterializedViewJoinRule(Filter)] rels [rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;))]</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [MaterializedViewFilterScanRule] rels [rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;)), rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS])]</span><br><span class="line">TRACE - new LogicalProject#12</span><br><span class="line">TRACE - new RelSubset#13</span><br><span class="line">TRACE - Register rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9) in rel#13:Subset#2.NONE.[]</span><br><span class="line">TRACE - Importance of [rel#11:Subset#1.NONE.[]] to its parent [rel#13:Subset#2.NONE.[]] is 0.0 (parent importance=0.0, child cost=1.0E30, parent cost=1.0E30)</span><br><span class="line">TRACE - Importance of [rel#11:Subset#1.NONE.[]] is 0.0</span><br><span class="line">TRACE - Importance of [rel#13:Subset#2.NONE.[]] is 0.0</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [EnumerableProjectRule] rels [rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9)]</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [ProjectRemoveRule] rels [rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9)]</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [MaterializedViewJoinRule(Project-Filter)] rels [rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9), rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;))]</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [ProjectFilterTransposeRule] rels [rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9), rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;))]</span><br><span class="line">TRACE - Importance of [rel#13:Subset#2.NONE.[]] is 1.0</span><br><span class="line">TRACE - new LogicalFilter#14</span><br><span class="line">TRACE - Register: rel#14 is equivalent to rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;))</span><br><span class="line">TRACE - new LogicalProject#15</span><br><span class="line">TRACE - Register: rel#15 is equivalent to rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9)</span><br><span class="line">TRACE - new RelSubset#16</span><br><span class="line">TRACE - new AbstractConverter#17</span><br><span class="line">TRACE - Register rel#17:AbstractConverter.ENUMERABLE.[](input=RelSubset#13,convention=ENUMERABLE,sort=[]) in rel#16:Subset#2.ENUMERABLE.[]</span><br><span class="line">TRACE - Importance of [rel#13:Subset#2.NONE.[]] to its parent [rel#16:Subset#2.ENUMERABLE.[]] is 0.495 (parent importance=0.5, child cost=1.0E30, parent cost=1.0E30)</span><br><span class="line">TRACE - Importance of [rel#13:Subset#2.NONE.[]] is 0.495</span><br><span class="line">TRACE - Importance of [rel#16:Subset#2.ENUMERABLE.[]] is 1.0</span><br><span class="line">TRACE - OPTIMIZE Rule-match queued: rule [ExpandConversionRule] rels [rel#17:AbstractConverter.ENUMERABLE.[](input=RelSubset#13,convention=ENUMERABLE,sort=[])]</span><br></pre></td></tr></table></figure>

<h2 id="findBestExp"><a href="#findBestExp" class="headerlink" title="findBestExp"></a>findBestExp</h2><p>这里有 4 个 phase, 但是真的有用的只有一个 <code>OPTIMIZE</code> 阶段.</p>
<h3 id="setInitialImportance"><a href="#setInitialImportance" class="headerlink" title="setInitialImportance()"></a>setInitialImportance()</h3><p>从 root 开始, 将 root SubSet 设置 importance 为 1.0, 之后的其他 children SubSet 设置 importance 为 <code>pow(0.9, n)</code>, n 为 children 在的层数. 经过这一步之后的信息, 可以看到 importance 分别为 1, 0.9, 0.81…:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Root: rel#<span class="number">16</span>:Subset#<span class="number">2.</span>ENUMERABLE.[]</span><br><span class="line">Original rel:</span><br><span class="line">LogicalProject(EMPNO=[$<span class="number">0</span>], NAME=[$<span class="number">1</span>], DEPTNO=[$<span class="number">2</span>], GENDER=[$<span class="number">3</span>], CITY=[$<span class="number">4</span>], EMPID=[$<span class="number">5</span>], AGE=[$<span class="number">6</span>], SLACKER=[$<span class="number">7</span>], MANAGER=[$<span class="number">8</span>], JOINEDAT=[$<span class="number">9</span>]): rowcount = <span class="number">15.0</span>, cumulative cost = &#123;<span class="number">130.0</span> rows, <span class="number">351.0</span> cpu, <span class="number">0.0</span> io&#125;, id = <span class="number">7</span></span><br><span class="line">  LogicalFilter(condition=[=($<span class="number">1</span>, <span class="string">&#x27;John&#x27;</span>)]): rowcount = <span class="number">15.0</span>, cumulative cost = &#123;<span class="number">115.0</span> rows, <span class="number">201.0</span> cpu, <span class="number">0.0</span> io&#125;, id = <span class="number">5</span></span><br><span class="line">    LogicalTableScan(table=[[SALES, EMPS]]): rowcount = <span class="number">100.0</span>, cumulative cost = &#123;<span class="number">100.0</span> rows, <span class="number">101.0</span> cpu, <span class="number">0.0</span> io&#125;, id = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">Sets:</span><br><span class="line">Set#<span class="number">0</span>, type: RecordType(INTEGER EMPNO, VARCHAR NAME, INTEGER DEPTNO, VARCHAR GENDER, VARCHAR CITY, INTEGER EMPID, INTEGER AGE, BOOLEAN SLACKER, BOOLEAN MANAGER, DATE JOINEDAT)</span><br><span class="line">    rel#<span class="number">9</span>:Subset#<span class="number">0.</span>NONE.[], best=<span class="keyword">null</span>, importance=<span class="number">0.7290000000000001</span></span><br><span class="line">        rel#<span class="number">0</span>:LogicalTableScan.NONE.[](table=[SALES, EMPS]), rowcount=<span class="number">100.0</span>, cumulative cost=&#123;inf&#125;</span><br><span class="line">Set#<span class="number">1</span>, type: RecordType(INTEGER EMPNO, VARCHAR NAME, INTEGER DEPTNO, VARCHAR GENDER, VARCHAR CITY, INTEGER EMPID, INTEGER AGE, BOOLEAN SLACKER, BOOLEAN MANAGER, DATE JOINEDAT)</span><br><span class="line">    rel#<span class="number">11</span>:Subset#<span class="number">1.</span>NONE.[], best=<span class="keyword">null</span>, importance=<span class="number">0.81</span></span><br><span class="line">        rel#<span class="number">10</span>:LogicalFilter.NONE.[](input=RelSubset#<span class="number">9</span>,condition==($<span class="number">1</span>, <span class="string">&#x27;John&#x27;</span>)), rowcount=<span class="number">15.0</span>, cumulative cost=&#123;inf&#125;</span><br><span class="line">Set#<span class="number">2</span>, type: RecordType(INTEGER EMPNO, VARCHAR NAME, INTEGER DEPTNO, VARCHAR GENDER, VARCHAR CITY, INTEGER EMPID, INTEGER AGE, BOOLEAN SLACKER, BOOLEAN MANAGER, DATE JOINEDAT)</span><br><span class="line">    rel#<span class="number">13</span>:Subset#<span class="number">2.</span>NONE.[], best=<span class="keyword">null</span>, importance=<span class="number">0.9</span></span><br><span class="line">        rel#<span class="number">12</span>:LogicalProject.NONE.[](input=RelSubset#<span class="number">11</span>,EMPNO=$<span class="number">0</span>,NAME=$<span class="number">1</span>,DEPTNO=$<span class="number">2</span>,GENDER=$<span class="number">3</span>,CITY=$<span class="number">4</span>,EMPID=$<span class="number">5</span>,AGE=$<span class="number">6</span>,SLACKER=$<span class="number">7</span>,MANAGER=$<span class="number">8</span>,JOINEDAT=$<span class="number">9</span>), rowcount=<span class="number">15.0</span>, cumulative cost=&#123;inf&#125;</span><br><span class="line">    rel#<span class="number">16</span>:Subset#<span class="number">2.</span>ENUMERABLE.[], best=<span class="keyword">null</span>, importance=<span class="number">1.0</span></span><br><span class="line">        rel#<span class="number">17</span>:AbstractConverter.ENUMERABLE.[](input=RelSubset#<span class="number">13</span>,convention=ENUMERABLE,sort=[]), rowcount=<span class="number">15.0</span>, cumulative cost=&#123;inf&#125;</span><br></pre></td></tr></table></figure>

<p>由于现在还没有一个完整的物理执行计划, 所以整个计划的 best cost 还是 inf.</p>
<h3 id="RuleQueue-popMatch"><a href="#RuleQueue-popMatch" class="headerlink" title="RuleQueue#popMatch"></a>RuleQueue#popMatch</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">VolcanoRuleMatch match = ruleQueue.popMatch(phase);</span><br><span class="line">match.onMatch();</span><br></pre></td></tr></table></figure>

<p>在初始 RuleQueue 里有 13 个 Rule Match.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rule [ExpandConversionRule] rels [rel#17:AbstractConverter.ENUMERABLE.[](input=RelSubset#13,convention=ENUMERABLE,sort=[])]</span><br><span class="line">rule [EnumerableProjectRule(in:NONE,out:ENUMERABLE)] rels [rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9)]&quot;</span><br><span class="line">rule [ProjectRemoveRule] rels [rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9)]&quot;</span><br><span class="line">rule [ProjectFilterTransposeRule] rels [rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9), rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;))]&quot;</span><br><span class="line">rule [MaterializedViewJoinRule(Project-Filter)] rels [rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9), rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;))]&quot;</span><br><span class="line">rule [ReduceExpressionsRule(Filter)] rels [rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;))]&quot;</span><br><span class="line">rule [MaterializedViewFilterScanRule] rels [rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;)), rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS])]&quot;</span><br><span class="line">rule [FilterTableScanRule] rels [rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;)), rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS])]&quot;</span><br><span class="line">rule [MaterializedViewJoinRule(Filter)] rels [rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;))]&quot;</span><br><span class="line">rule [EnumerableFilterRule(in:NONE,out:ENUMERABLE)] rels [rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;))]&quot;</span><br><span class="line">rule [TableScanRule] rels [rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS])]&quot;</span><br><span class="line">rule [BindableTableScanRule] rels [rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS])]&quot;</span><br><span class="line">rule [EnumerableTableScanRule(in:NONE,out:ENUMERABLE)] rels [rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS])]&quot;</span><br></pre></td></tr></table></figure>

<p>挑几个 rule 来举例子:</p>
<h4 id="EnumerableProjectRule"><a href="#EnumerableProjectRule" class="headerlink" title="EnumerableProjectRule"></a>EnumerableProjectRule</h4><p>可以看到在 convert 方法中生成了一个 EnumerableProject, 然后 transformTo 了过去.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Rule to convert a &#123;<span class="doctag">@link</span> org.apache.calcite.rel.logical.LogicalProject&#125; to an</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> EnumerableProject&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EnumerableProjectRule</span> <span class="keyword">extends</span> <span class="title">ConverterRule</span> </span>&#123;</span><br><span class="line">  EnumerableProjectRule() &#123;</span><br><span class="line">    <span class="keyword">super</span>(LogicalProject.class,</span><br><span class="line">        (Predicate&lt;LogicalProject&gt;) RelOptUtil::containsMultisetOrWindowedAgg,</span><br><span class="line">        Convention.NONE, EnumerableConvention.INSTANCE,</span><br><span class="line">        RelFactories.LOGICAL_BUILDER, <span class="string">&quot;EnumerableProjectRule&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onMatch</span><span class="params">(RelOptRuleCall call)</span> </span>&#123;</span><br><span class="line">    RelNode rel = call.rel(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (rel.getTraitSet().contains(inTrait)) &#123;</span><br><span class="line">      <span class="keyword">final</span> RelNode converted = convert(rel);</span><br><span class="line">      <span class="keyword">if</span> (converted != <span class="keyword">null</span>) &#123;</span><br><span class="line">        call.transformTo(converted);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> RelNode <span class="title">convert</span><span class="params">(RelNode rel)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> LogicalProject project = (LogicalProject) rel;</span><br><span class="line">    <span class="keyword">return</span> EnumerableProject.create(</span><br><span class="line">        convert(project.getInput(),</span><br><span class="line">            project.getInput().getTraitSet()</span><br><span class="line">                .replace(EnumerableConvention.INSTANCE)),</span><br><span class="line">        project.getProjects(),</span><br><span class="line">        project.getRowType());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Converts a relation expression to a given set of traits, if it does not</span></span><br><span class="line"><span class="comment">   * already have those traits.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> rel      Relational expression to convert</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> toTraits desired traits</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@return</span> a relational expression with the desired traits; never null</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> RelNode <span class="title">convert</span><span class="params">(RelNode rel, RelTraitSet toTraits)</span> </span>&#123;</span><br><span class="line">    RelOptPlanner planner = rel.getCluster().getPlanner();</span><br><span class="line"></span><br><span class="line">    RelTraitSet outTraits = rel.getTraitSet();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; toTraits.size(); i++) &#123;</span><br><span class="line">      RelTrait toTrait = toTraits.getTrait(i);</span><br><span class="line">      <span class="keyword">if</span> (toTrait != <span class="keyword">null</span>) &#123;</span><br><span class="line">        outTraits = outTraits.replace(i, toTrait);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (rel.getTraitSet().matches(outTraits)) &#123;</span><br><span class="line">      <span class="keyword">return</span> rel;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> planner.changeTraits(rel, outTraits);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在方法 <code>transformTo</code> 中可以看到 register 了这个新生成的物理算子(EnumerableProject): <strong>volcanoPlanner.ensureRegistered(rel, rels[0], this)</strong>, 这次第二个参数 equivRel 不是 null 了, 而是 <code>rels[0](LogicalProject)</code>. 在 VolcanoPlanner#register 中会拿到 equivRel 对应的 RelSet, 再走下去就又是上面的 <code>registerImpl</code>, 只不过这时候会有一个 set 传入.</p>
<p><img src="/2020/02/09/Calcite-Volcano-Planner/image-20200130235141251.png" alt="image-20200130235141251"></p>
<p>这样就把新生成的这个物理算子注册到了原来的 RelSet 树上, 完成了 transform 的过程.</p>
<p>由于有新的算子生成(EnumerableProject), fireRule 会匹配一条新的规则到 ruleQueue 中.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OPTIMIZE Rule-match queued: rule [ProjectRemoveRule] rels [rel#19:EnumerableProject.ENUMERABLE.[](input= ...]</span><br></pre></td></tr></table></figure>

<p><strong>Before</strong></p>
<p><img src="/2020/02/09/Calcite-Volcano-Planner/image-20200208225742683.png" alt="image-20200208225742683"></p>
<p><strong>After</strong></p>
<p><img src="/2020/02/09/Calcite-Volcano-Planner/image-20200208225746328.png" alt="image-20200208225746328"></p>
<h4 id="ProjectRemoveRule"><a href="#ProjectRemoveRule" class="headerlink" title="ProjectRemoveRule"></a>ProjectRemoveRule</h4><p>因为查询是 select * , 所以可以直接取出这个 projection.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onMatch</span><span class="params">(RelOptRuleCall call)</span> </span>&#123;</span><br><span class="line">  Project project = call.rel(<span class="number">0</span>);</span><br><span class="line">  RelNode stripped = project.getInput();</span><br><span class="line">  ...</span><br><span class="line">  RelNode child = call.getPlanner().register(stripped, project);</span><br><span class="line">  call.transformTo(child);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到这个规则直接就把 project 节点的 child 注册到当前 RelSet, 也就是说直接消除了这个 projection, 故这个新的 plan 的 cost 一定会比原来低.</p>
<p><strong>merge</strong>: <code>RelSet merge(RelSet set, RelSet set2)</code>: 合并两个 RelSet.</p>
<p>RelSet#mergeWith(VolcanoPlanner planner, RelSet otherSet)</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (RelSubset otherSubset : otherSet.subsets) &#123;</span><br><span class="line">  planner.ruleQueue.subsetImportances.remove(otherSubset);</span><br><span class="line">  RelSubset subset = getOrCreateSubset(</span><br><span class="line">          otherSubset.getCluster(),</span><br><span class="line">          otherSubset.getTraitSet());</span><br><span class="line">  <span class="comment">// collect RelSubset instances, whose best should be changed</span></span><br><span class="line">  <span class="keyword">if</span> (otherSubset.bestCost.isLt(subset.bestCost)) &#123;</span><br><span class="line">    changedSubsets.put(subset, otherSubset.best);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (RelNode otherRel : otherSubset.getRels()) &#123;</span><br><span class="line">    planner.reregister(<span class="keyword">this</span>, otherRel);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>经过这个 rule 之后, 来看看 root 的 relset 的前后对比, 可以看到在这一层多了一个 logical filter.</p>
<p><strong>before</strong></p>
<p><img src="/2020/02/09/Calcite-Volcano-Planner/image-20200131214015824.png" alt="image-20200131214015824"></p>
<p><strong>after</strong></p>
<p><img src="/2020/02/09/Calcite-Volcano-Planner/image-20200131214114884.png" alt="image-20200131214114884"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Root: rel#18:Subset#1.ENUMERABLE.[]</span><br><span class="line">Original rel:</span><br><span class="line">LogicalProject(EMPNO=[$0], NAME=[$1], DEPTNO=[$2], GENDER=[$3], CITY=[$4], EMPID=[$5], AGE=[$6], SLACKER=[$7], MANAGER=[$8], JOINEDAT=[$9]): rowcount = 15.0, cumulative cost = &#123;130.0 rows, 351.0 cpu, 0.0 io&#125;, id = 7</span><br><span class="line">  LogicalFilter(condition=[=($1, &#x27;John&#x27;)]): rowcount = 15.0, cumulative cost = &#123;115.0 rows, 201.0 cpu, 0.0 io&#125;, id = 5</span><br><span class="line">    LogicalTableScan(table=[[SALES, EMPS]]): rowcount = 100.0, cumulative cost = &#123;100.0 rows, 101.0 cpu, 0.0 io&#125;, id = 0</span><br><span class="line"></span><br><span class="line">Sets:</span><br><span class="line">Set#0, type: RecordType(INTEGER EMPNO, VARCHAR NAME, INTEGER DEPTNO, VARCHAR GENDER, VARCHAR CITY, INTEGER EMPID, INTEGER AGE, BOOLEAN SLACKER, BOOLEAN MANAGER, DATE JOINEDAT)</span><br><span class="line">  rel#9:Subset#0.NONE.[], best=null, importance=0.7290000000000001</span><br><span class="line">    rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS]), rowcount=100.0, cumulative cost=&#123;inf&#125;</span><br><span class="line"></span><br><span class="line">Set#1, type: RecordType(INTEGER EMPNO, VARCHAR NAME, INTEGER DEPTNO, VARCHAR GENDER, VARCHAR CITY, INTEGER EMPID, INTEGER AGE, BOOLEAN SLACKER, BOOLEAN MANAGER, DATE JOINEDAT)</span><br><span class="line">  rel#11:Subset#1.NONE.[], best=null, importance=0.81</span><br><span class="line">    rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;)), rowcount=15.0, cumulative cost=&#123;inf&#125;</span><br><span class="line">    rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9), rowcount=15.0, cumulative cost=&#123;inf&#125;</span><br><span class="line">  rel#18:Subset#1.ENUMERABLE.[], best=null, importance=0.405</span><br><span class="line">    rel#17:AbstractConverter.ENUMERABLE.[](input=RelSubset#11,convention=ENUMERABLE,sort=[]), rowcount=15.0, cumulative cost=&#123;inf&#125;</span><br><span class="line">    rel#19:EnumerableProject.ENUMERABLE.[](input=RelSubset#18,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9), rowcount=15.0, cumulative cost=&#123;inf&#125;</span><br></pre></td></tr></table></figure>

<h4 id="经过所有规则之后"><a href="#经过所有规则之后" class="headerlink" title="经过所有规则之后:"></a>经过所有规则之后:</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Root: rel#18:Subset#1.ENUMERABLE.[]</span><br><span class="line">Original rel:</span><br><span class="line">LogicalProject(EMPNO=[$0], NAME=[$1], DEPTNO=[$2], GENDER=[$3], CITY=[$4], EMPID=[$5], AGE=[$6], SLACKER=[$7], MANAGER=[$8], JOINEDAT=[$9]): rowcount = 15.0, cumulative cost = &#123;130.0 rows, 351.0 cpu, 0.0 io&#125;, id = 7</span><br><span class="line">  LogicalFilter(condition=[=($1, &#x27;John&#x27;)]): rowcount = 15.0, cumulative cost = &#123;115.0 rows, 201.0 cpu, 0.0 io&#125;, id = 5</span><br><span class="line">    LogicalTableScan(table=[[SALES, EMPS]]): rowcount = 100.0, cumulative cost = &#123;100.0 rows, 101.0 cpu, 0.0 io&#125;, id = 0</span><br><span class="line"></span><br><span class="line">Sets:</span><br><span class="line">Set#0, type: RecordType(INTEGER EMPNO, VARCHAR NAME, INTEGER DEPTNO, VARCHAR GENDER, VARCHAR CITY, INTEGER EMPID, INTEGER AGE, BOOLEAN SLACKER, BOOLEAN MANAGER, DATE JOINEDAT)</span><br><span class="line">  rel#9:Subset#0.NONE.[], best=null, importance=0.81</span><br><span class="line">    rel#0:LogicalTableScan.NONE.[](table=[SALES, EMPS]), rowcount=100.0, cumulative cost=&#123;inf&#125;</span><br><span class="line">  rel#22:Subset#0.ENUMERABLE.[], best=rel#27, importance=0.9</span><br><span class="line">    rel#27:EnumerableInterpreter.ENUMERABLE.[](input=RelSubset#26), rowcount=100.0, cumulative cost=&#123;51.0 rows, 51.01 cpu, 0.0 io&#125;</span><br><span class="line">  rel#26:Subset#0.BINDABLE.[], best=rel#25, importance=0.81</span><br><span class="line">    rel#25:BindableTableScan.BINDABLE.[](table=[SALES, EMPS]), rowcount=100.0, cumulative cost=&#123;1.0 rows, 1.01 cpu, 0.0 io&#125;</span><br><span class="line">Set#1, type: RecordType(INTEGER EMPNO, VARCHAR NAME, INTEGER DEPTNO, VARCHAR GENDER, VARCHAR CITY, INTEGER EMPID, INTEGER AGE, BOOLEAN SLACKER, BOOLEAN MANAGER, DATE JOINEDAT)</span><br><span class="line">  rel#11:Subset#1.NONE.[], best=null, importance=0.9</span><br><span class="line">    rel#10:LogicalFilter.NONE.[](input=RelSubset#9,condition==($1, &#x27;John&#x27;)), rowcount=15.0, cumulative cost=&#123;inf&#125;</span><br><span class="line">    rel#12:LogicalProject.NONE.[](input=RelSubset#11,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9), rowcount=15.0, cumulative cost=&#123;inf&#125;</span><br><span class="line">  rel#18:Subset#1.ENUMERABLE.[], best=rel#30, importance=1.0</span><br><span class="line">    rel#17:AbstractConverter.ENUMERABLE.[](input=RelSubset#11,convention=ENUMERABLE,sort=[]), rowcount=15.0, cumulative cost=&#123;inf&#125;</span><br><span class="line">    rel#19:EnumerableProject.ENUMERABLE.[](input=RelSubset#18,EMPNO=$0,NAME=$1,DEPTNO=$2,GENDER=$3,CITY=$4,EMPID=$5,AGE=$6,SLACKER=$7,MANAGER=$8,JOINEDAT=$9), rowcount=100.0, cumulative cost=&#123;150.5 rows, 1050.505 cpu, 0.0 io&#125;</span><br><span class="line">    rel#23:EnumerableFilter.ENUMERABLE.[](input=RelSubset#22,condition==($1, &#x27;John&#x27;)), rowcount=15.0, cumulative cost=&#123;66.0 rows, 151.01 cpu, 0.0 io&#125;</span><br><span class="line">    rel#30:EnumerableInterpreter.ENUMERABLE.[](input=RelSubset#21), rowcount=100.0, cumulative cost=&#123;50.5 rows, 50.505 cpu, 0.0 io&#125;</span><br><span class="line">  rel#21:Subset#1.BINDABLE.[], best=rel#20, importance=0.9</span><br><span class="line">    rel#20:BindableTableScan.BINDABLE.[](table=[SALES, EMPS],filters=[=($1, &#x27;John&#x27;)]), rowcount=100.0, cumulative cost=&#123;0.5 rows, 0.505 cpu, 0.0 io&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/09/Calcite-Volcano-Planner/image-20200131224924660.png" alt="image-20200131224924660"></p>
<h3 id="buildCheapestPlan"><a href="#buildCheapestPlan" class="headerlink" title="buildCheapestPlan"></a>buildCheapestPlan</h3><p>从 root 开始一路选subset 中的 best, 就得到了一颗最优的树:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">EnumerableInterpreter</span><br><span class="line">  BindableTableScan(<span class="keyword">table</span><span class="operator">=</span>[[SALES, EMPS]], filters<span class="operator">=</span>[[<span class="operator">=</span>($<span class="number">1</span>, <span class="string">&#x27;John&#x27;</span>)]])</span><br></pre></td></tr></table></figure>

<h4 id="优化策略"><a href="#优化策略" class="headerlink" title="优化策略"></a>优化策略</h4><ol>
<li>第一次找到可执行计划的计划(cost 不为 inf), 其对应的 Cost 暂时记为 BestCost</li>
<li>制定下一次优化要达到的目标为 BestCost*0.9，再根据当前的迭代次数计算 giveUpTick，这个值代表的意思是：如果迭代次数超过这个值还没有达到优化目标，那么将会放弃迭代</li>
<li>如果 RuleQueue 中 RuleMatch 为空，那么也会退出迭代</li>
<li>在每次迭代时都会从 RuleQueue 中选择一个 RuleMatch，策略是选择一个最高 importance 的 RuleMatch</li>
<li>最后根据 best plan，构建其对应的 RelNode</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点:"></a>优点:</h4><ol>
<li>为异构数据源提供了原生支持 (conversion 机制)</li>
<li>volcano 的优化规则顺序不需要人工保证, 因为每次新生成一个节点, 会去执行 fireRule, 找到所有这个节点感兴趣的 rule 放到 RuleQueue 中, 当同时带来缺点 1.</li>
</ol>
<p>缺点:</p>
<ol>
<li>CBO 是个伪命题, 执行的框架模型很不错, 但是如何准确预估 cost 是难点(Filter/Join/Agg)</li>
<li>更好的方式是做成 runtime 的, 一边执行, 一边根据执行的 stats 调整查询计划的形式(ae/runtime filter).</li>
<li>重复应用规则, 有一些(很多)规则会同时被 logical/physical nodes 触发, 比如 ProjectRemoveRule 接受 Project.class, 而 Project 是逻辑/物理 Project 的 基类, 很多规则仅在逻辑执行计划上 apply 就可以了. 在社区的讨论中, 去掉了这种 case, planning 的时间提升 30% (CALCITE-2970);</li>
<li>Calcite 的 Volcano Planner 没有任何剪枝, 举例来说, 当计算一个计划到某个节点的 cost 已经比之前的 best 都高了, 则可以剪去这一枝;</li>
<li>本质上还是一个单机的优化引擎, 没有考虑分布式的优化(对比 Spark , Calcite 可以理解为无物理优化)<ol>
<li>默认优化流程里根本没有加入 RelDistribution 的考虑, 也没有提供配置</li>
<li>Distribution 也不是分布式的, 举例 calcite 的 HASH_DISTRIBUTED 值考虑了 Key, hash 的 func/num 都没有记录</li>
<li>Aggregate 只有一种, 对比 Spark 有多种策略(planAggregateWithoutDistinct/planAggregateWithOneDistinct/planStreamingAggregation)</li>
<li>有多种 join, 但是 join 策略比较弱, 没有考虑数据量(对比 Spark 的 JoinSelection), 也不支持 hint</li>
</ol>
</li>
<li>引入 AbstractConverter 来做 Spark ensureRequirement 类似的事情, 但是做的方式比较别扭, AbstractConverter 是一个执行计划中的节点, 会触发一个特定的规则 ExpandConversionRule 来保证 Distribution/Sort/Convertion, 邮件列表中讨论到这种方式污染了规则的 search space(polluting the search space), 导致了3-9 倍不必要的规则触发(对比 spark 是递归处理 root-child, calcite 每次只处理父子两个节点, 不递归). 由于它潜在的性能问题(CALCITE-2970 ), AbstractConverter 在 Calcite 代码中是默认关闭的, 详见社区讨论: Volcano’s problem with trait propagation: current state and future;</li>
<li>复杂度高, 不利于调试问题.</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> u.id <span class="keyword">AS</span> user_id,</span><br><span class="line">    u.name <span class="keyword">AS</span> user_name,</span><br><span class="line">    j.company <span class="keyword">AS</span> user_company,</span><br><span class="line">    u.age <span class="keyword">AS</span> user_age</span><br><span class="line"><span class="keyword">FROM</span> users u</span><br><span class="line"><span class="keyword">JOIN</span> jobs j <span class="keyword">ON</span> u.id<span class="operator">=</span>j.id</span><br><span class="line"><span class="keyword">WHERE</span> u.age <span class="operator">&gt;</span> <span class="number">30</span></span><br><span class="line"> <span class="keyword">AND</span> j.id<span class="operator">&gt;</span><span class="number">10</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> user_id&quot;</span><br></pre></td></tr></table></figure>

<p>共形成了 31 个 group(relset), 1000 个节点(开启了3 种 RelTraitDef)</p>
<h2 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h2><p>[1]. <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/58801070">https://zhuanlan.zhihu.com/p/58801070</a></p>
<p>[2]. <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/60223655">https://zhuanlan.zhihu.com/p/60223655</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2020/02/08/Calcite%20-%20Parser%20%E9%83%A8%E5%88%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/02/08/Calcite%20-%20Parser%20%E9%83%A8%E5%88%86/" class="post-title-link" itemprop="url">Calcite - Parser 部分</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-02-08 20:49:53" itemprop="dateCreated datePublished" datetime="2020-02-08T20:49:53+08:00">2020-02-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:27:55" itemprop="dateModified" datetime="2021-05-08T11:27:55+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="编译知识"><a href="#编译知识" class="headerlink" title="编译知识"></a>编译知识</h2><p><strong>词法分析(lexing)</strong>: 词法分析就是将文本分解成token，token就是具有特殊含义的原子单位, 如果语言的保留字、标点符号、数字、字符串等.</p>
<p><strong>语法分析(paring)</strong>:语法分析器使用词法分析从输入中分离出一个个的token，并将token流作为其输入</p>
<ul>
<li><p>根据某种给定的</p>
<p>形式文法</p>
<p>对由输入的 token 进行分析并确定其语法结构的过程</p>
<ul>
<li>自顶向下分析, 对应<strong>LL分析器</strong></li>
<li>自底向上分析, 对应<strong>LR分析器</strong></li>
</ul>
</li>
</ul>
<h2 id="javacc"><a href="#javacc" class="headerlink" title="javacc"></a>javacc</h2><p>使用递归下降语法解析，LL(k)</p>
<ul>
<li>第一个L表示从左到右扫描输入</li>
<li>第二个L表示每次都进行最左推导（在推导语法树的过程中每次都替换句型中最左的非终结符为终结符）</li>
<li>k表示每次向前探索(lookahead) k个终结符<ul>
<li><strong>LOOKAHEAD</strong>: 设置在解析过程中面临 choice point 可以look ahead的token数量, 缺省的值是1</li>
<li>调大 K 可以消除二义性, 但会减慢解析速度</li>
<li>比如 <strong>LOOKAHEAD</strong>(2) 就表示要看两个 Token 才能决定下一步的动作</li>
</ul>
</li>
</ul>
<p><strong>基本结构</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">options &#123;</span><br><span class="line">    JavaCC的选项</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PARSER_BEGIN(解析器类名)</span><br><span class="line"><span class="keyword">package</span> 包名;</span><br><span class="line"><span class="keyword">import</span> 库名;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 解析器类名 </span>&#123;</span><br><span class="line">    任意的Java代码</span><br><span class="line">&#125;</span><br><span class="line">PARSER_END(解析器类名)</span><br><span class="line"></span><br><span class="line">词法描述器</span><br><span class="line"></span><br><span class="line">语法分析器</span><br></pre></td></tr></table></figure>

<h3 id="Option块和class声明块"><a href="#Option块和class声明块" class="headerlink" title="Option块和class声明块"></a>Option块和class声明块</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* adder.jj Adding up numbers */</span></span><br><span class="line">options &#123;</span><br><span class="line">    STATIC = <span class="keyword">false</span> ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">PARSER_BEGIN(Adder)</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Adder</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> ParseException, TokenMgrError </span>&#123;</span><br><span class="line">            Adder parser = <span class="keyword">new</span> Adder( System.in );</span><br><span class="line">            parser.Start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">PARSER_END(Adder)</span><br></pre></td></tr></table></figure>

<ul>
<li>STATIC默认是true，这里要将其修改为false，使得生成的函数不是static 的。</li>
<li>ARSER_BEGIN(XXX)……PARSER_END(XXX)块，这里定义了一个名为 Adder的类</li>
</ul>
<h3 id="词法描述器"><a href="#词法描述器" class="headerlink" title="词法描述器"></a>词法描述器</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 忽略的字符</span></span><br><span class="line">SKIP:&#123;</span><br><span class="line">    <span class="string">&quot; &quot;</span></span><br><span class="line">    | <span class="string">&quot;\t&quot;</span></span><br><span class="line">    | <span class="string">&quot;\n&quot;</span></span><br><span class="line">    | <span class="string">&quot;\r&quot;</span></span><br><span class="line">    | <span class="string">&quot;\r\n&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 关键字</span></span><br><span class="line">TOKEN:&#123;</span><br><span class="line">    &lt;PLUS :<span class="string">&quot;+&quot;</span>&gt;</span><br><span class="line">    | &lt;NUMBER : ([<span class="string">&quot;0&quot;</span>-<span class="string">&quot;9&quot;</span>])+ &gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="语法分析器"><a href="#语法分析器" class="headerlink" title="语法分析器"></a>语法分析器</h3><h4 id="语法介绍"><a href="#语法介绍" class="headerlink" title="语法介绍"></a>语法介绍</h4><ul>
<li><p>java代码块用<code>&#123;&#125;</code>声明</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义java代码块</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">javaCodeDemo</span><span class="params">()</span>:</span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line">&#123;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">        System.out.println(i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>java函数<br>需要使用JAVACODE声明</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">JAVACODE <span class="keyword">void</span> <span class="title">print</span><span class="params">(Token t)</span></span>&#123;</span><br><span class="line">    System.out.println(t);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>条件</p>
</li>
</ul>
<ol>
<li><p>if语句 []</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// if语句</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ifExpr</span><span class="params">()</span>:</span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line">&#123;</span><br><span class="line">    [</span><br><span class="line">        &lt;SELECT&gt;</span><br><span class="line">        &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;if select&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 循环，出现一次</span></span><br><span class="line">    (&lt;SELECT&gt;)?</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>if else 语句 |</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// if - else</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ifElseExpr</span><span class="params">()</span>:</span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line">&#123;</span><br><span class="line">    (</span><br><span class="line">        &lt;SELECT&gt; &#123;System.out.println(<span class="string">&quot;if else select&quot;</span>);&#125;</span><br><span class="line">        |</span><br><span class="line">        &lt;UPDATE&gt;  &#123;System.out.println(<span class="string">&quot;if else update&quot;</span>);&#125;</span><br><span class="line">        |</span><br><span class="line">        &lt;DELETE&gt;  &#123;System.out.println(<span class="string">&quot;if else delete&quot;</span>);&#125;</span><br><span class="line">        |</span><br><span class="line">        &#123;</span><br><span class="line">           System.out.println(<span class="string">&quot;other&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>循环</p>
</li>
</ol>
<ul>
<li><p>while 0~n</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// while 0~n</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">whileExpr</span><span class="params">()</span>:</span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line">&#123;</span><br><span class="line">    (&lt;SELECT&gt;)*</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Start</span><span class="params">()</span> :</span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line">&#123;</span><br><span class="line">    &lt;NUMBER&gt;</span><br><span class="line">    (</span><br><span class="line">        &lt;PLUS&gt;</span><br><span class="line">        &lt;NUMBER&gt;</span><br><span class="line">    )*</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">options &#123;</span><br><span class="line">    STATIC = <span class="keyword">false</span> ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">PARSER_BEGIN(Adder)</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Adder</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> ParseException, TokenMgrError </span>&#123;</span><br><span class="line">            Adder parser = <span class="keyword">new</span> Adder( System.in );</span><br><span class="line">            parser.Start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">PARSER_END(Adder)</span><br><span class="line"></span><br><span class="line">SKIP : &#123; <span class="string">&quot; &quot;</span>| <span class="string">&quot;\n&quot;</span> | <span class="string">&quot;\r&quot;</span> | <span class="string">&quot;\r\n&quot;</span> &#125;</span><br><span class="line"></span><br><span class="line">TOKEN : &#123; &lt; PLUS : <span class="string">&quot;+&quot;</span> &gt; &#125;</span><br><span class="line">TOKEN : &#123; &lt; NUMBER : ([<span class="string">&quot;0&quot;</span>-<span class="string">&quot;9&quot;</span>])+ &gt; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Start</span><span class="params">()</span> :</span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line">&#123;</span><br><span class="line">    &lt;NUMBER&gt;</span><br><span class="line">    (</span><br><span class="line">        &lt;PLUS&gt;</span><br><span class="line">        &lt;NUMBER&gt;</span><br><span class="line">    )*</span><br><span class="line">    &lt;EOF&gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>介绍下生成的一些类:</strong></p>
<ul>
<li>Adder 是语法分析器</li>
<li>TokenMgrError 是一个简单的定义错误的类，它是Throwable类的子类，用于定义在词法分析阶段检测到的错误。</li>
<li>ParseException是另一个定义错误的类。它是Exception 和Throwable的子类，用于定义在语法分析阶段检测到的错误。</li>
<li>Token类是一个用于表示token的类。我们在.jj文件中定义的每一个token（PLUS, NUMBER, or EOF），在Token类中都有对应的一个整数属性来表示，此外每一个token都有名为image的string类型的属性，用来表示token所代表的从输入中获取到的真实值。</li>
<li>SimpleCharStream是一个转接器类，用于把字符传递给语法分析器。<br>AdderConstants是一个接口，里面定义了一些词法分析器和语法分析器中都会用到的常量。AdderTokenManager 是词法分析器。</li>
</ul>
<h4 id="一个更复杂的例子"><a href="#一个更复杂的例子" class="headerlink" title="一个更复杂的例子"></a>一个更复杂的例子</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">PARSER_BEGIN(Calculator)</span><br><span class="line"><span class="keyword">package</span> com.github.aaaaaron.parser.javacc.calc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.* ;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Calculator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Calculator</span><span class="params">(String expr)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>((Reader)(<span class="keyword">new</span> StringReader(expr)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span>   </span>&#123;</span><br><span class="line">       Calculator calc = <span class="keyword">new</span> Calculator(args[<span class="number">0</span>]);</span><br><span class="line">       System.out.println(calc.calc());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PARSER_END(Calculator)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 忽略的字符</span></span><br><span class="line">SKIP:&#123;</span><br><span class="line">    <span class="string">&quot; &quot;</span></span><br><span class="line">    | <span class="string">&quot;\t&quot;</span></span><br><span class="line">    | <span class="string">&quot;\n&quot;</span></span><br><span class="line">    | <span class="string">&quot;\r&quot;</span></span><br><span class="line">    | <span class="string">&quot;\r\n&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 关键字</span></span><br><span class="line">TOKEN:&#123;</span><br><span class="line">    &lt;ADD :<span class="string">&quot;+&quot;</span>&gt;</span><br><span class="line">    | &lt;SUB :<span class="string">&quot;-&quot;</span>&gt;</span><br><span class="line">    | &lt;MUL :<span class="string">&quot;*&quot;</span>&gt;</span><br><span class="line">    | &lt;DIV :<span class="string">&quot;/&quot;</span>&gt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TOKEN : &#123;</span><br><span class="line">    &lt;NUMBER : &lt;DIGITS&gt;</span><br><span class="line">            | &lt;DIGITS&gt; <span class="string">&quot;.&quot;</span> &lt;DIGITS&gt;</span><br><span class="line">            | &lt;DIGITS&gt; <span class="string">&quot;.&quot;</span></span><br><span class="line">            | <span class="string">&quot;.&quot;</span> &lt;DIGITS&gt;&gt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// DIGITS这个并不是token，这意味着在后面生成的Token类中，将不会有DIGITS对应的属性，而在语法分析器中也无法使用DIGITS</span></span><br><span class="line">TOKEN : &#123; &lt; #DIGITS : ([<span class="string">&quot;0&quot;</span>-<span class="string">&quot;9&quot;</span>])+ &gt; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">calc</span><span class="params">()</span>:</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">double</span> value ;</span><br><span class="line">    <span class="keyword">double</span> result = <span class="number">0.0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">  result = mulDiv()</span><br><span class="line">  <span class="comment">// 加减</span></span><br><span class="line">  (</span><br><span class="line">      &lt;ADD&gt;</span><br><span class="line">      value = mulDiv()</span><br><span class="line">      &#123;result += value;&#125;</span><br><span class="line">      |</span><br><span class="line">      &lt;SUB&gt;</span><br><span class="line">      value =mulDiv()</span><br><span class="line">      &#123;result -= value;&#125;</span><br><span class="line">  )*</span><br><span class="line">  &#123;<span class="keyword">return</span> result;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 乘除</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">mulDiv</span><span class="params">()</span>:</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">double</span> value;</span><br><span class="line">    <span class="keyword">double</span> result;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    result = getNumber()</span><br><span class="line">    (</span><br><span class="line">       &lt;MUL&gt;</span><br><span class="line">        value = getNumber()</span><br><span class="line">        &#123;result *= value;&#125;</span><br><span class="line">       |</span><br><span class="line">       &lt;DIV&gt;</span><br><span class="line">       value = getNumber()</span><br><span class="line">       &#123;result /= value;&#125;</span><br><span class="line">    )*</span><br><span class="line">    &#123;<span class="keyword">return</span> result;&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取字符串</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">getNumber</span><span class="params">()</span>:</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">double</span> number;</span><br><span class="line">    Token t;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    t = &lt;NUMBER&gt;</span><br><span class="line">    &#123;number = Double.parseDouble(t.image);</span><br><span class="line">    <span class="keyword">return</span> number;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Calcite"><a href="#Calcite" class="headerlink" title="Calcite"></a>Calcite</h1><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">    SQL-- Parser --&gt;A(SqlNode);</span><br><span class="line">    A-- Validate --&gt;B(SqlNode);</span><br><span class="line">    B-- SqlToRelConverter --&gt; C(RelNode);</span><br><span class="line">    C-- Optimizer --&gt; RelNode;</span><br></pre></td></tr></table></figure>

<ul>
<li>SqlNode: 抽象语法树(AST), 树状结构.</li>
<li>RelNode: 逻辑执行计划节点, 如TableScan, Project, Sort, Join等, 树状结构.<ul>
<li>继承自 RelOptNode, 代表能被优化器进行优化<ul>
<li><code>RelTraitSet#getTraitSet();</code>用来定义逻辑表的物理相关属性(分布/排序)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="SQL-解析阶段（SQL–-gt-SqlNode）"><a href="#SQL-解析阶段（SQL–-gt-SqlNode）" class="headerlink" title="SQL 解析阶段（SQL–&gt;SqlNode）"></a>SQL 解析阶段（SQL–&gt;SqlNode）</h2><p>Calcite 使用 JavaCC 做 SQL 解析，JavaCC 根据 Calcite 中定义的 <a target="_blank" rel="noopener" href="https://github.com/apache/calcite/blob/master/core/src/main/codegen/templates/Parser.jj">Parser.jj</a> 文件，生成一系列的 java 代码，生成的 Java 代码会把 SQL 转换成 AST 的数据结构（这里是 SqlNode 类型）</p>
<blockquote>
<p>与 Javacc 相似的有 ANTLR，JavaCC 中的 jj 文件跟 ANTLR 中的 G4文件类似，Apache Spark 中使用ANTLR</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">String sql = <span class="string">&quot;select * from emps where id = 1 group by locationid order by empno limit 100&quot;</span>;</span><br><span class="line"><span class="comment">//quoting, quotedCasing, unquotedCasing, caseSensitive</span></span><br><span class="line">SqlParser.Config mysqlConfig = SqlParser.configBuilder().setLex(Lex.MYSQL).build();</span><br><span class="line">SqlParser parser = SqlParser.create(sql, mysqlConfig);</span><br><span class="line">SqlNode sqlNode = parser.parseStmt();</span><br></pre></td></tr></table></figure>

<h3 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parseQuery</span><span class="params">()</span> <span class="keyword">throws</span> SqlParseException </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> parser.parseSqlStmtEof();</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable ex) &#123;</span><br><span class="line">    <span class="keyword">throw</span> handleException(ex);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里的 parser 就是 javacc 文件生成的解析类:</p>
<h5 id="生成代码"><a href="#生成代码" class="headerlink" title="生成代码"></a>生成代码</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.sql.parser.impl.SqlParserImpl</span></span><br><span class="line"><span class="comment">//generated from Parser.jj by JavaCC.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parseSqlStmtEof</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> SqlStmtEof();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">public</span> SqlNode <span class="title">SqlStmtEof</span><span class="params">()</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">  SqlNode stmt;</span><br><span class="line">  stmt = SqlStmt();</span><br><span class="line">  jj_consume_token(<span class="number">0</span>);</span><br><span class="line">      &#123;<span class="keyword">if</span> (<span class="keyword">true</span>) <span class="keyword">return</span> stmt;&#125;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">&quot;Missing return statement in function&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">public</span> SqlNode <span class="title">SqlStmt</span><span class="params">()</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">  SqlNode stmt;</span><br><span class="line">  <span class="keyword">if</span> (jj_2_34(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlSetOption(Span.of(), <span class="keyword">null</span>);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_35(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlAlter();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_36(<span class="number">2</span>)) &#123;</span><br><span class="line">    <span class="comment">// select语句</span></span><br><span class="line">    stmt = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_37(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlExplain();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_38(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlDescribe();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_39(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlInsert();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_40(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlDelete();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_41(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlUpdate();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_42(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlMerge();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_43(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlProcedureCall();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    jj_consume_token(-<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ParseException();</span><br><span class="line">  &#125;</span><br><span class="line">      &#123;<span class="keyword">if</span> (<span class="keyword">true</span>) <span class="keyword">return</span> stmt;&#125;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">&quot;Missing return statement in function&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="javacc-定义"><a href="#javacc-定义" class="headerlink" title="javacc 定义"></a>javacc 定义</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Parses an SQL statement.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">SqlNode <span class="title">SqlStmt</span><span class="params">()</span> :</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    SqlNode stmt;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    (</span><br><span class="line">&lt;#-- Add methods to parse additional statements here --&gt;</span><br><span class="line">&lt;#list parser.statementParserMethods as method&gt;</span><br><span class="line">        LOOKAHEAD(<span class="number">2</span>) stmt = $&#123;method&#125;</span><br><span class="line">    |</span><br><span class="line">&lt;/#list&gt;</span><br><span class="line">        stmt = SqlSetOption(Span.of(), <span class="keyword">null</span>)</span><br><span class="line">    |</span><br><span class="line">        stmt = SqlAlter()</span><br><span class="line">    |</span><br><span class="line">&lt;#<span class="keyword">if</span> parser.createStatementParserMethods?size != <span class="number">0</span>&gt;</span><br><span class="line">        stmt = SqlCreate()</span><br><span class="line">    |</span><br><span class="line">&lt;/#<span class="keyword">if</span>&gt;</span><br><span class="line">&lt;#<span class="keyword">if</span> parser.dropStatementParserMethods?size != <span class="number">0</span>&gt;</span><br><span class="line">        stmt = SqlDrop()</span><br><span class="line">    |</span><br><span class="line">&lt;/#<span class="keyword">if</span>&gt;</span><br><span class="line">        stmt = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY) <span class="comment">//select语句</span></span><br><span class="line">    |</span><br><span class="line">        stmt = SqlExplain()</span><br><span class="line">    |</span><br><span class="line">        stmt = SqlDescribe()</span><br><span class="line">    |</span><br><span class="line">        stmt = SqlInsert()</span><br><span class="line">    |</span><br><span class="line">        stmt = SqlDelete()</span><br><span class="line">    |</span><br><span class="line">        stmt = SqlUpdate()</span><br><span class="line">    |</span><br><span class="line">        stmt = SqlMerge()</span><br><span class="line">    |</span><br><span class="line">        stmt = SqlProcedureCall()</span><br><span class="line">    )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> stmt;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Parses a leaf SELECT expression without ORDER BY.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">SqlSelect <span class="title">SqlSelect</span><span class="params">()</span> :</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">final</span> List&lt;SqlLiteral&gt; keywords = <span class="keyword">new</span> ArrayList&lt;SqlLiteral&gt;();</span><br><span class="line">    <span class="keyword">final</span> SqlNodeList keywordList;</span><br><span class="line">    List&lt;SqlNode&gt; selectList;</span><br><span class="line">    <span class="keyword">final</span> SqlNode fromClause;</span><br><span class="line">    <span class="keyword">final</span> SqlNode where;</span><br><span class="line">    <span class="keyword">final</span> SqlNodeList groupBy;</span><br><span class="line">    <span class="keyword">final</span> SqlNode having;</span><br><span class="line">    <span class="keyword">final</span> SqlNodeList windowDecls;</span><br><span class="line">    <span class="keyword">final</span> List&lt;SqlNode&gt; hints = <span class="keyword">new</span> ArrayList&lt;SqlNode&gt;();</span><br><span class="line">    <span class="keyword">final</span> Span s;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    &lt;SELECT&gt;</span><br><span class="line">    &#123;</span><br><span class="line">        s = span();</span><br><span class="line">    &#125;</span><br><span class="line">    [</span><br><span class="line">        &lt;HINT_BEG&gt;</span><br><span class="line">        CommaSepatatedSqlHints(hints)</span><br><span class="line">        &lt;COMMENT_END&gt;</span><br><span class="line">    ]</span><br><span class="line">    SqlSelectKeywords(keywords)</span><br><span class="line">    (</span><br><span class="line">        &lt;STREAM&gt; &#123;</span><br><span class="line">            keywords.add(SqlSelectKeyword.STREAM.symbol(getPos()));</span><br><span class="line">        &#125;</span><br><span class="line">    )?</span><br><span class="line">    (</span><br><span class="line">        &lt;DISTINCT&gt; &#123;</span><br><span class="line">            keywords.add(SqlSelectKeyword.DISTINCT.symbol(getPos()));</span><br><span class="line">        &#125;</span><br><span class="line">    |   &lt;ALL&gt; &#123;</span><br><span class="line">            keywords.add(SqlSelectKeyword.ALL.symbol(getPos()));</span><br><span class="line">        &#125;</span><br><span class="line">    )?</span><br><span class="line">    &#123;</span><br><span class="line">        keywordList = <span class="keyword">new</span> SqlNodeList(keywords, s.addAll(keywords).pos());</span><br><span class="line">    &#125;</span><br><span class="line">    selectList = SelectList()</span><br><span class="line">    (</span><br><span class="line">        &lt;FROM&gt; fromClause = FromClause()</span><br><span class="line">        where = WhereOpt()</span><br><span class="line">        groupBy = GroupByOpt()</span><br><span class="line">        having = HavingOpt()</span><br><span class="line">        windowDecls = WindowOpt()</span><br><span class="line">    )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> SqlSelect(s.end(<span class="keyword">this</span>), keywordList,</span><br><span class="line">            <span class="keyword">new</span> SqlNodeList(selectList, Span.of(selectList).pos()),</span><br><span class="line">            fromClause, where, groupBy, having, windowDecls, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>,</span><br><span class="line">            <span class="keyword">new</span> SqlNodeList(hints, getPos()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Parses an ORDER BY clause.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">SqlNodeList <span class="title">OrderBy</span><span class="params">()</span> :</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    List&lt;SqlNode&gt; list;</span><br><span class="line">    SqlNode e;</span><br><span class="line">    <span class="keyword">final</span> Span s;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    &lt;ORDER&gt; &#123;</span><br><span class="line">        s = span();</span><br><span class="line">    &#125;</span><br><span class="line">    &lt;BY&gt; e = OrderItem() &#123;</span><br><span class="line">        list = startList(e);</span><br><span class="line">    &#125;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> SqlNodeList(list, s.addAll(list).pos());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Parses one list item in an ORDER BY clause.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">SqlNode <span class="title">OrderItem</span><span class="params">()</span> :</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    SqlNode e;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    e = Expression(ExprContext.ACCEPT_SUB_QUERY)</span><br><span class="line">    (</span><br><span class="line">        &lt;ASC&gt;</span><br><span class="line">    |   &lt;DESC&gt; &#123;</span><br><span class="line">            e = SqlStdOperatorTable.DESC.createCall(getPos(), e);</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="生成逻辑执行计划（SqlNode-gt-RelNode）"><a href="#生成逻辑执行计划（SqlNode-gt-RelNode）" class="headerlink" title="生成逻辑执行计划（SqlNode-&gt;RelNode）"></a>生成逻辑执行计划（SqlNode-&gt;RelNode）</h2><p>SqlToRelConverter 中的 <code>convertQuery()</code> 将 SqlNode 转换为 RelRoot, 其实现如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Recursively converts a query to a relational expression.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> query         Query</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> top           Whether this query is the top-level query of the</span></span><br><span class="line"><span class="comment"> *                      statement</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> targetRowType Target row type, or null</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Relational expression</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> RelRoot <span class="title">convertQueryRecursive</span><span class="params">(SqlNode query, <span class="keyword">boolean</span> top,</span></span></span><br><span class="line"><span class="params"><span class="function">    RelDataType targetRowType)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> SqlKind kind = query.getKind();</span><br><span class="line">  <span class="keyword">switch</span> (kind) &#123;</span><br><span class="line">  <span class="keyword">case</span> SELECT:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertSelect((SqlSelect) query, top), kind);</span><br><span class="line">  <span class="keyword">case</span> INSERT:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertInsert((SqlInsert) query), kind);</span><br><span class="line">  <span class="keyword">case</span> DELETE:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertDelete((SqlDelete) query), kind);</span><br><span class="line">  <span class="keyword">case</span> UPDATE:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertUpdate((SqlUpdate) query), kind);</span><br><span class="line">  <span class="keyword">case</span> MERGE:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertMerge((SqlMerge) query), kind);</span><br><span class="line">  <span class="keyword">case</span> UNION:</span><br><span class="line">  <span class="keyword">case</span> INTERSECT:</span><br><span class="line">  <span class="keyword">case</span> EXCEPT:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertSetOp((SqlCall) query), kind);</span><br><span class="line">  <span class="keyword">case</span> WITH:</span><br><span class="line">    <span class="keyword">return</span> convertWith((SqlWith) query, top);</span><br><span class="line">  <span class="keyword">case</span> VALUES:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertValues((SqlCall) query, targetRowType), kind);</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">&quot;not a query: &quot;</span> + query);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">convertSelectImpl</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">final</span> Blackboard bb,</span></span></span><br><span class="line"><span class="params"><span class="function">    SqlSelect select)</span> </span>&#123;</span><br><span class="line">  convertFrom(</span><br><span class="line">      bb,</span><br><span class="line">      select.getFrom());</span><br><span class="line">  convertWhere(</span><br><span class="line">      bb,</span><br><span class="line">      select.getWhere());</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> List&lt;SqlNode&gt; orderExprList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="keyword">final</span> List&lt;RelFieldCollation&gt; collationList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  gatherOrderExprs(</span><br><span class="line">      bb,</span><br><span class="line">      select,</span><br><span class="line">      select.getOrderList(),</span><br><span class="line">      orderExprList,</span><br><span class="line">      collationList);</span><br><span class="line">  <span class="keyword">final</span> RelCollation collation =</span><br><span class="line">      cluster.traitSet().canonize(RelCollations.of(collationList));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (validator.isAggregate(select)) &#123;</span><br><span class="line">    convertAgg(</span><br><span class="line">        bb,</span><br><span class="line">        select,</span><br><span class="line">        orderExprList);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    convertSelectList(</span><br><span class="line">        bb,</span><br><span class="line">        select,</span><br><span class="line">   </span><br><span class="line">     orderExprList);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (select.isDistinct()) &#123;</span><br><span class="line">    distinctify(bb, <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  convertOrder(</span><br><span class="line">      select, bb, collation, orderExprList, select.getOffset(),</span><br><span class="line">      select.getFetch());</span><br><span class="line"></span><br><span class="line">  bb.setRoot(bb.root, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>最后生成的逻辑执行计划</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span>: </span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> EMPNO, GENDER, NAME</span><br><span class="line"><span class="keyword">FROM</span> EMPS</span><br><span class="line"><span class="keyword">WHERE</span> GENDER <span class="operator">=</span> <span class="string">&#x27;F&#x27;</span> <span class="keyword">AND</span> EMPNO <span class="operator">&gt;</span> <span class="number">125</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> NAME </span><br><span class="line">LIMIT <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span></span><br><span class="line"></span><br><span class="line">LogicalSort(sort0<span class="operator">=</span>[$<span class="number">2</span>], dir0<span class="operator">=</span>[<span class="keyword">ASC</span>], <span class="keyword">fetch</span><span class="operator">=</span>[<span class="number">10</span>])</span><br><span class="line">  LogicalProject(EMPNO<span class="operator">=</span>[$<span class="number">0</span>], GENDER<span class="operator">=</span>[$<span class="number">3</span>], NAME<span class="operator">=</span>[$<span class="number">1</span>])</span><br><span class="line">    LogicalFilter(<span class="keyword">condition</span><span class="operator">=</span>[<span class="keyword">AND</span>(<span class="operator">=</span>($<span class="number">3</span>, <span class="string">&#x27;F&#x27;</span>), <span class="operator">&gt;</span>($<span class="number">0</span>, <span class="number">125</span>))])</span><br><span class="line">      LogicalTableScan(<span class="keyword">table</span><span class="operator">=</span>[[SALES, EMPS]])</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2020/02/07/Paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/02/07/Paper/" class="post-title-link" itemprop="url">Paper</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-02-07 20:49:53" itemprop="dateCreated datePublished" datetime="2020-02-07T20:49:53+08:00">2020-02-07</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-07 21:03:11" itemprop="dateModified" datetime="2021-05-07T21:03:11+08:00">2021-05-07</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="数据库相关"><a href="#数据库相关" class="headerlink" title="数据库相关"></a>数据库相关</h1><p>Implementing Data Cubes Efficiently (Lattices)</p>
<p>Optimizing queries using materialized views: A practical, scalable solution</p>
<p>The Cascades Framework for Query Optimization</p>
<p>An Overview of Query Optimization in Relational Systems</p>
<p>Orca: A Modular Query Optimizer Architecture for Big Data</p>
<p>Efficiency in the columbia database query optimizer</p>
<p>Counting, Enumerating, and Sampling of Execution Plans in a Cost-Based Query Optimizer</p>
<p>Query Optimization in Microsoft SQL Server PDW</p>
<p>Incorporating Partitioning and Parallel Plans into the SCOPE Optimizer</p>
<p>The MemSQL Query Optimizer: A modern optimizer for real-time analytics in a distributed database</p>
<p>F1 Query: Declarative Querying at Scale</p>
<p>Optimization of Common Table Expressions in MPP Database Systems</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2020/02/07/Kylin-query-process/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/02/07/Kylin-query-process/" class="post-title-link" itemprop="url">Kylin 查询的基本流程</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-02-07 20:49:53" itemprop="dateCreated datePublished" datetime="2020-02-07T20:49:53+08:00">2020-02-07</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 00:38:25" itemprop="dateModified" datetime="2021-05-08T00:38:25+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="1-查询入口"><a href="#1-查询入口" class="headerlink" title="1.查询入口"></a>1.查询入口</h1><p>Kylin server 接到用户端的查询后, 会使用 JDBC 与 Calcite 连接, 并告诉 Calcite 要查询的表的 schema 信息, 并且注册一系列转化的 RULE 到 calcite.</p>
<p>Calcite 作为一个通用的 SQL 框架, 他出发点是希望能为不同计算存储引擎提供统一的 SQL 查询引擎, 它本身不知道也不关心底层的 storage, 所以使用 Calcite 框架, 需要注意两点:</p>
<ol>
<li>schema 信息(通过 JDBC 的 Properties 传入)</li>
<li>TableScan 如何读取数据(需要框架使用方自己实现, 后面讲代码生成时会提到)</li>
</ol>
<p><img src="/2020/02/07/Kylin-query-process/image-20200207232940630-20210507203629200.png" alt="image-20200207232940630"></p>
<p><img src="/2020/02/07/Kylin-query-process/image-20200207232956828-20210507203634094.png" alt="image-20200207232956828"></p>
<h1 id="2-定义规则-Hook-到-Calcite"><a href="#2-定义规则-Hook-到-Calcite" class="headerlink" title="2.定义规则 Hook 到 Calcite"></a>2.定义规则 Hook 到 Calcite</h1><p>Kylin 使用上面注册进去的这些 Rule 把各个 Calcite 逻辑执行计划节点转换成 Kylin 的逻辑执行计划节点(OLAP*Rel)</p>
<ol>
<li>OLAPRel 也继承自RelNode(Calcite 的逻辑执行计划), 简单来说每个OLAP*Rel只是包了下 Calcite 的逻辑执行计划节点, 只是为了多加一些自己的抽象方法, 需要各个子类去实现, 后面会详细介绍</li>
<li>这些抽象方法是用来完成[查询信息收集/选 Cube/rewrite 执行计划/生成具体的物理执行计划]</li>
</ol>
<p><img src="/2020/02/07/Kylin-query-process/image-20200207233022958.png" alt="image-20200207233022958"></p>
<ol>
<li>可以看到这个 Rule 当遇到 LogicalFilter 时, 会把它转换成 OLAPFilterRel, 他们都是 RelNode 的子类.</li>
</ol>
<p><img src="/2020/02/07/Kylin-query-process/image-20200207233039360.png" alt="image-20200207233039360"></p>
<ol>
<li>这里有一个在 calcite 代码里 Hack 的点, 就是所有生成的查询计划树, 头结点一定会是 OLAPToEnumerableConverter, 如果不是, 会抛错.</li>
</ol>
<p><img src="/2020/02/07/Kylin-query-process/image-20200207233054038.png" alt="image-20200207233054038"></p>
<p>这个是串起整个代码流程的关键位置, 下面会详细讲解:</p>
<p><strong>OLAPToEnumerableConverter.implement</strong></p>
<p><img src="/2020/02/07/Kylin-query-process/image-20200207233114376.png" alt="image-20200207233114376"></p>
<h1 id="3-切分-OLAPContext与选择-Cube"><a href="#3-切分-OLAPContext与选择-Cube" class="headerlink" title="3.切分 OLAPContext与选择 Cube"></a>3.切分 OLAPContext与选择 Cube</h1><p>OLAPContext 与 Cube 一一对应, 它定义了很多属性, 帮助我们定位一个 OLAPContext 是否有 Cube 与其对应.</p>
<p>所以当我们把一颗执行计划树分解成一个个 OLAPContext, 并且找到这一个个 OLAPContext 有哪些对应的 cube 的时候, 我们就知道了这个查询中, 哪些部分能够用 Cube 来回答, 哪些部分不能.</p>
<p><strong>OLAPContext记录的信息:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Aggregations</span><br><span class="line">Filter columns</span><br><span class="line">Group by columns</span><br><span class="line">Joins</span><br><span class="line">Tables</span><br><span class="line"> ...</span><br></pre></td></tr></table></figure>

<h2 id="3-1-如何切分-OLAPContext"><a href="#3-1-如何切分-OLAPContext" class="headerlink" title="3.1 如何切分 OLAPContext"></a>3.1 如何切分 OLAPContext</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">OLAPRel.OLAPImplementor olapImplementor = new OLAPRel.OLAPImplementor();</span><br><span class="line">olapImplementor.visitChild(getInput(), this);</span><br></pre></td></tr></table></figure>

<p>OLAPRel 接口有个方法: implementOLAP,之前通过 Rule 转化成的各种 OLAP*Rel 都实现了这个方法, 从根节点OLAPToEnumerableConverter 开始, 使用 Visitor 模式访问, 当如果遇到 aggregation 就需要划分一个新的 OLAPContext.</p>
<p>对于 case1 , 在 Visitor 模式下, 会访问根节点 agg, 这需要先访问它的⼦子节点 ﬁlter, 类似的, 访问 ﬁlter 节点需要先访问其⼦子节点 join节点 …, 当所 有的⼦子节点全部访问完重新回到 agg时, 这时候需要划分出⼀一个 OLAPContext.同理理, case2 和 case3 会 被拆成两个 OLAPContext.</p>
<p>注意, case2 虽然有两个 OLAPContext, 但是左边那个 OLAPContext 无法对应一个 cube, 真正 cube 能加速的部分是右下角红色的那个OLAPContext, 同理 case3, 上面白色的两个算子都需要现算.</p>
<p><img src="/2020/02/07/Kylin-query-process/image-20200207233136751.png" alt="image-20200207233136751"></p>
<h2 id="3-2-选择-cube"><a href="#3-2-选择-cube" class="headerlink" title="3.2 选择 cube"></a>3.2 选择 cube</h2><p>根据 context 选择 Cube 的逻辑在: RealizationChooser.selectRealization(contexts);其实现比较简单, 就是遍历得到的所有 contexts, 再遍历cube, 看看其度量和维度是不是和我们提前定义的 cube 中的是一致的, 如果一致, 则表示这个 OC 可以用 cube 来回答.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Collection unmatchedDimensions = unmatchedDimensions(dimensionColumns, cube);</span><br><span class="line">Collection unmatchedAggregations = unmatchedAggregations(aggrFunctions, cube);</span><br></pre></td></tr></table></figure>

<h1 id="4-Rewrite-执行计划"><a href="#4-Rewrite-执行计划" class="headerlink" title="4. Rewrite 执行计划"></a>4. Rewrite 执行计划</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// rewrite query if necessary</span></span><br><span class="line">OLAPRel.RewriteImplementor rewriteImplementor = <span class="keyword">new</span> OLAPRel.RewriteImplementor();</span><br><span class="line">rewriteImplementor.visitChild(<span class="keyword">this</span>, getInput());</span><br></pre></td></tr></table></figure>

<p>经过上面那步, Kylin 已经对用户的查询选出 cube 去回答了, 但是用户 SQL的语义到这里还并没有改变.</p>
<p>假设用户想查商品的 PV: SELECT item, COUNT(user_id) FROM stock GROUP BY item, 定义的 Cube 维度为 item, date, 度量为 COUNT(user_id), 到这一步, 执行计划的 agg 的节点里, 还是一个count() 的函数, 如果我们想用已经预计算过后的 cube 来回答这个SQL, 就会导致问题:</p>
<ol>
<li>根本没有 user_id 这个列, 预计算过的 Cube 有三个列: user/date 以及一个 M_开头的度量列, 比如 M_C</li>
<li>即使是对这个度量列求 COUNT, 结果也不对, 因为这是预计算过后的条数, 最终改写的 SQL 应该是 SELECT item, SUM(M_C) FROM cuboid GROUP BY item, 为什么还要保留这个 group by 呢? 对于上文定义的 cube, 不是精确回答这句查询, 还需要从 item, date 聚合出 item_name 来, 但是相比较于从源表聚合, input 以及少了很多了</li>
</ol>
<p>同理, rewrite 这步也是一个 visitor 模式, 每个算子都实现了 implementRewrite, 由每个算子自行决定要不要 rewrite 执行计划, 像 filter/limit 等算子, 是啥都不干的.</p>
<h1 id="5-生成物理执行计划"><a href="#5-生成物理执行计划" class="headerlink" title="5.生成物理执行计划"></a>5.生成物理执行计划</h1><p><strong>注意: 这是老的实现, 新架构用 Spark 进行计算, 不走这边了</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// implement as EnumerableRel</span></span><br><span class="line">OLAPRel.JavaImplementor impl = <span class="keyword">new</span> OLAPRel.JavaImplementor(enumImplementor);</span><br><span class="line">EnumerableRel enumerable = impl.visitChild((OLAPRel) getInput());</span><br></pre></td></tr></table></figure>

<p>同上面两步一样, 这里同样是一个 visitor 方法: EnumerableRel implementEnumerable(List inputs), 每个 OLAP*Rel 都实现了这个方法, 将每一个 <code>OLAP*Rel</code> 根据本次查询的参数, 生成 Calcite 自身的 EnumerableXXX 算子执行, 即逻辑节点转为物理节点.</p>
<p>这里用到的 Calcite 的 Enumerable 的机制, 这是 Calcite 默认的物理算子, 可以生成执行的代码. 等于说刚刚从正常的 Calite 逻辑执行计划转到 Kylin 的逻辑执行计划, 再进行改写, 都是 Kylin 为了预计算叉出去的, 现在又叉回到了 Calcite 本来的轨道上来, 但是这时候你的执行计划已经是改变过后的了, 对应生成的物理执行计划也可以用于查询 Cube 的数据.</p>
<p><strong>OLAPSortRel#implementEnumerable</strong></p>
<p><img src="/2020/02/07/Kylin-query-process/image-20200207233214957.png" alt="image-20200207233214957"></p>
<p><strong>注意</strong></p>
<p>特别需要注意的是: OLAPTableScan 这个算子很特殊, 他们没有转回到 Calcite, 而是自己实现了EnumerableRel, 直接返回 this.</p>
<p>原因是因为这个算子是真正读取数据的地方, Kylin 需要在这个算子中, 接入 Cube 数据, 由于涉及到一些代码生成的东西, 所以下节介绍.</p>
<h1 id="6-物理执行计划生成代码真正执行"><a href="#6-物理执行计划生成代码真正执行" class="headerlink" title="6.物理执行计划生成代码真正执行"></a>6.物理执行计划生成代码真正执行</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">OLAPRel.JavaImplementor impl = <span class="keyword">new</span> OLAPRel.JavaImplementor(enumImplementor);</span><br><span class="line"><span class="keyword">this</span>.replaceInput(<span class="number">0</span>, enumerable);</span><br><span class="line"><span class="keyword">return</span> impl.visitChild(<span class="keyword">this</span>, <span class="number">0</span>, enumerable, pref);</span><br></pre></td></tr></table></figure>

<p>同样还是 visitor 方法: Result implement(EnumerableRelImplementor implementor, Prefer pref);,每个 EnumerableRel 都实现了这个方法</p>
<p>截图是 EnumerableSort 生成代码的使用的代码, 用了 linq4j, 仿照的是 C# 的 linq, 也是 Calcite 作者 Julian 的大作, 大家有兴趣的可以自行研究下.</p>
<p><strong>EnumerableSort#implement</strong></p>
<p><img src="/2020/02/07/Kylin-query-process/image-20200207233235160.png" alt="image-20200207233235160"></p>
<p>上文提到使用 Calcite 框架, 需要注意的第二点: TableScan 如何读取数据 接下来介绍, 这个很重要:</p>
<h2 id="6-1-TableScan-如何读取数据"><a href="#6-1-TableScan-如何读取数据" class="headerlink" title="6.1 TableScan 如何读取数据"></a>6.1 TableScan 如何读取数据</h2><p>上文提到 OLAPTableScan 没有转换回 Calcite 的物理执行计划, 而是自己实现了EnumerableRel 的接口,也就说他自己实现了 implement 方法来读取数据:</p>
<p><strong>OLAPTableScan#implement</strong></p>
<p><img src="/2020/02/07/Kylin-query-process/image-20200207233314381.png" alt="image-20200207233314381"></p>
<p>我们只需要关注 execFunction, 这个是最后生产代码取数据会执行的函数, 如果击中cube, 会走到 executeOLAPQuery, 这个方法在:OLAPTable#executeOLAPQuery</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Enumerable <span class="title">executeOLAPQuery</span><span class="params">(DataContext optiqContext, <span class="keyword">int</span> ctxSeq)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> OLAPQuery(optiqContext, EnumeratorTypeEnum.OLAP, ctxSeq);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>一路追下去, 会调用到 OLAPEnumerator.queryStorage, 这里就是具体 query storage 的地方:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Iterator <span class="title">queryStorage</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  logger.debug(<span class="string">&quot;query storage...&quot;</span>);</span><br><span class="line">  <span class="comment">// query storage engine</span></span><br><span class="line">  IStorageQuery storageEngine = StorageFactory.createQuery(olapContext.realization);</span><br><span class="line">  ITupleIterator iterator = storageEngine.search(olapContext.storageContext, sqlDigest, olapContext.returnTupleInfo);</span><br><span class="line">  <span class="keyword">return</span> iterator;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>再追下去就是具体从 Hbase 扫数据等流程了, 这里就先不看了, 感兴趣的同学可以自己看, 欢迎随时交流</p>
<h2 id="6-2-生成代码摘录"><a href="#6-2-生成代码摘录" class="headerlink" title="6.2 生成代码摘录"></a>6.2 生成代码摘录</h2><p>摘一个生成的代码, 注意这个不是 spark 的全阶段代码生成, 是一个算子自己的一份代码, 还是传统的火山模型, 所以还会有迭代器的开销.</p>
<p>整个计算过程迭代的读取指定 cuboid 数据，并执行相应的计算逻辑，是一个基于内存的单机计算过程(calcite 的计算是单机的! 大数据时代你敢信?).</p>
<p>可以看到整个数据是从 executeOLAPQuery 读上来, 假设后面有了新的 storage, 我们只需要做在 executeOLAPQuery 中就可以了.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">*<span class="comment">// _inputEnumerable 为 OLAPQuery 类型，OLAPQuery*</span></span><br><span class="line"><span class="keyword">final</span> org.apache.calcite.linq4j.Enumerable _inputEnumerable = ((org.apache.kylin.query.schema.OLAPTable) root.getRootSchema().getSubSchema(<span class="string">&quot;DEFAULT&quot;</span>).getTable(<span class="string">&quot;KYLIN_SALES&quot;</span>)).executeOLAPQuery(root, <span class="number">0</span>);</span><br><span class="line"><span class="keyword">final</span> org.apache.calcite.linq4j.AbstractEnumerable child = <span class="keyword">new</span> org.apache.calcite.linq4j.AbstractEnumerable()&#123;</span><br><span class="line"> <span class="keyword">public</span> org.apache.calcite.linq4j.<span class="function">Enumerator <span class="title">enumerator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> org.apache.calcite.linq4j.Enumerator()&#123;</span><br><span class="line">    *<span class="comment">// 类型，OLAPQuery.enumerator() 得到的 inputEnumerator 为 OLAPEnumerator 类型*</span></span><br><span class="line">    *<span class="comment">// inputEnumerator 会调用 StorageEngine 去 HBase 中查询指定 cube、指定 cuboid（及可能的 filter 下推）数据*</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> org.apache.calcite.linq4j.Enumerator inputEnumerator = _inputEnumerable.enumerator();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reset</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     inputEnumerator.reset();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">moveNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     <span class="keyword">while</span> (inputEnumerator.moveNext()) &#123;</span><br><span class="line">      <span class="keyword">final</span> Integer inp4_ = (Integer) ((Object[]) inputEnumerator.current())[<span class="number">4</span>];</span><br><span class="line">      <span class="keyword">if</span> (inp4_ != <span class="keyword">null</span> &amp;&amp; inp4_.intValue() != <span class="number">1000</span>) &#123;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">      &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     inputEnumerator.close();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">current</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     <span class="keyword">final</span> Object[] current = (Object[]) inputEnumerator.current();</span><br><span class="line">     <span class="keyword">return</span> <span class="keyword">new</span> Object[] &#123;</span><br><span class="line">       current[<span class="number">0</span>],</span><br><span class="line">       current[<span class="number">5</span>],</span><br><span class="line">       current[<span class="number">13</span>],</span><br><span class="line">       current[<span class="number">11</span>],</span><br><span class="line">       current[<span class="number">10</span>]&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">return</span> child.groupBy(<span class="keyword">new</span> org.apache.calcite.linq4j.function.Function1() &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Long <span class="title">apply</span><span class="params">(Object[] a0)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> (Long) a0[<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">apply</span><span class="params">(Object a0)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> apply(</span><br><span class="line">    (Object[]) a0);</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> , <span class="keyword">new</span> org.apache.calcite.linq4j.function.Function0() &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">apply</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   java.math.BigDecimal a0s0;</span><br><span class="line">   <span class="keyword">boolean</span> a0s1;</span><br><span class="line">   a0s1 = <span class="keyword">false</span>;</span><br><span class="line">   a0s0 = <span class="keyword">new</span> java.math.BigDecimal(<span class="number">0L</span>);</span><br><span class="line">   <span class="keyword">long</span> a1s0;</span><br><span class="line">   a1s0 = <span class="number">0</span>;</span><br><span class="line">   Record3_0 record0;</span><br><span class="line">   record0 = <span class="keyword">new</span> Record3_0();</span><br><span class="line">   record0.f0 = a0s0;</span><br><span class="line">   record0.f1 = a0s1;</span><br><span class="line">   record0.f2 = a1s0;</span><br><span class="line">   <span class="keyword">return</span> record0;</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> , <span class="keyword">new</span> org.apache.calcite.linq4j.function.Function2() &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Record3_0 <span class="title">apply</span><span class="params">(Record3_0 acc, Object[] in)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">final</span> java.math.BigDecimal inp4_ = in[<span class="number">4</span>] == <span class="keyword">null</span> ? (java.math.BigDecimal) <span class="keyword">null</span> : org.apache.calcite.runtime.SqlFunctions.toBigDecimal(in[<span class="number">4</span>]);</span><br><span class="line">   <span class="keyword">if</span> (inp4_ != <span class="keyword">null</span>) &#123;</span><br><span class="line">    acc.f1 = <span class="keyword">true</span>;</span><br><span class="line">    acc.f0 = acc.f0.add(inp4_);</span><br><span class="line">   &#125;</span><br><span class="line">   acc.f2 = acc.f2 + org.apache.calcite.runtime.SqlFunctions.toLong(in[<span class="number">3</span>]);</span><br><span class="line">   <span class="keyword">return</span> acc;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Record3_0 <span class="title">apply</span><span class="params">(Object acc, Object in)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> apply(</span><br><span class="line">    (Record3_0) acc,</span><br><span class="line">    (Object[]) in);</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> , <span class="keyword">new</span> org.apache.calcite.linq4j.function.Function2() &#123;</span><br><span class="line">  <span class="keyword">public</span> Object[] apply(Long key, Record3_0 acc) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="keyword">new</span> Object[] &#123;</span><br><span class="line">     key,</span><br><span class="line">     acc.f1 ? acc.f0 : (java.math.BigDecimal) <span class="keyword">null</span>,</span><br><span class="line">     acc.f2&#125;;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">public</span> Object[] apply(Object key, Object acc) &#123;</span><br><span class="line">   <span class="keyword">return</span> apply(</span><br><span class="line">    (Long) key,</span><br><span class="line">    (Record3_0) acc);</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> ).orderBy(<span class="keyword">new</span> org.apache.calcite.linq4j.function.Function1() &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Long <span class="title">apply</span><span class="params">(Object[] v)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> (Long) v[<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">apply</span><span class="params">(Object v)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> apply(</span><br><span class="line">    (Object[]) v);</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> , org.apache.calcite.linq4j.function.Functions.nullsComparator(<span class="keyword">false</span>, <span class="keyword">false</span>)).take(<span class="number">10</span>);</span><br></pre></td></tr></table></figure>

<h1 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/yu616568/article/details/50838504">Kylin执行查询流程分析</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/12/31/Spark-Bucketing-Deep-Dive/" class="post-title-link" itemprop="url">Spark bucketing Deep Dive</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-12-31 17:10:27" itemprop="dateCreated datePublished" datetime="2019-12-31T17:10:27+08:00">2019-12-31</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:48:40" itemprop="dateModified" datetime="2021-05-08T11:48:40+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="0-开篇"><a href="#0-开篇" class="headerlink" title="0. 开篇"></a>0. 开篇</h1><p>Spark 的 bucket 原理上其实和 repartition 非常相似(其实对数据的操作都是一样的), 但是 Spark 的 repartition 是用来调整 Dataframe 的分区数, 而 bucketing 机制相比, 更多了以下的功能:</p>
<ol>
<li>当有点查的时候, 可以 pruning 掉不必要的文件.</li>
<li>当 join 的双边都有 bucketBy 且满足一定条件之后, 可以进行 bucket join, 极大的优化 join 大-大表 join 性能(能优化掉 shuffle, 这个真的是大杀器).</li>
</ol>
<p>以下的文章讲先介绍 bucket 的原理, 然后具体展开上面的这两点优化, 最后会讲下 bucketing 机制存在的问题.</p>
<h1 id="1-基础"><a href="#1-基础" class="headerlink" title="1. 基础"></a>1. 基础</h1><p>bucketing 与 repartition 都是对数据里每条记录通过一个 Hash 函数计算 key(<code>Murmur3Hash</code>)得到一个值, 相同值的记录放到同一个分片中去. </p>
<p>bucketBy 的写入比较特殊, 不能直接 write.parquet, 因为需要记录一些信息到元数据信息, 在我们自己测试的时候, 可以这样写: <code>df.write.format(&quot;parquet&quot;).option(&quot;path&quot;, &quot;/tmp/bueket&quot;).bucketBy(3, &quot;id&quot;).saveAsTable(&quot;tbl&quot;)</code>, 我们 buketBy 的列是 id, 分了三个桶, 最后产生出来的文件会小于等于3个(如果 id 只有一个值, 只会有一个文件). 如下, 文件名中<code>_</code>后的, 就是每个文件的 buckId(也是里面记录的 hash 值).</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00000.c000.snappy.parquet</span><br><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00001.c000.snappy.parquet</span><br><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00002.c000.snappy.parquet</span><br></pre></td></tr></table></figure>

<p>写入的流程比较杂, 后续会专门讲下, 文末贴了两张调用 debug 的图, 感兴趣的读者可以自己去追下, 虽然流程很长, 但是代码还是很简单的.</p>
<p>这里提下 buckId 是怎么加到文件上的: 在 <code>DynamicPartitionDataWriter#newOutputWriter</code> 中:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 所以可以看到最多五位数的 bucketId</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bucketIdToString</span></span>(id: <span class="type">Int</span>): <span class="type">String</span> = <span class="string">f&quot;_<span class="subst">$id</span>%05d&quot;</span></span><br><span class="line"><span class="keyword">val</span> bucketIdStr = bucketId.map(<span class="type">BucketingUtils</span>.bucketIdToString).getOrElse(<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>提一句, 其实如果你是 repartition 了之后存下来的, <code>part-</code> 后的数字也就是 hash 之后的值, 这个 task attempt ID 是之前 execute task 时传入的 sparkPartitionId, 代码在 <code>FileFormatWriter#executeTask</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getFilename</span></span>(taskContext: <span class="type">TaskAttemptContext</span>, ext: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> split = taskContext.getTaskAttemptID.getTaskID.getId</span><br><span class="line">  <span class="string">f&quot;part-<span class="subst">$split</span>%05d-<span class="subst">$jobId</span><span class="subst">$ext</span>&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="2-优化-Bucket-Pruning"><a href="#2-优化-Bucket-Pruning" class="headerlink" title="2. 优化: Bucket Pruning"></a>2. 优化: Bucket Pruning</h1><p>对于点查, 还是上面 bucketBy id 的例子, 例如 id=5, 可以把id=5当做原始表中一条记录, 同样的我们可以计算出它的 hash 值, 得到它的 bucket ID, 那么我们只要扫这个 bucket ID 文件就可以了, 因为其他 bucket ID 的文件里肯定没有5这个元素.</p>
<h2 id="2-1-Strategy-部分"><a href="#2-1-Strategy-部分" class="headerlink" title="2.1 Strategy 部分"></a>2.1 Strategy 部分</h2><p>先简单带过下 Spark read parquet 的流程, 想了解更多的可以参考这篇<a href="https://aaaaaaron.github.io/2018/11/01/Spark-Read-Deep-Dive/">Spark Read Deep Dive
</a>. </p>
<p>Spark 具体读取底层数据文件的 <code>SparkStrategy</code> 叫做 <code>FileSourceStrategy</code>, 在其 <code>apply</code> 方法中我们可以看到他需要一个 <code>LogicalRelation</code> 来触发到 apply 方法中的各个逻辑: <code>l @ LogicalRelation(fsRelation: HadoopFsRelation, _, table, _)) =&gt;</code>. </p>
<p><code>LogicalRelation</code> 中最重要的是<code>HadoopFsRelation</code>, 我们也可以构造自己的 <code> HadoopFsRelation</code> 传入, 从而生成 DataFrame, 这里不展开:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseRelationToDataFrame</span></span>(baseRelation: <span class="type">BaseRelation</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="type">Dataset</span>.ofRows(self, <span class="type">LogicalRelation</span>(baseRelation))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>提一句, Spark 的 <code>partitionBy</code> 的 pruning 通过 FileIndex, FileIndex 记录在 <code>HadoopFsRelation</code> 一路传到 <code>FileSourceScanExec</code>, <code>FileSourceScanExec</code> 里调用 <code>FileIndex#listFiles</code> pruning 文件, 但是 bucket 并不走 FileIndex 这一套, 事实上我觉得这两个逻辑是类似, 不知道为啥 Spark 不实现在一起, 有知道同学可以说下. </p>
<p>在 <code>FileSourceStrategy#apply</code> 中会对于可以进行 bucket pruning 的情况(<code>bucketColumnNames.length == 1 &amp;&amp; numBuckets &gt; 1</code>), 会传给<code>FileSourceScanExec</code>一个 <code>bucketSet</code>, 它是一个bitset, 通过这个 <code>bucketSet</code> 我们就能知道有哪些文件被选中了(010就代表第二个文件被选中了, 这个用法还是有点装逼). </p>
<p>下面来看看这个 <code>bucketSet</code> 是如何返回的, 代码如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getExpressionBuckets</span></span>(</span><br><span class="line">    expr: <span class="type">Expression</span>,</span><br><span class="line">    bucketColumnName: <span class="type">String</span>,</span><br><span class="line">    numBuckets: <span class="type">Int</span>): <span class="type">BitSet</span> = &#123;</span><br><span class="line">  ...</span><br><span class="line">  expr <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">Equality</span>(a: <span class="type">Attribute</span>, <span class="type">Literal</span>(v, _)) <span class="keyword">if</span> a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromValue(a, v)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">In</span>(a: <span class="type">Attribute</span>, list)</span><br><span class="line">      <span class="keyword">if</span> list.forall(_.isInstanceOf[<span class="type">Literal</span>]) &amp;&amp; a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromIterable(a, list.map(e =&gt; e.eval(<span class="type">EmptyRow</span>)))</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">InSet</span>(a: <span class="type">Attribute</span>, hset)</span><br><span class="line">      <span class="keyword">if</span> hset.forall(_.isInstanceOf[<span class="type">Literal</span>]) &amp;&amp; a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromIterable(a, hset.map(e =&gt; expressions.<span class="type">Literal</span>(e).eval(<span class="type">EmptyRow</span>)))</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">IsNull</span>(a: <span class="type">Attribute</span>) <span class="keyword">if</span> a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromValue(a, <span class="literal">null</span>)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">And</span>(left, right) =&gt;</span><br><span class="line">      getExpressionBuckets(left, bucketColumnName, numBuckets) &amp;</span><br><span class="line">        getExpressionBuckets(right, bucketColumnName, numBuckets)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">Or</span>(left, right) =&gt;</span><br><span class="line">      getExpressionBuckets(left, bucketColumnName, numBuckets) |</span><br><span class="line">      getExpressionBuckets(right, bucketColumnName, numBuckets)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="keyword">val</span> matchedBuckets = <span class="keyword">new</span> <span class="type">BitSet</span>(numBuckets)</span><br><span class="line">      matchedBuckets.setUntil(numBuckets)</span><br><span class="line">      matchedBuckets</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>最主要的逻辑在 <code>FileSourceStrategy#getExpressionBuckets</code> 中, 可以看到 bucket 的 pruning 只支持 <code>Equality, In, InSet, IsNull</code> (And/Or 也支持, 只不过 And/Or 的 left/right 也必须是前面的类型), 其他情况是不支持 pruning 的, 直接返回所有 buckets.</p>
<p>我们来看 Equality 情况的处理, 调用了 <code>getBucketIdFromValue</code>, 里面逻辑是使用 HashPartitioning 求出 filter 里 literal 的 hash 值. 直接把这个 hash 值写入bitset 就是 最后返回的 <code>bucketSet</code> (这里还搞了 InternalRow/UnsafeProjection, 主要是为了处理各个类型的 value)</p>
<p>这里需要注意的一点是, 如果你的 filter 是 <code>cast(id as string)=&#39;1&#39;</code>, 或者等号右边的不是一个 lit, 是没法做 pruning 的.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Given bucketColumn, numBuckets and value, returns the corresponding bucketId</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBucketIdFromValue</span></span>(bucketColumn: <span class="type">Attribute</span>, numBuckets: <span class="type">Int</span>, value: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> mutableInternalRow = <span class="keyword">new</span> <span class="type">SpecificInternalRow</span>(<span class="type">Seq</span>(bucketColumn.dataType))</span><br><span class="line">  mutableInternalRow.update(<span class="number">0</span>, value)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> bucketIdGenerator = <span class="type">UnsafeProjection</span>.create(</span><br><span class="line">    <span class="type">HashPartitioning</span>(<span class="type">Seq</span>(bucketColumn), numBuckets).partitionIdExpression :: <span class="type">Nil</span>,</span><br><span class="line">    bucketColumn :: <span class="type">Nil</span>)</span><br><span class="line">  bucketIdGenerator(mutableInternalRow).getInt(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>PS. 之前我们仿照这个 bucket pruning 写了一个 file index 的时候, 还踩了一个坑, 当时觉得 <code>not</code> 也是支持的, 本来是选中那个文件, not 的话就取反一下, 变成 pruning 掉那个文件. 但是其实是有问题的, 事实上, 所有的 pruning, 只要不是精确到每个值都做 index 的, not 的情况都不能支持, 举例来说, a != 5, 但是 a 可能等于除5之外的任何值, 你把包含5的文件去掉了, 但是这个文件里除了5的记录, 还有其他值的记录.</p>
<h2 id="2-2-Exec-部分"><a href="#2-2-Exec-部分" class="headerlink" title="2.2 Exec 部分"></a>2.2 Exec 部分</h2><p>Spark 用 Strategy 来构造 Exec 的, <code>FileSourceStrategy</code> 用来构造 <code>FileSourceScanExec</code></p>
<p>在第一部分中, 我们传入了 <code>bucketSet</code> 给 <code>FileSourceScanExec</code>, 这个 bitset 告诉了我们要扫描哪些文件. </p>
<p><code>FileSourceScanExec</code> 通过 <code>inputRDD</code> 暴露出数据给上层的算子, 所以说 Spark SQL 用的也是 RDD. <code>inputRDD</code> 有两种逻辑, 如果是 bucketing 的话, 会调用 <code>FileSourceScanExec#createBucketedReadRDD</code>.</p>
<p><code>createBucketedReadRDD</code> 逻辑也很简单: </p>
<ol>
<li>找到每个 partitions 里的所有文件(如果没有用 partitionBy 机制, selectedPartitions 就只会有一个)</li>
<li>找到每个文件对应的 bucketID(文件名里记录着, 正则匹配), group by 这个 ID, 得到类似 Map[bucketID, Arrar[file]] 这样的结构</li>
<li>只取出 <code>bucketSet</code> 中记录了的 bucketID 对应的 files.</li>
</ol>
<p>到这里, 就完成了 bucket pruning 的逻辑.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> bucketedFileName = <span class="string">&quot;&quot;&quot;.*_(\d+)(?:\..*)?$&quot;&quot;&quot;</span>.r</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBucketId</span></span>(fileName: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">Int</span>] = fileName <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> bucketedFileName(bucketId) =&gt; <span class="type">Some</span>(bucketId.toInt)</span><br><span class="line">  <span class="keyword">case</span> other =&gt; <span class="type">None</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The algorithm is pretty simple: each RDD partition being returned should include all the files</span></span><br><span class="line"><span class="comment"> * with the same bucket id from all the given Hive partitions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createBucketedReadRDD</span></span>(</span><br><span class="line">    bucketSpec: <span class="type">BucketSpec</span>,</span><br><span class="line">    readFile: (<span class="type">PartitionedFile</span>) =&gt; <span class="type">Iterator</span>[<span class="type">InternalRow</span>],</span><br><span class="line">    selectedPartitions: <span class="type">Seq</span>[<span class="type">PartitionDirectory</span>],</span><br><span class="line">    fsRelation: <span class="type">HadoopFsRelation</span>): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  logInfo(<span class="string">s&quot;Planning with <span class="subst">$&#123;bucketSpec.numBuckets&#125;</span> buckets&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> filesGroupedToBuckets =</span><br><span class="line">    selectedPartitions.flatMap &#123; p =&gt;</span><br><span class="line">      p.files.map &#123; f =&gt;</span><br><span class="line">        <span class="keyword">val</span> hosts = getBlockHosts(getBlockLocations(f), <span class="number">0</span>, f.getLen)</span><br><span class="line">        <span class="type">PartitionedFile</span>(p.values, f.getPath.toUri.toString, <span class="number">0</span>, f.getLen, hosts)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.groupBy &#123; f =&gt;</span><br><span class="line">      <span class="type">BucketingUtils</span></span><br><span class="line">        .getBucketId(<span class="keyword">new</span> <span class="type">Path</span>(f.filePath).getName)</span><br><span class="line">        .getOrElse(sys.error(<span class="string">s&quot;Invalid bucket file <span class="subst">$&#123;f.filePath&#125;</span>&quot;</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> prunedFilesGroupedToBuckets = <span class="keyword">if</span> (optionalBucketSet.isDefined) &#123;</span><br><span class="line">    <span class="keyword">val</span> bucketSet = optionalBucketSet.get</span><br><span class="line">    filesGroupedToBuckets.filter &#123;</span><br><span class="line">      f =&gt; bucketSet.get(f._1)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    filesGroupedToBuckets</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> filePartitions = <span class="type">Seq</span>.tabulate(bucketSpec.numBuckets) &#123; bucketId =&gt;</span><br><span class="line">    <span class="type">FilePartition</span>(bucketId, prunedFilesGroupedToBuckets.getOrElse(bucketId, <span class="type">Nil</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">FileScanRDD</span>(fsRelation.sparkSession, readFile, filePartitions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="3-优化-Bucket-Join"><a href="#3-优化-Bucket-Join" class="headerlink" title="3. 优化: Bucket Join"></a>3. 优化: Bucket Join</h1><h2 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h2><p>Bucket 还有一个很大的用处是可以用来做 Bucket Join, 这也是我觉得非常黑魔法的一个特性. 众所周知, SQL 的 join 是非常耗费时间的, Spark 为此也做了多种策略, 可以参考我之前的这篇博客 <a href="https://aaaaaaron.github.io/2019/06/29/Spark-SQL-Join-Deep-Dive/">Spark SQL Join Deep Dive</a>, 对于 大-大表进行 join, Spark 一般会选择 <code>SortMergeJoin</code> (SMJ), 因为SMJ 对比 HashJoin 来说, 不需要把一侧分片的数据都加载到内存中去, 提升了系统稳定性, 但是缺点是慢, 见下面两个例子:</p>
<h3 id="例子一"><a href="#例子一" class="headerlink" title="例子一"></a>例子一</h3><p>这个是最简单一个例子, 两个表直接 join, 无子查询.</p>
<h4 id="优化前"><a href="#优化前" class="headerlink" title="优化前"></a>优化前</h4><img src="/2019/12/31/Spark-Bucketing-Deep-Dive/78b48f23-6c82-4db0-9237-ecfcadbe8248-20210508113716354.png" alt="img" style="zoom:50%;">

<h4 id="优化后"><a href="#优化后" class="headerlink" title="优化后"></a>优化后</h4><img src="/2019/12/31/Spark-Bucketing-Deep-Dive/20191019131517-20210508113251983.png" style="zoom:50%;">

<p>可以见到, 由于是一个 SMJ, 而我们的源表又没有任何处理, 所以 Spark 自动给执行计划上加上了几个 exchange(shuffle) 和 sort, 对大数据有点了解的同学都知道, 这两步操作会十分的耗费时间与资源. 而要是启用了 Bucket Join 之后, 执行计划图会变成什么样呢? 可以见下图, 可以见之前的 exchange 和 sort 都没有了, 整体 query 时间从 40s+ 下降到 5s(图一到图三).</p>
<h3 id="例子二"><a href="#例子二" class="headerlink" title="例子二"></a>例子二</h3><p>我们来看更复杂的一个例子:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line"><span class="keyword">FROM</span> test_kylin_fact t1</span><br><span class="line"><span class="keyword">JOIN</span></span><br><span class="line">  (<span class="keyword">SELECT</span> lstg_format_name</span><br><span class="line">   <span class="keyword">FROM</span> test_kylin_fact</span><br><span class="line">   <span class="keyword">GROUP</span> <span class="keyword">BY</span> lstg_format_name) t2 <span class="keyword">ON</span> t1.lstg_format_name <span class="operator">=</span> t2.lstg_format_name</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> t1.lstg_format_name</span><br></pre></td></tr></table></figure>

<h4 id="优化前-1"><a href="#优化前-1" class="headerlink" title="优化前"></a>优化前</h4><img src="/2019/12/31/Spark-Bucketing-Deep-Dive/20191019201545-20210508113430256.png" style="zoom:50%;">

<h4 id="优化后-1"><a href="#优化后-1" class="headerlink" title="优化后"></a>优化后</h4><img src="/2019/12/31/Spark-Bucketing-Deep-Dive/2d709ebe-ee49-495a-8971-c8cf988cabf2-20210508114216719.png" alt="img" style="zoom:50%;">

<p>有意思的是, 去掉最后一个 group by, 执行图会变成这样, 这个就留给读者朋友们自己去想了.</p>
<img src="/2019/12/31/Spark-Bucketing-Deep-Dive/20191019202151-20210508114058946.png" alt="img" style="zoom:50%;">

<h3 id="3-2-原理说明"><a href="#3-2-原理说明" class="headerlink" title="3.2 原理说明"></a>3.2 原理说明</h3><p>我们先来看下 SMJ 的原理:</p>
<ol>
<li>为了让两条记录能连接到一起, 需要将具有相同 key 的记录分发到同一个分区, 这一步会导致 shuffle(Exchange).</li>
<li>分别对两个表中每个分区里的数据按照 key 进行 sort(SortExec), 然后后续做 merge sort 操作, 这样就可以不用像 HashJoin 需要把所有数据都拉到内存中.</li>
</ol>
<p>那么 Spark 是怎么知道是否需要添加这两步操作的呢? 假如我原始数据已经按 key 进行过了 sort, 那么是不是可以省下后面的 sort? 我们来看 <code>SortMergeJoinExec</code> 的两个方法:</p>
<ol>
<li><p><strong>requiredChildDistribution: Seq[Distribution]</strong>: <code>HashClusteredDistribution(leftKeys) :: HashClusteredDistribution(rightKeys) :: Nil</code>, SMJ 要求 Join 的两张表都是 HashClusteredDistribution 的</p>
</li>
<li><p><strong>requiredChildOrdering: Seq[SortOrder]</strong>: <code>leftKeys.map(SortOrder(_, Ascending)) :: rightKeys.map(SortOrder(_, Ascending)) :: Nil</code>, SMJ 要求 Join 的两张表都是排序的</p>
</li>
</ol>
<p>会在 <code>EnsureRequirements#ensureDistributionAndOrdering</code> 中判断一个节点的子节点是否符合上述的两个require:</p>
<h5 id="1-添加-exchange-节点"><a href="#1-添加-exchange-节点" class="headerlink" title="1. 添加 exchange 节点"></a>1. 添加 exchange 节点</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">children = children.zip(requiredChildDistributions).map &#123;</span><br><span class="line">  <span class="keyword">case</span> (child, distribution) <span class="keyword">if</span> child.outputPartitioning.satisfies(distribution) =&gt;</span><br><span class="line">    child</span><br><span class="line">  <span class="keyword">case</span> (child, <span class="type">BroadcastDistribution</span>(mode)) =&gt;</span><br><span class="line">    <span class="type">BroadcastExchangeExec</span>(mode, child)</span><br><span class="line">  <span class="keyword">case</span> (child, distribution) =&gt;</span><br><span class="line">    <span class="keyword">val</span> numPartitions = distribution.requiredNumPartitions</span><br><span class="line">      .getOrElse(defaultNumPreShufflePartitions)</span><br><span class="line">    <span class="type">ShuffleExchangeExec</span>(distribution.createPartitioning(numPartitions), child)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-添加-sort-节点"><a href="#2-添加-sort-节点" class="headerlink" title="2. 添加 sort 节点"></a>2. 添加 sort 节点</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">children = children.zip(requiredChildOrderings).map &#123; <span class="keyword">case</span> (child, requiredOrdering) =&gt;</span><br><span class="line">  <span class="comment">// If child.outputOrdering already satisfies the requiredOrdering, we do not need to sort.</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="type">SortOrder</span>.orderingSatisfies(child.outputOrdering, requiredOrdering)) &#123;</span><br><span class="line">    child</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">SortExec</span>(requiredOrdering, global = <span class="literal">false</span>, child = child)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-替换-children"><a href="#2-替换-children" class="headerlink" title="2. 替换 children"></a>2. 替换 children</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">operator.withNewChildren(children)</span><br></pre></td></tr></table></figure>

<p>那么为何 bucketing 可以省略掉上面这两步呢? 答案就是使用了 bucketing 机制, <code>FileSourceScanExec</code> 会暴露 SMJ 需要的 <code>Distribution</code> 和 <code>Ordering</code>, 代码见<code>FileSourceScanExec#val (outputPartitioning, outputOrdering): (Partitioning, Seq[SortOrder])</code>:</p>
<p>需要注意的一点是, 当用了 partitionBy, 或者就是你的 DF 有多个 partition 时, 会每个 partition 都有 bucketNum 个文件 (防止write 的时候数据倾斜), 所以一个 bucketID 可能会对应多个文件, 这些文件自己是有序的, 但是合在一起是无序的, 所以 ordering 是 Nil.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">lazy</span> <span class="keyword">val</span> (outputPartitioning, outputOrdering): (<span class="type">Partitioning</span>, <span class="type">Seq</span>[<span class="type">SortOrder</span>]) = &#123;</span><br><span class="line">  <span class="keyword">val</span> bucketSpec = <span class="keyword">if</span> (relation.sparkSession.sessionState.conf.bucketingEnabled) &#123;</span><br><span class="line">    relation.bucketSpec</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">None</span></span><br><span class="line">  &#125;</span><br><span class="line">  bucketSpec <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(spec) =&gt;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">toAttribute</span></span>(colName: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">Attribute</span>] = output.find(_.name == colName)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> bucketColumns = spec.bucketColumnNames.flatMap(n =&gt; toAttribute(n))</span><br><span class="line">      <span class="keyword">if</span> (bucketColumns.size == spec.bucketColumnNames.size) &#123;</span><br><span class="line">        <span class="keyword">val</span> partitioning = <span class="type">HashPartitioning</span>(bucketColumns, spec.numBuckets)</span><br><span class="line">        <span class="keyword">val</span> sortColumns =</span><br><span class="line">          spec.sortColumnNames.map(x =&gt; toAttribute(x)).takeWhile(x =&gt; x.isDefined).map(_.get)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sortOrder = <span class="keyword">if</span> (sortColumns.nonEmpty) &#123;</span><br><span class="line">          <span class="keyword">val</span> files = selectedPartitions.flatMap(partition =&gt; partition.files)</span><br><span class="line">          <span class="keyword">val</span> bucketToFilesGrouping =</span><br><span class="line">            files.map(_.getPath.getName).groupBy(file =&gt; <span class="type">BucketingUtils</span>.getBucketId(file))</span><br><span class="line">          <span class="keyword">val</span> singleFilePartitions = bucketToFilesGrouping.forall(p =&gt; p._2.length &lt;= <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (singleFilePartitions) &#123;</span><br><span class="line">            sortColumns.map(attribute =&gt; <span class="type">SortOrder</span>(attribute, <span class="type">Ascending</span>))</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">Nil</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="type">Nil</span></span><br><span class="line">        &#125;</span><br><span class="line">        (partitioning, sortOrder)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        (<span class="type">UnknownPartitioning</span>(<span class="number">0</span>), <span class="type">Nil</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      (<span class="type">UnknownPartitioning</span>(<span class="number">0</span>), <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="优化-SMJ-条件"><a href="#优化-SMJ-条件" class="headerlink" title="优化 SMJ 条件"></a>优化 SMJ 条件</h5><ol>
<li>Join 的列需要是 bucketBy/sortBy 的列, 且两边 bucketBy 的 num 要一样</li>
<li>如果有多个分区或者用了 partitionBy, Ordering 不能去除</li>
</ol>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol>
<li>DF 的每个 partition 都会有 bucket num 个文件, 比如 <code>df.repartition(5).write.format(&quot;parquet&quot;).option(&quot;path&quot;, &quot;/tmp/bueket&quot;).bucketBy(3, &quot;id&quot;)</code> 会产生15个文件, 由于这个限制, 我们使用 repartition 重新实现了类似 bucket 的功能.</li>
</ol>
<h1 id="写入-debug-截图"><a href="#写入-debug-截图" class="headerlink" title="写入 debug 截图"></a>写入 debug 截图</h1><p>TBD, 后面写个文章介绍</p>
<img src="/2019/12/31/Spark-Bucketing-Deep-Dive/20191018084225-20210508113437602.png" style="zoom:50%;">

<img src="/2019/12/31/Spark-Bucketing-Deep-Dive/20191018084345-20210508113440064.png" style="zoom: 37%;">
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2019/11/03/%E6%95%B4%E7%90%86-%E7%A3%81%E7%9B%98-I-O/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/11/03/%E6%95%B4%E7%90%86-%E7%A3%81%E7%9B%98-I-O/" class="post-title-link" itemprop="url">[整理] 磁盘 I/O</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-11-03 20:21:13" itemprop="dateCreated datePublished" datetime="2019-11-03T20:21:13+08:00">2019-11-03</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:44:13" itemprop="dateModified" datetime="2021-05-08T11:44:13+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>注: 本文非原创, 只是对网上一些内容进行了整理总结.</p>
<h1 id="Linux-文件系统简介"><a href="#Linux-文件系统简介" class="headerlink" title="Linux 文件系统简介"></a>Linux 文件系统简介</h1><h3 id="影响硬盘性能的因素"><a href="#影响硬盘性能的因素" class="headerlink" title="影响硬盘性能的因素"></a>影响硬盘性能的因素</h3><p>影响磁盘的关键因素是磁盘服务时间, 即磁盘完成一个I/O请求所花费的时间, 它由寻道时间、旋转延迟和数据传输时间三部分构成. </p>
<h4 id="1-寻道时间"><a href="#1-寻道时间" class="headerlink" title="1. 寻道时间"></a>1. 寻道时间</h4><p>Tseek是指将读写磁头移动至正确的磁道上所需要的时间. 寻道时间越短, I/O操作越快, 目前磁盘的平均寻道时间一般在3-15ms. </p>
<h4 id="2-旋转延迟"><a href="#2-旋转延迟" class="headerlink" title="2. 旋转延迟"></a>2. 旋转延迟</h4><p>Trotation是指盘片旋转将请求数据所在的扇区移动到读写磁盘下方所需要的时间. 旋转延迟取决于磁盘转速, 通常用磁盘旋转一周所需时间的1/2表示. 比如：7200 rpm 的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms, 而转速为15000rpm的磁盘其平均旋转延迟为2ms. </p>
<h4 id="3-数据传输时间"><a href="#3-数据传输时间" class="headerlink" title="3. 数据传输时间"></a>3. 数据传输时间</h4><p>Ttransfer是指完成传输所请求的数据所需要的时间, 它取决于数据传输率, 其值等于数据大小除以数据传输率. 目前IDE/ATA能达到133MB/s, SATA II可达到300MB/s的接口数据传输率, 数据传输时间通常远小于前两部分消耗时间. 简单计算时可忽略. </p>
<h3 id="衡量性能的指标"><a href="#衡量性能的指标" class="headerlink" title="衡量性能的指标"></a>衡量性能的指标</h3><p>机械硬盘的连续读写性能很好, 但随机读写性能很差, 这主要是因为磁头移动到正确的磁道上需要时间, 随机读写时, 磁头需要不停的移动, 时间都浪费在了磁头寻址上, 所以性能不高. 衡量磁盘的重要主要指标是IOPS和吞吐量. </p>
<h4 id="IOPS"><a href="#IOPS" class="headerlink" title="IOPS"></a>IOPS</h4><p>IOPS（Input/Output Per Second）即每秒的输入输出量（或读写次数）, 即指每秒内系统能处理的I/O请求数量. 随机读写频繁的应用, 如小文件存储等, 关注随机读写性能, IOPS是关键衡量指标. 可以推算出磁盘的IOPS = 1000ms / (Tseek + Trotation + Transfer), 如果忽略数据传输时间, 理论上可以计算出随机读写最大的IOPS. 常见磁盘的随机读写最大IOPS为：</p>
<ul>
<li>7200rpm的磁盘 IOPS = 76 IOPS</li>
<li>10000rpm的磁盘IOPS = 111 IOPS</li>
<li>15000rpm的磁盘IOPS = 166 IOPS</li>
</ul>
<p>虽然15000rpm的磁盘计算出的理论最大IOPS仅为166, 但在实际运行环境中, 实际磁盘的IOPS往往能够突破200甚至更高. 这其实就是在系统调用过程中, 操作系统进行了一系列的优化(page cache, 预读). </p>
<h4 id="吞吐量"><a href="#吞吐量" class="headerlink" title="吞吐量"></a>吞吐量</h4><p>吞吐量（Throughput）, 指单位时间内可以成功传输的数据数量. 顺序读写频繁的应用, 如视频点播, 关注连续读写性能、数据吞吐量是关键衡量指标. 它主要取决于磁盘阵列的架构, 通道的大小以及磁盘的个数. 不同的磁盘阵列存在不同的架构, 但他们都有自己的内部带宽, 一般情况下, 内部带宽都设计足够充足, 不会存在瓶颈. 磁盘阵列与服务器之间的数据通道对吞吐量影响很大, 比如一个2Gbps的光纤通道, 其所能支撑的最大流量仅为250MB/s. 最后, 当前面的瓶颈都不再存在时, 硬盘越多的情况下吞吐量越大. </p>
<h1 id="read-write"><a href="#read-write" class="headerlink" title="read / write"></a>read / write</h1><p>在现代操作系统中, 一个 “真正的” 文件, 当调用 <code>read</code> / <code>write</code> 的时候, 数据当然不会简单地就直达硬盘. 对于 Linux 而言, 这个过程的<strong>一部分</strong>是这样的：</p>
<p><img src="/2019/11/03/%E6%95%B4%E7%90%86-%E7%A3%81%E7%9B%98-I-O/588476624-59fadc92da61c_articlex.jpeg"></p>
<p>在操作系统内核空间内, read / write 到硬件设备之间, 按顺序有这么几层：</p>
<ul>
<li>  <strong>VFS</strong>：虚拟文件系统, 可以大致理解为 <code>read</code> / <code>write</code> / <code>ioctl</code> 之类的系统调用就在这一层. 当调用 <code>open</code> 之后, 内核会为每一个 file descriptor 创建一个 <code>file_operations</code> 结构体实例. 这个结构体里包含了 open、write、seek 等的实例（回调函数）. 这一层其实是 Linux 文件和设备体系的精华之一, 很多东西都隐藏或暴露在这一层. 不过本文不研究这一块</li>
<li>  <strong>文件系统</strong>： 这一层是实际的文件系统实现层, 向上隐藏了实现细节. 当然, 实际上除了文件系统之外, 还包含其他的虚拟文件, 包括设备节点、<code>/proc</code> 文件等等</li>
<li>  <strong>buffer cache</strong>：这就是本文所说的 “缓存”. 后文再讲. </li>
<li>  <strong>设备驱动</strong>：这是具体硬件设备的设备驱动了, 比如 SSD 的读写驱动、磁盘的读写驱动、字符设备的读写驱动等等. </li>
<li>  <strong>硬件设备</strong>：这没什么好讲的了, 就是实际的硬件设备接口. 参见<a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000011743916">上一篇文章</a></li>
</ul>
<h4 id="从系统调用角度"><a href="#从系统调用角度" class="headerlink" title="从系统调用角度"></a>从系统调用角度</h4><p><img src="/2019/11/03/%E6%95%B4%E7%90%86-%E7%A3%81%E7%9B%98-I-O/20191103212251.png"></p>
<ol>
<li>进程向内核发起一个系统调用, </li>
<li>内核接收到系统调用, 知道是对文件的请求, 于是告诉磁盘, 把文件读取出来</li>
<li>磁盘接收到来着内核的命令后, 把文件载入到内核的内存空间里面 (第一次 copy)</li>
<li>内核的内存空间接收到数据之后, 把数据 copy 到用户进程的内存空间(第二次 copy)</li>
<li>进程内存空间得到数据后, 给内核发送通知</li>
<li>内核把接收到的通知回复给进程, 此过程为唤醒进程, 然后进程得到数据, 进行下一步操作</li>
</ol>
<p>下面举例说明一个阻塞的文件 IO: 从 OS 角度看, 进程A执行过程中, 进行IO操作, IO操作引起进程A阻塞, 等待一个外部事件的发生（外部事件发生后, 该进程A才可以由阻塞态转为就绪态）, 同时, 上下文切换, 另一个进程B获得时间片执行. 在此期间, 阻塞态的进程A执行底层的IO处理（设备控制器和设备交换数据）, 进程B执行CPU操作. 当进程A完成IO处理后, 相应的外部事件（设备控制器发出中断请求信号）发生, 当前运行的进程B被中断, 进行外部事件处理, CPU和设备控制器之间传输数据, 完成后, 系统修改阻塞进程A的状态为就绪态. 然后, 依据剥夺或非剥夺调度算法, 选择被中断的进程B, 或刚解除阻塞的进程A, 或其它就绪进程C执行. </p>
<p>虽然 CPU 的时间片没有浪费, 但是进程/线程的上下文切换代价高.</p>
<h5 id="MMAP"><a href="#MMAP" class="headerlink" title="MMAP"></a>MMAP</h5><p>mmap 把文件映射到用户空间里的虚拟内存, 省去了从内核缓冲区复制到用户空间的过程, 文件中的位置在虚拟内存中有了对应的地址, 可以像操作内存一样操作这个文件, 相当于已经把整个文件放入内存, 但在真正使用到这些数据前却不会消耗物理内存, 也不会有读写磁盘的操作, 只有真正使用这些数据时, 虚拟内存管理系统 VMS 才根据缺页加载的机制从磁盘加载对应的数据块到物理内存进行渲染. 这样的文件读写文件方式少了数据从内核缓存到用户空间的拷贝, 效率很高.</p>
<p>比较适用小文件随机 IO, 比如说索引</p>
<ol>
<li>要提前确定大小</li>
<li>Java 中的 mmap 回收比较蛋疼</li>
<li>MMAP 使用的是虚拟内存, 和 PageCache 一样是由操作系统来控制刷盘的, 可以通过 force 强刷</li>
</ol>
<p><img src="/2019/11/03/%E6%95%B4%E7%90%86-%E7%A3%81%E7%9B%98-I-O/20191103215919.png"></p>
<h6 id="性能测试-Java"><a href="#性能测试-Java" class="headerlink" title="性能测试(Java)"></a>性能测试(Java)</h6><ul>
<li><p>文件大小 1.5G</p>
</li>
<li><p>读取方式:顺序读取</p>
</li>
<li><p>读取时间</p>
<p>  冷数据(排除 page cache):</p>
<pre><code>  1. FileChannel:10153ms
  2. FileChannel(use direct memory):8881ms
  3. Mmeory mapping:8111ms
  4. BufferedInputStream:8252ms
</code></pre>
<p>  热数据(有 page cache):</p>
<pre><code>  1. FileChannel:2262ms
  2. FileChannel(use direct memory):1354ms
  3. Mmeory mapping:1305ms
  4. BufferedInputStream:1961ms
</code></pre>
</li>
</ul>
<h5 id="Zero-copy"><a href="#Zero-copy" class="headerlink" title="Zero copy"></a>Zero copy</h5><p>用于网络传输减少上下文切换和 copy.</p>
<h6 id="普通流程-read-send"><a href="#普通流程-read-send" class="headerlink" title="普通流程(read/send)"></a>普通流程(read/send)</h6><ol>
<li>OS 将数据从磁盘复制到操作系统内核的页缓存中 (磁盘(DMA) -&gt; kernel buffer, U -&gt; K)</li>
<li>应用将数据从内核空间复制到应用的用户空间 (kernel buffer -&gt; user space buffer, K -&gt; U)</li>
<li>应用将数据复制回内核态的 Socket 缓存中 (user space buffer -&gt; kernel buffer, U -&gt; K)</li>
<li>OS 将Socket缓存复制到网卡发出 (kernel buffer -&gt; 网卡(DMA))</li>
</ol>
<p>1,2 步是 read, 3,4 步是 send, 整个过程总共发生了四次拷贝和四次的用户态和内核态的切换. </p>
<p><img src="/2019/11/03/%E6%95%B4%E7%90%86-%E7%A3%81%E7%9B%98-I-O/20191103221636.png"></p>
<h6 id="Zero-copy-sendfile"><a href="#Zero-copy-sendfile" class="headerlink" title="Zero copy (sendfile)"></a>Zero copy (sendfile)</h6><ol>
<li>OS 将数据从磁盘复制到操作系统内核的页缓存中 (磁盘(DMA) -&gt; kernel buffer, U -&gt; K)</li>
<li>内核的页缓存复制到内核态的 Socket 缓存中 (kernel buffer -&gt; Socket kernel buffer)</li>
<li>OS 将Socket缓存复制到网卡发出 (kernel buffer -&gt; 网卡(DMA))</li>
</ol>
<p><img src="/2019/11/03/%E6%95%B4%E7%90%86-%E7%A3%81%E7%9B%98-I-O/20191103221734.png"></p>
<p>如果网卡支持 gather operations 内核就可以进一步减少数据拷贝:</p>
<ol>
<li>OS 将数据从磁盘复制到操作系统内核的页缓存中 (磁盘(DMA) -&gt; kernel buffer, U -&gt; K)</li>
<li>无数据被复制到 Socket buffer. 只是描述了需要被写入的数据的位置和长度. DMA直接把数据从 kernel buffer 复制到网卡(DMA gather copy)</li>
</ol>
<h4 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h4><p>DMA（直接内存访问, Direct Memory Access）可以不经过CPU而直接进行磁盘和内存的数据交换. 在DMA模式下, CPU只需要向DMA控制器下达指令, 让DMA控制器来处理数据的传送即可, DMA控制器通过系统总线来传输数据, 传送完毕再通知CPU, 这样就在很大程度上降低了CPU占有率, 大大节省了系统资源</p>
<p>没有 DMA, 磁盘数据 copy 到内核需要 CPU 参与.</p>
<h1 id="工作机制"><a href="#工作机制" class="headerlink" title="工作机制"></a>工作机制</h1><h3 id="通用块层"><a href="#通用块层" class="headerlink" title="通用块层"></a>通用块层</h3><p>硬盘中每个扇区的大小固定为512字节, 扇区是最小的可寻址单元.</p>
<p>对于VFS和具体的文件系统来说, 数据是以块(Block)为单位进行管理的, 当内核访问文件的数据时, 它首先从磁盘上读取一个块, 由于扇区是磁盘的最小可寻址单元, 所以块不能比扇区还小, 只能整数倍于扇区大小, 即一个块对应磁盘上的一个或多个扇区. 而且由于Page Cache层的最小单元是页（Page）, 所以块大小不能超过一页的长度. 每块一般设定为4Kb, 当OS需要获取磁盘某个数据时, 将产生一个I/O中断, 本次I/O中断将带上具体的块号去驱动磁盘进行块数据查找和读取操作, 这些操作都是以某块作为起点, 每次读取数据的最小单位也是4Kb.</p>
<p>通用块层是粘合所有上层和底层的部分, 一个页的磁盘数据布局如下图所示：</p>
<p><img src="/2019/11/03/%E6%95%B4%E7%90%86-%E7%A3%81%E7%9B%98-I-O/20191103210628.png"></p>
<p><a target="_blank" rel="noopener" href="https://www.ibm.com/developerworks/cn/linux/l-cache/index.html">IBM 的资料</a> 摘抄：</p>
<p>当应用程序需要读取文件中的数据时, 操作系统先分配一些内存, 将数据从存储设备读入到这些内存中, 然后再将数据分发给应用程序；当需要往文件中写数据时, 操作系统先分配内存接收用户数据, 然后再将数据从内存写到磁盘上. </p>
<p>对于每个文件的<strong>第一个</strong>读请求, 系统读入所请求的页面并读入紧随其后的少数几个页面(不少于一个页面, 通常是三个页面), 这时的预读称为<strong>同步预读</strong>. </p>
<p>如果应用程序接下来是顺序读取的话, 那么文件 cache 命中, OS 会加大同步预读的范围, 增强缓存效率, 此时的预读被称为<strong>异步预读</strong></p>
<p>如果接下来 cache 没命中, 那么 OS 会继续使用同步预读. </p>
<h3 id="PAGE-CACHE层"><a href="#PAGE-CACHE层" class="headerlink" title="PAGE CACHE层"></a>PAGE CACHE层</h3><p>引入Cache层的目的是为了提高Linux操作系统对磁盘访问的性能. Cache层在内存中缓存了磁盘上的部分数据. 当数据的请求到达时, 如果在Cache中存在该数据且是最新的, 则直接将数据传递给用户程序, 免除了对底层磁盘的操作, 提高了性能. Cache层也正是磁盘IOPS为什么能突破200的主要原因之一. </p>
<p>在Linux的实现中, 文件Cache分为两个层面, 一是Page Cache, 另一个Buffer Cache, 每一个Page Cache包含若干Buffer Cache. Page Cache主要用来作为文件系统上的文件数据的缓存来用, 尤其是针对当进程对文件有read/write操作的时候. Buffer Cache则主要是设计用来在系统对块设备进行读写的时候, 对块进行数据缓存的系统来使用. </p>
<p>磁盘Cache有两大功能：预读和回写. 预读其实就是利用了局部性原理, 具体过程是：对于每个文件的第一个读请求, 系统读入所请求的页面并读入紧随其后的少数几个页面（通常是三个页面）, 这时的预读称为同步预读. 对于第二次读请求, 如果所读页面不在Cache中, 即不在前次预读的页中, 则表明文件访问不是顺序访问, 系统继续采用同步预读；如果所读页面在Cache中, 则表明前次预读命中, 操作系统把预读页的大小扩大一倍, 此时预读过程是异步的, 应用程序可以不等预读完成即可返回, 只要后台慢慢读页面即可, 这时的预读称为异步预读. 任何接下来的读请求都会处于两种情况之一：第一种情况是所请求的页面处于预读的页面中, 这时继续进行异步预读；第二种情况是所请求的页面处于预读页面之外, 这时系统就要进行同步预读. </p>
<p>回写是通过暂时将数据存在Cache里, 然后统一异步写到磁盘中. 通过这种异步的数据I/O模式解决了程序中的计算速度和数据存储速度不匹配的鸿沟, 减少了访问底层存储介质的次数, 使存储系统的性能大大提高. Linux 2.6.32内核之前, 采用pdflush机制来将脏页真正写到磁盘中, 什么时候开始回写呢？下面两种情况下, 脏页会被写回到磁盘：</p>
<p>在空闲内存低于一个特定的阈值时, 内核必须将脏页写回磁盘, 以便释放内存.<br>当脏页在内存中驻留超过一定的阈值时, 内核必须将超时的脏页写会磁盘, 以确保脏页不会无限期地驻留在内存中.<br>回写开始后, pdflush会持续写数据, 直到满足以下两个条件：</p>
<ol>
<li>已经有指定的最小数目的页被写回到磁盘. </li>
<li>空闲内存页已经回升, 超过了阈值. </li>
</ol>
<p>Linux 2.6.32内核之后, 放弃了原有的pdflush机制, 改成了bdi_writeback机制. bdi_writeback机制主要解决了原有fdflush机制存在的一个问题：在多磁盘的系统中, pdflush管理了所有磁盘的Cache, 从而导致一定程度的I/O瓶颈. bdi_writeback机制为每个磁盘都创建了一个线程, 专门负责这个磁盘的Page Cache的刷新工作, 从而实现了每个磁盘的数据刷新在线程级的分离, 提高了I/O性能. </p>
<p>回写机制存在的问题是回写不及时引发数据丢失（可由sync|fsync解决）, 回写期间读I/O性能很差. </p>
<h4 id="清除-Cache"><a href="#清除-Cache" class="headerlink" title="清除 Cache"></a>清除 Cache</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">清理 pagecache (页缓存)</span></span><br><span class="line">sysctl -w vm.drop_caches=1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">清理 dentries（目录缓存）和 inodes</span></span><br><span class="line">sysctl -w vm.drop_caches=2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">清理 pagecache、dentries 和 inodes</span></span><br><span class="line">sysctl -w vm.drop_caches=3</span><br></pre></td></tr></table></figure>

<h2 id="高性能硬盘-I-O-优化方案"><a href="#高性能硬盘-I-O-优化方案" class="headerlink" title="高性能硬盘 I/O 优化方案"></a>高性能硬盘 I/O 优化方案</h2><h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>从缓存的工作机制来看, 很简单, 如果要充分利用 Linux 的文件缓存机制, 那么最好的方法就是：每一个文件都尽可能地采用<strong>顺序读写</strong>, 避免大量的 <code>seek</code> 调用. </p>
<h3 id="尽可能顺序地读写一个文件"><a href="#尽可能顺序地读写一个文件" class="headerlink" title="尽可能顺序地读写一个文件"></a>尽可能顺序地读写一个文件</h3><p>从文件缓存角度, 如果频繁地随机读取一个文件不同的位置, 很可能导致缓存命中率下降. 那么 OS 就不得不频繁地往硬盘上预读, 进一步导致硬盘利用率低下. 所以在读写文件的时候, 尽可能的只是简单写入或者简单读取文件, 而不要使用 <code>seek</code>. </p>
<p>这条原则非常适用于 log 文件的写入：当写入 log 的时候, 写就好了, 不要经常翻回去查看以前的内容. </p>
<h3 id="单进程读写硬盘"><a href="#单进程读写硬盘" class="headerlink" title="单进程读写硬盘"></a>单进程读写硬盘</h3><p>整个系统, 最好只有一个进程进行磁盘的读写. 而不是多个进程进行文件存取. 这个思路, 一方面和上一条 “顺序写” 原则的理由其实是一致的. 当多个进程进行磁盘读写的时候, 随机度瞬间飙升. 特别是多个进程操作多个文件的时候, 磁盘的磁头很可能需要频繁大范围地移动. </p>
<p>如果确实有必要多个进程分别读取多个不同文件的话, 可以考虑下面的替代方案：</p>
<ul>
<li>  这多个进程是否功能上是独立的？能不能分开放在几个不同的服务器之中？</li>
<li>  如果这几个进程确实需要放在同一台服务器上, 那么能不能考虑为每个频繁读写的文件, 单独分配一个磁盘？</li>
<li>  如果成本允许, 并且文件大小不大的话, 能否将磁盘更换为 SSD ？因为 SSD 没有磁头和磁盘的物理寻址动作, 响应会快很多. </li>
</ul>
<p>如果是多个进程同时写入一个文件（比如 log）, 那就更好办了. 这种情况下, 可以在这几个进程和文件中间加入一个内部文件服务器, 将所有进程的存取文件需求汇总到该文件服务器中进行统一处理. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ProcessA   ProcessB   ProcessC</span><br><span class="line">   |          |          |</span><br><span class="line">   |          V          |</span><br><span class="line">   *----&gt;  The File  &lt;---*</span><br></pre></td></tr></table></figure>

<p>改为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ProcessA   ProcessB   ProcessC</span><br><span class="line">   |          |          |</span><br><span class="line">   |          V          |</span><br><span class="line">   *----&gt;  ProcessD  &lt;---*</span><br><span class="line">              |</span><br><span class="line">              V</span><br><span class="line">           The File</span><br></pre></td></tr></table></figure>

<p>顺便还可以在这个服务进程中实现一些自己的缓存机制, 配合 Linux 自身的文件缓存进一步优化磁盘 I/O 效率. </p>
<h4 id="以-4kB-为单位写文件"><a href="#以-4kB-为单位写文件" class="headerlink" title="以 4kB 为单位写文件"></a>以 4kB 为单位写文件</h4><p>这里可以看看下面这个伪代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const int WRITE_BLOCK_SIZE = 4096</span><br><span class="line"></span><br><span class="line">for (int i = 0 to 999) &#123;</span><br><span class="line">    write(fd, buff, WRITE_BLOCK_SIZE)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其实这个问题, 就是我在上一篇文章的 <a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000011743916#articleHeader2">“硬盘文件存取速度的考量”</a> 小节中所说的内容了.<br>这里有一个常量 <code>WRITE_BLOCK_SIZE</code>, 这并不是可以随意取的值, 比较合适的是 4096 或者其倍数, 理由是文件系统往往以 4kB 为页, 如果没有写够 4kB 的话, 将导致文件需要多余的读出动作. 虽然文件缓存在一定程度上能够帮你缓解, 但总会有一部分操作会最落地到底层 I/O 的. 所以实际操作中, 要尽量以 4kB 为边界操作大文件. </p>
<h2 id="大目录的寻址效率"><a href="#大目录的寻址效率" class="headerlink" title="大目录的寻址效率"></a>大目录的寻址效率</h2><p>有一个问题被提了出来：我们都知道, 当我们面对一个大目录（目录中有很多很多文件）的时候, 这个目录刷出来需要很长的时间. 那么我们在开发的时候是不是要避免经常在这个大目录中读写文件呢？</p>
<p>实际上, 当你第一次操作这个大目录的时候, 可能延时确实会比较大. 但是实测只要进入了这个目录之后, 再后续操作的时候, 却一点都不慢, 和其他的普通目录相当. </p>
<p>这个问题的原因, 我个人猜测（求权威人士指正）是这样的：<br>　　<strong>目录</strong>在文件系统中, 是以一个 <strong>inode</strong> 的方式存在的, 那么载入目录, 实际上就是载入这个 inode. 从存储的角度, inode 也只是一个普通的文件, 那么载入 inode 的动作和载入其他文件一样, 也会经过文件缓存策略. 载入了一次之后, 只要你持续地访问它, 那么操作系统就会将这个 inode 保持在缓存中. 因此后续的操作, 就是直接读写 RAM 了, 并不会受到硬盘 I/O 瓶颈的影响. </p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000011830405">高性能磁盘 I/O 开发学习笔记 – 软件手段篇</a><br><a target="_blank" rel="noopener" href="https://www.ibm.com/developerworks/cn/linux/l-cache/index.html">Linux 内核的文件 Cache 管理机制介绍</a><br><a target="_blank" rel="noopener" href="https://tech.meituan.com/about-desk-io.html">磁盘I/O那些事</a><br><a target="_blank" rel="noopener" href="http://blog.csdn.net/hguisu/article/details/6122513">Linux系统结构 详解</a><br><a target="_blank" rel="noopener" href="http://blog.leanote.com/post/xiangfei/4#title-4">【Linux编程基础】文件与目录–知识总结</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2019/10/30/Spark-Parquet-file-split/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/10/30/Spark-Parquet-file-split/" class="post-title-link" itemprop="url">Spark Parquet file split</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-10-30 20:14:43" itemprop="dateCreated datePublished" datetime="2019-10-30T20:14:43+08:00">2019-10-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:18:35" itemprop="dateModified" datetime="2021-05-08T11:18:35+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>在实际使用 Spark + Parquet 的时候, 遇到了两个<strong>不解</strong>的地方:</p>
<ol>
<li>我们只有一个 Parquet 文件(小于 HDFS block size), 但是 Spark 在某个 stage 生成了4个 tasks 来处理.</li>
<li>4个 tasks 中只有一个 task 处理了所有数据, 其他几个都没有在处理数据.</li>
</ol>
<p>这两个问题牵涉到对于 Parquet, Spark 是如何来进行切分 partitions, 以及每个 partition 要处理哪部分数据的.</p>
<p><strong>先说结论</strong>, Spark 中, Parquet 是 splitable 的, 代码见<code>ParquetFileFormat#isSplitable</code>. 那会不会把数据切碎? 答案是不会, 因为是以 Spark row group 为最小单位切分 Parquet 的, 这也会导致一些 partitions 会没有数据, 极端情况下, 如果只有一个 row group 的话, partitions 再多, 也只会一个 partition 有数据.</p>
<p>接下来开始我们的源码之旅:</p>
<h3 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h3><h4 id="1-根据-Parquet-按文件大小切块生成-partitions"><a href="#1-根据-Parquet-按文件大小切块生成-partitions" class="headerlink" title="1. 根据 Parquet 按文件大小切块生成 partitions:"></a>1. 根据 Parquet 按文件大小切块生成 partitions:</h4><p>在 <code>FileSourceScanExec#createNonBucketedReadRDD</code> 中, 如果文件是 splitable 的, 会按照 maxSplitBytes 把文件切分, 最后生成的数量, 就是 RDD partition 的数量, 这个解释了<strong>不解1</strong>, 代码如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxSplitBytes = <span class="type">Math</span>.min(defaultMaxSplitBytes, <span class="type">Math</span>.max(openCostInBytes, bytesPerCore))</span><br><span class="line">logInfo(<span class="string">s&quot;Planning scan with bin packing, max size: <span class="subst">$maxSplitBytes</span> bytes, &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;open cost is considered as scanning <span class="subst">$openCostInBytes</span> bytes.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> splitFiles = selectedPartitions.flatMap &#123; partition =&gt;</span><br><span class="line">  partition.files.flatMap &#123; file =&gt;</span><br><span class="line">    <span class="keyword">val</span> blockLocations = getBlockLocations(file)</span><br><span class="line">    <span class="keyword">if</span> (fsRelation.fileFormat.isSplitable(</span><br><span class="line">        fsRelation.sparkSession, fsRelation.options, file.getPath)) &#123;</span><br><span class="line">      (<span class="number">0</span>L until file.getLen by maxSplitBytes).map &#123; offset =&gt;</span><br><span class="line">        <span class="keyword">val</span> remaining = file.getLen - offset</span><br><span class="line">        <span class="keyword">val</span> size = <span class="keyword">if</span> (remaining &gt; maxSplitBytes) maxSplitBytes <span class="keyword">else</span> remaining</span><br><span class="line">        <span class="keyword">val</span> hosts = getBlockHosts(blockLocations, offset, size)</span><br><span class="line">        <span class="type">PartitionedFile</span>(</span><br><span class="line">          partition.values, file.getPath.toUri.toString, offset, size, hosts)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> hosts = getBlockHosts(blockLocations, <span class="number">0</span>, file.getLen)</span><br><span class="line">      <span class="type">Seq</span>(<span class="type">PartitionedFile</span>(</span><br><span class="line">        partition.values, file.getPath.toUri.toString, <span class="number">0</span>, file.getLen, hosts))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;.toArray.sortBy(_.length)(implicitly[<span class="type">Ordering</span>[<span class="type">Long</span>]].reverse)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> partitions = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">FilePartition</span>]</span><br><span class="line"><span class="keyword">val</span> currentFiles = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">PartitionedFile</span>]</span><br><span class="line"><span class="keyword">var</span> currentSize = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Close the current partition and move to the next. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">closePartition</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (currentFiles.nonEmpty) &#123;</span><br><span class="line">    <span class="keyword">val</span> newPartition =</span><br><span class="line">      <span class="type">FilePartition</span>(</span><br><span class="line">        partitions.size,</span><br><span class="line">        currentFiles.toArray.toSeq) <span class="comment">// Copy to a new Array.</span></span><br><span class="line">    partitions += newPartition</span><br><span class="line">  &#125;</span><br><span class="line">  currentFiles.clear()</span><br><span class="line">  currentSize = <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Assign files to partitions using &quot;First Fit Decreasing&quot; (FFD)</span></span><br><span class="line">splitFiles.foreach &#123; file =&gt;</span><br><span class="line">  <span class="keyword">if</span> (currentSize + file.length &gt; maxSplitBytes) &#123;</span><br><span class="line">    closePartition()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Add the given file to the current partition.</span></span><br><span class="line">  currentSize += file.length + openCostInBytes</span><br><span class="line">  currentFiles += file</span><br><span class="line">&#125;</span><br><span class="line">closePartition()</span><br><span class="line"></span><br><span class="line"><span class="keyword">new</span> <span class="type">FileScanRDD</span>(fsRelation.sparkSession, readFile, partitions)</span><br></pre></td></tr></table></figure>

<p>如果是一个文件被分成多个 splits, 那么一个 file split 对应一个 partition. 如果是很多小文件这种的多个 file splits, 可能一个 partition 会有多个 file splits.</p>
<p><strong>一个 partition 对应一个 task.</strong></p>
<p>题外话, MR 引擎对于小文件的处理也有优化, 可以一个 map 处理多个 文件..</p>
<h4 id="2-使用-ParquetInputSplit-构造-reader"><a href="#2-使用-ParquetInputSplit-构造-reader" class="headerlink" title="2. 使用 ParquetInputSplit 构造 reader:"></a>2. 使用 ParquetInputSplit 构造 reader:</h4><p>在 <code>ParquetFileFormat#buildReaderWithPartitionValues</code> 实现中, 会使用 split 来初始化 reader, 并且根据配置可以把 reader 分为否是 vectorized 的, 关于 vectorized, 我会在日后再开一篇博文来介绍:</p>
<ul>
<li><code>vectorizedReader.initialize(split, hadoopAttemptContext)</code></li>
<li><code>reader.initialize(split, hadoopAttemptContext)</code></li>
</ul>
<p>关于 步骤2 在画外中还有更详细的代码, 但与本文的主流程关系不大, 这里先不表.</p>
<h4 id="3-划分-Parquet-的-row-groups-到不同的Spark-partitions-中去"><a href="#3-划分-Parquet-的-row-groups-到不同的Spark-partitions-中去" class="headerlink" title="3. 划分 Parquet 的 row groups 到不同的Spark partitions 中去"></a>3. 划分 Parquet 的 row groups 到不同的Spark partitions 中去</h4><p>在 步骤1 中根据文件大小均分了一些 partitions, 但不是所有这些 partitions 最后都会有数据. </p>
<p>接回 步骤2 中的 init, 在 <code>SpecificParquetRecordReaderBase#initialize</code> 中, 会在 <code>readFooter</code> 的时候传入一个 <code>RangeMetadataFilter</code>, 这个 filter 的range 是根据你的 split 的边界来的, 最后会用这个 range 来划定 row group 的归属(footer.getBlocks()):</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    footer = readFooter(configuration, file, range(inputSplit.getStart(), inputSplit.getEnd()));</span><br><span class="line">    FilterCompat.Filter filter = getFilter(configuration);</span><br><span class="line">    blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Parquet 的<code>ParquetFileReader#readFooter</code>方法会用到<code>ParquetMetadataConverter#converter.readParquetMetadata(f, filter);</code>, 这个<code>readParquetMetadata</code> 使用了一个访问者模式, 而其中对于<code>RangeMetadataFilter</code>的处理是:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FileMetaData <span class="title">visit</span><span class="params">(RangeMetadataFilter filter)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> filterFileMetaDataByMidpoint(readFileMetaData(from), filter);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>终于到了最关键的切分的地方, 最关键的就是这一段, 谁拥有这个 row group的中点, 谁就可以处理这个 row group. </p>
<p>现在假设我们有一个40m 的文件, 只有一个 row group, 10m 一分, 那么将会有4个 partitions, 但是只有一个 partition 会占有这个 row group 的中点, 所以也只有这一个 partition 会有数据.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> midPoint = startIndex + totalSize / <span class="number">2</span>;</span><br><span class="line"><span class="keyword">if</span> (filter.contains(midPoint)) &#123;</span><br><span class="line">  newRowGroups.add(rowGroup);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>完整代码如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> FileMetaData <span class="title">filterFileMetaDataByMidpoint</span><span class="params">(FileMetaData metaData, RangeMetadataFilter filter)</span> </span>&#123;</span><br><span class="line">  List&lt;RowGroup&gt; rowGroups = metaData.getRow_groups();</span><br><span class="line">  List&lt;RowGroup&gt; newRowGroups = <span class="keyword">new</span> ArrayList&lt;RowGroup&gt;();</span><br><span class="line">  <span class="keyword">for</span> (RowGroup rowGroup : rowGroups) &#123;</span><br><span class="line">    <span class="keyword">long</span> totalSize = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">long</span> startIndex = getOffset(rowGroup.getColumns().get(<span class="number">0</span>));</span><br><span class="line">    <span class="keyword">for</span> (ColumnChunk col : rowGroup.getColumns()) &#123;</span><br><span class="line">      totalSize += col.getMeta_data().getTotal_compressed_size();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">long</span> midPoint = startIndex + totalSize / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">if</span> (filter.contains(midPoint)) &#123;</span><br><span class="line">      newRowGroups.add(rowGroup);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  metaData.setRow_groups(newRowGroups);</span><br><span class="line">  <span class="keyword">return</span> metaData;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="画外"><a href="#画外" class="headerlink" title="画外:"></a>画外:</h4><p>步骤2 中的代码其实是 spark 正儿八经如何读文件的代码, 最后返回一个<code>FileScanRDD</code>, 也很值得顺路看一下, 完整代码如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">(file: <span class="type">PartitionedFile</span>) =&gt; &#123;</span><br><span class="line">  assert(file.partitionValues.numFields == partitionSchema.size)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fileSplit =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">FileSplit</span>(<span class="keyword">new</span> <span class="type">Path</span>(<span class="keyword">new</span> <span class="type">URI</span>(file.filePath)), file.start, file.length, <span class="type">Array</span>.empty)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> split =</span><br><span class="line">    <span class="keyword">new</span> org.apache.parquet.hadoop.<span class="type">ParquetInputSplit</span>(</span><br><span class="line">      fileSplit.getPath,</span><br><span class="line">      fileSplit.getStart,</span><br><span class="line">      fileSplit.getStart + fileSplit.getLength,</span><br><span class="line">      fileSplit.getLength,</span><br><span class="line">      fileSplit.getLocations,</span><br><span class="line">      <span class="literal">null</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> attemptId = <span class="keyword">new</span> <span class="type">TaskAttemptID</span>(<span class="keyword">new</span> <span class="type">TaskID</span>(<span class="keyword">new</span> <span class="type">JobID</span>(), <span class="type">TaskType</span>.<span class="type">MAP</span>, <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">  <span class="keyword">val</span> hadoopAttemptContext =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">TaskAttemptContextImpl</span>(broadcastedHadoopConf.value.value, attemptId)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Try to push down filters when filter push-down is enabled.</span></span><br><span class="line">  <span class="comment">// Notice: This push-down is RowGroups level, not individual records.</span></span><br><span class="line">  <span class="keyword">if</span> (pushed.isDefined) &#123;</span><br><span class="line">    <span class="type">ParquetInputFormat</span>.setFilterPredicate(hadoopAttemptContext.getConfiguration, pushed.get)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> parquetReader = <span class="keyword">if</span> (enableVectorizedReader) &#123;</span><br><span class="line">    <span class="keyword">val</span> vectorizedReader = <span class="keyword">new</span> <span class="type">VectorizedParquetRecordReader</span>()</span><br><span class="line">    vectorizedReader.initialize(split, hadoopAttemptContext)</span><br><span class="line">    logDebug(<span class="string">s&quot;Appending <span class="subst">$partitionSchema</span> <span class="subst">$&#123;file.partitionValues&#125;</span>&quot;</span>)</span><br><span class="line">    vectorizedReader.initBatch(partitionSchema, file.partitionValues)</span><br><span class="line">    <span class="keyword">if</span> (returningBatch) &#123;</span><br><span class="line">      vectorizedReader.enableReturningBatches()</span><br><span class="line">    &#125;</span><br><span class="line">    vectorizedReader</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    logDebug(<span class="string">s&quot;Falling back to parquet-mr&quot;</span>)</span><br><span class="line">    <span class="comment">// ParquetRecordReader returns UnsafeRow</span></span><br><span class="line">    <span class="keyword">val</span> reader = pushed <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(filter) =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ParquetRecordReader</span>[<span class="type">UnsafeRow</span>](</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ParquetReadSupport</span>,</span><br><span class="line">          <span class="type">FilterCompat</span>.get(filter, <span class="literal">null</span>))</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ParquetRecordReader</span>[<span class="type">UnsafeRow</span>](<span class="keyword">new</span> <span class="type">ParquetReadSupport</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    reader.initialize(split, hadoopAttemptContext)</span><br><span class="line">    reader</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> iter = <span class="keyword">new</span> <span class="type">RecordReaderIterator</span>(parquetReader)</span><br><span class="line">  <span class="type">Option</span>(<span class="type">TaskContext</span>.get()).foreach(_.addTaskCompletionListener(_ =&gt; iter.close()))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// UnsafeRowParquetRecordReader appends the columns internally to avoid another copy.</span></span><br><span class="line">  <span class="keyword">if</span> (parquetReader.isInstanceOf[<span class="type">VectorizedParquetRecordReader</span>] &amp;&amp;</span><br><span class="line">      enableVectorizedReader) &#123;</span><br><span class="line">    iter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">InternalRow</span>]]</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> fullSchema = requiredSchema.toAttributes ++ partitionSchema.toAttributes</span><br><span class="line">    <span class="keyword">val</span> joinedRow = <span class="keyword">new</span> <span class="type">JoinedRow</span>()</span><br><span class="line">    <span class="keyword">val</span> appendPartitionColumns = <span class="type">GenerateUnsafeProjection</span>.generate(fullSchema, fullSchema)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// This is a horrible erasure hack...  if we type the iterator above, then it actually check</span></span><br><span class="line">    <span class="comment">// the type in next() and we get a class cast exception.  If we make that function return</span></span><br><span class="line">    <span class="comment">// Object, then we can defer the cast until later!</span></span><br><span class="line">    <span class="keyword">if</span> (partitionSchema.length == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// There is no partition columns</span></span><br><span class="line">      iter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">InternalRow</span>]]</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      iter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">InternalRow</span>]]</span><br><span class="line">        .map(d =&gt; appendPartitionColumns(joinedRow(d, file.partitionValues)))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个返回的<code>(PartitionedFile) =&gt; Iterator[InternalRow] </code>, 是在<code>FileSourceScanExec#inputRDD</code>用的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">lazy</span> <span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> readFile: (<span class="type">PartitionedFile</span>) =&gt; <span class="type">Iterator</span>[<span class="type">InternalRow</span>] =</span><br><span class="line">    relation.fileFormat.buildReaderWithPartitionValues(</span><br><span class="line">      sparkSession = relation.sparkSession,</span><br><span class="line">      dataSchema = relation.dataSchema,</span><br><span class="line">      partitionSchema = relation.partitionSchema,</span><br><span class="line">      requiredSchema = requiredSchema,</span><br><span class="line">      filters = pushedDownFilters,</span><br><span class="line">      options = relation.options,</span><br><span class="line">      hadoopConf = relation.sparkSession.sessionState.newHadoopConfWithOptions(relation.options))</span><br><span class="line"></span><br><span class="line">  relation.bucketSpec <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(bucketing) <span class="keyword">if</span> relation.sparkSession.sessionState.conf.bucketingEnabled =&gt;</span><br><span class="line">      createBucketedReadRDD(bucketing, readFile, selectedPartitions, relation)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      createNonBucketedReadRDD(readFile, selectedPartitions, relation)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FileScanRDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FileScanRDD</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    @transient private val sparkSession: <span class="type">SparkSession</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    readFunction: (<span class="type">PartitionedFile</span></span>) <span class="title">=&gt;</span> <span class="title">Iterator</span>[<span class="type">InternalRow</span>],</span></span><br><span class="line">    <span class="meta">@transient</span> <span class="keyword">val</span> filePartitions: <span class="type">Seq</span>[<span class="type">FilePartition</span>])</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">RDD</span>[<span class="type">InternalRow</span>](sparkSession.sparkContext, <span class="type">Nil</span>) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">RDDPartition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> files = split.asInstanceOf[<span class="type">FilePartition</span>].files.toIterator</span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> currentFile: <span class="type">PartitionedFile</span> = <span class="literal">null</span> <span class="comment">// 根据 currentFile = files.next() 来的, 具体实现我就不贴了 有兴趣的可以自己看下.</span></span><br><span class="line">    ...</span><br><span class="line">    readFunction(currentFile)</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>提升一个 Parquet 中的 row group 中的行数阈值, 籍此提示 Spark 并行度.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2019/10/16/Spark-MapOutputTracker-Deep-Dive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/10/16/Spark-MapOutputTracker-Deep-Dive/" class="post-title-link" itemprop="url">Spark MapOutputTracker Deep Dive</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-10-16 22:35:36" itemprop="dateCreated datePublished" datetime="2019-10-16T22:35:36+08:00">2019-10-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:28:11" itemprop="dateModified" datetime="2021-05-08T11:28:11+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><p>Shuffle writer 会将中间数据保存到 Block 里面, 然后将数据的位置发送给 <code>MapOutputTracker</code>; Shuffle reader 通过向 <code>MapOutputTracker</code> 获取中间数据的位置之后, 才能读取到数据.</p>
<p><code>MapOutputTrackerMaster</code> 启动在 driver 端, <code>MapOutputTrackerWorker</code> 启动在 executor 端.</p>
<p>ShuffleStatus 就是 <code>Map[Int, Array[MapStatus]]</code>, key 是 Shuffle 的 ID, value 数组的大小是该 ShuffleMapTask 的个数, MapStatus 会记录 stage reduce 端 task 个数的 status, 具体实现有两种: <code>CompressedMapStatus</code>/<code>HighlyCompressedMapStatus</code>, 具体实现之后分析, 当 reduce 端 task 超过 2000 (<code>SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS</code>) 的时候, 会使用 <code>HighlyCompressedMapStatus</code>, 看名字可以看出后一种压缩率更高. 因为其实这个 <code>Map[Int, Array[MapStatus]]</code> 会非常占据内存, 试想下, 假如我有10w 个 map 端的 task 和10w 个 reduce 端的 task, 那么这个 <code>Array[MapStatus]</code> 实际存了 100亿 task 的信息, 而且后面这些 status 还要序列化发给 executor, 又会占用更多的空间, 同时 Spark 这里代码写的也不是非常好, 导致内存占用会很高. 而 driver 端的内存大家一般不会设置的特别高, 这里就会导致 OOM, 而 driver 又是 Spark 的单点, 这是一个非常严重的稳定性问题. 之后我会给出具体的例子和修复.</p>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程:"></a>流程:</h2><h3 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h3><ol>
<li><code>MapOutputTrackerMaster</code> 会 <code>registerShuffle</code> 和 <code>registerMapOutput</code>. registerShuffle 是 DAGScheduler 在创建一个 <code>ShuffleMapStage</code> 时会把这个 stage 对应的 shuffle 注册进来(<code>createShuffleMapStage</code>); <code>registerMapOutput</code> 是 在一个 <code>shuffleMapTask</code> 任务完成后(<code>DAGScheduler.handleTaskCompletion</code>)，会把 <code>shuffleMapTask</code> 输出的信息(<code>MapStatus</code>)放进来.</li>
</ol>
<h3 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h3><ol>
<li><p>当 shuffle read 的时候, <code>BlockStoreShuffleReader中</code>，会调用 <code>MapOutputTrackerWorker.getMapSizesByExecutorId</code> (master 端的这个方法只在 local 用)</p>
</li>
<li><p>调用 <code>MapOutputTrackerWorker#getStatuses(shuffleID)</code>, Worker 有个 mapStatuses 缓存 <code>Map[Int, Array[MapStatus]]</code>, 当 Miss 的时候, 会去 fetching, 就有两个很重要的方法:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fetchedBytes = askTracker[<span class="type">Array</span>[<span class="type">Byte</span>]](<span class="type">GetMapOutputStatuses</span>(shuffleId))</span><br><span class="line">fetchedStatuses = <span class="type">MapOutputTracker</span>.deserializeMapStatuses(fetchedBytes) <span class="comment">// 有两种模式 direct 的和 broadcast 的</span></span><br></pre></td></tr></table></figure>
<p>Worker askTracker 向 <code>MapOutputTrackerMasterEndpoint</code> 要 statues, 这个 endpoint 会向 MapOutputTrackerMaster post 一个 <code>GetMapOutputMessage(shufflID)</code> 事件(放入 <code>LinkedBlockingQueue[GetMapOutputMessage]</code>), 且 master 会启动一个 <code>MessageLoop</code>, 会 take 这个阻塞队列的事件, 从 master 自己内存中维护的 <code>shuffleStatuses</code> 找到对应 shuffleID 的 ShuffleStatus(<code>Map[Int, Array[MapStatus]]</code>), 在 Write 中提过, 当 <code>shuffleMapTask</code> 完成的时候, 会通知 <code>DAGScheduler.handleTaskCompletion</code>, 所以 driver 有所有的 <code>MapStatues</code>.</p>
</li>
<li><p>driver 拿到对应的 shuffleStatuses 之后, 需要把它 reply 回 请求的发起方, 也就是 executor, 这是最耗费内存的一步操作, 也是外面后期性能优化的点: <code>context.reply(shuffleStatus.serializedMapStatus(broadcastManager, isLocal, minSizeForBroadcast))</code>, 这个方法会调用 <code>MapOutputTracker.serializeMapStatuses</code>. 这个方法会使用 Java 的序列化机制(ObjectOutputStream)来序列化一个 <code>Array[MapStatus]</code> (对应一个 shuffle 的所有 MapStatus 输出), 并使用 gzip 压缩, 当序列化完之后, 会有有两种通知给 executor 的模式: 当序列化后的 byte 数组大小小于 minBroadcastSize(512K) 时, 会直接返回 Array[Byte], 后续使用 Spark 的 RPC 模式返回给 executor, 否则则用 Broadcast 机制返回给 executor.</p>
</li>
</ol>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol>
<li>每个 executor 拿的对应 shuffle 的 <code>Array[MapStatus]</code> 都是全量的. 这其实没有必要, 最好每个 executor 只拿自己 task 需要的 map statues 就可以了, 但是这个实现不容易</li>
<li>当 reduce 端 task 非常多的时候, 会使用 <code>HighlyCompressedMapStatus</code>, 这里面会用一个 RoaringBitmap 存 emptyBlocks, 但是其实当 reduce 特别多的时候, 存有 block 的反而更少 </li>
<li>序列化的时候, 使用 <code>ByteArrayOutputStream</code>, 且没有设置初始化大小, 导致一直在 grow, 不断的发生 array copy. 而且 <code>ByteArrayOutputStream</code> 比较坑爹, toByteArray 还会进行一次 array copy.</li>
<li>当满足一定条件会进行 broadcast, toByteArray又生成一个 array. 且要是进行 broadcast 的话, 上面的序列化就根本没有必要, 因为 broadcast 还会进行一次序列化.</li>
</ol>
<h2 id="模拟"><a href="#模拟" class="headerlink" title="模拟"></a>模拟</h2><p>测试数据集为 tpch 50, 使用 Spark SQL 测试, 测试查询为<code>select count(*) from lineitem group by l_comment </code>, 启动 20w Map 端 task, 5w Reduce 端 task, 4g driver memory.</p>
<p>发生 oom</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">    at java.util.Arrays.copyOf(Arrays.java:3236)</span><br><span class="line">    at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)</span><br><span class="line">    at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)</span><br><span class="line">    at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)</span><br><span class="line">    at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253)</span><br><span class="line">    at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211)</span><br><span class="line">    at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145)</span><br><span class="line">    at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894)</span><br><span class="line">    at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875)</span><br><span class="line">    at java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822)</span><br><span class="line">    at java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719)</span><br><span class="line">    at java.io.ObjectOutputStream.close(ObjectOutputStream.java:740)</span><br><span class="line">    at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$2.apply$mcV$sp(MapOutputTracker.scala:804)</span><br><span class="line">    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1369)</span><br><span class="line">    at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:803)</span><br><span class="line">    at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:174)</span><br><span class="line">    at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:397)</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">    at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>

<p>查看 dump:</p>
<p><img src="/2019/10/16/Spark-MapOutputTracker-Deep-Dive/20191016230210.png"></p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>社区已经 fix 了</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2019/10/09/Callback%E4%B8%8E-Coroutine-%E5%8D%8F%E7%A8%8B%E6%A6%82%E5%BF%B5%E8%AF%B4%E6%98%8E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/10/09/Callback%E4%B8%8E-Coroutine-%E5%8D%8F%E7%A8%8B%E6%A6%82%E5%BF%B5%E8%AF%B4%E6%98%8E/" class="post-title-link" itemprop="url">Callback 与 Coroutine 协程概念说明</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-10-09 20:54:46" itemprop="dateCreated datePublished" datetime="2019-10-09T20:54:46+08:00">2019-10-09</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:48:09" itemprop="dateModified" datetime="2021-05-08T11:48:09+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="小谈阻塞非阻塞"><a href="#小谈阻塞非阻塞" class="headerlink" title="小谈阻塞非阻塞"></a>小谈阻塞非阻塞</h1><p>阻塞非阻塞概念都是对于线程, 进程这种粒度来说的, 因为只有他们才是内核有感知的, 协程是你内核无感知, 是你用户自己实现的.</p>
<p>例如在 Golang 中, <code>resp, err := client.Do(req)</code> 看着是阻塞的写法(有网络 IO), 但是 Go 的 Http 包是异步的, 这边在协程粒度是阻塞住了, 但是线程该干嘛就干嘛去了, 对于系统来说, 就是非阻塞. 应用程序觉得我遇到阻塞, 比如 I/O什么的, 我就 yield 出去, 把控制权交出去.</p>
<p><strong>分清楚内核层和应用层是关键</strong></p>
<p><strong>阻塞还是非阻塞</strong> 应用程序的调用是否立即返回. 被挂起无法执行其他操作的则是阻塞型的, 可以被立即「抽离」去完成其他「任务」的则是非阻塞型的.<br><img src="/2019/10/09/Callback%E4%B8%8E-Coroutine-%E5%8D%8F%E7%A8%8B%E6%A6%82%E5%BF%B5%E8%AF%B4%E6%98%8E/20191009205805-20210508114757190.png"></p>
<p>但是挂起就不能干事情了吗, 答案是否定的, 一个线程读文件,被阻塞了,资源会出让(陈力就列, 不能者止). coroutine也是, 但是比如 goroutine, go的调度器会把处于阻塞的的go程上的资源分配给其他go程. 但是这里的重点就是线程切换的代价比协程切换的代价高很多(线程切换涉及到内核态和用户态的切换)</p>
<p><strong>协程线程</strong> 调度一个主动(协作式调度, 应用程序自己调度) 一个被动(抢占式调度, 操作系统调度)</p>
<p><strong>异步和同步</strong> 数据 copy 时进程是否阻塞.  同步:应用层自己去想内核询问(轮询?); 异步:内核主动通知应用层数据. IO 操作分为两个过程：内核等待事件, 把数据拷贝到用户缓冲区. 这两个过程只要等待任何一个都是同步 IO</p>
<p><img src="/2019/10/09/Callback%E4%B8%8E-Coroutine-%E5%8D%8F%E7%A8%8B%E6%A6%82%E5%BF%B5%E8%AF%B4%E6%98%8E/20191009205836-20210508114803795.png"></p>
<p>在异步非阻塞模型中, 没有无谓的挂起、休眠与等待, 也没有盲目无知的问询与检查, 应用层做到不等候片刻的最大化利用自身的资源, 系统内核也十分「善解人意」的在完成任务后主动通知应用层来接收任务成果. </p>
<p><img src="/2019/10/09/Callback%E4%B8%8E-Coroutine-%E5%8D%8F%E7%A8%8B%E6%A6%82%E5%BF%B5%E8%AF%B4%E6%98%8E/20191011223017-20210508114805873.png"></p>
<h3 id="进程、线程、协程"><a href="#进程、线程、协程" class="headerlink" title="进程、线程、协程"></a>进程、线程、协程</h3><h3 id="1-进程"><a href="#1-进程" class="headerlink" title="1. 进程"></a>1. 进程</h3><p>操作系统中最核心的概念是进程, 分布式系统中最重要的问题是进程间通信. </p>
<p>进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位. 每个进程都有自己的独立内存空间, 不同进程通过进程间通信来通信. 由于进程比较重量, 占据独立的内存, 所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大, 但相对比较稳定安全. </p>
<p>进程是“程序执行的一个实例” , 担当分配系统资源的实体. 进程创建必须分配一个完整的独立地址空间. </p>
<p>进程切换只发生在内核态, 两步：1. 切换页全局目录以安装一个新的地址空间 2. 切换内核态堆栈和硬件上下文.  另一种说法类似：1 保存CPU环境（寄存器值、程序计数器、堆栈指针）; 2. 修改内存管理单元MMU的寄存器; 3. 转换后备缓冲器TLB中的地址转换缓存内容标记为无效</p>
<h4 id="事件模型"><a href="#事件模型" class="headerlink" title="事件模型"></a>事件模型</h4><p>基于事件的模型, 一个进程处理多个请求, 并且通过epoll机制来通知用户请求完成, 事件驱动适合于IO密集型服务, 多进程或线程适合于CPU密集型服务</p>
<p>Nginx 在一个工作进程中处理多个连接和请求, 因为满负载进程的数量很少（通常每核CPU只有一个）而且恒定, 所以任务切换只消耗很少的内存, 而且不会浪费CPU周期, 当然前提是没有阻塞</p>
<p>为什么不使用很多进程: 每个进程都消耗额外的内存, 而且每次进程间的切换都会消耗CPU周期并丢弃CPU高速缓存中的数据(所有处理过程是在一个简单的循环中, 由一个线程完成)</p>
<h4 id="1-1-创建进程的过程"><a href="#1-1-创建进程的过程" class="headerlink" title="1.1 创建进程的过程"></a>1.1 创建进程的过程</h4><ol>
<li>创建一个PCB</li>
<li>赋予一个统一进程标识符</li>
<li>为进程映象分配空间</li>
<li>初始化进程控制块</li>
<li>许多默认值 (如: 状态为 New, 无I/O设备或文件…)</li>
<li>设置相应的链接. 如: 把新进程加到就绪队列的链表中</li>
</ol>
<h4 id="1-2-进程切换步骤-PCB-TLB-："><a href="#1-2-进程切换步骤-PCB-TLB-：" class="headerlink" title="1.2 进程切换步骤(PCB/TLB)："></a>1.2 进程切换步骤(PCB/TLB)：</h4><ol>
<li>保存被中断进程的处理器现场信息</li>
<li>修改被中断进程的进程控制块的有关信息, 如进程状态等</li>
<li>把被中断进程的PCB加入有关队列</li>
<li>选择下一个占有处理器运行的进程</li>
<li>修改被选中进程的PCB的有关信息</li>
<li>根据被选中进程设置操作系统用到的地址转换和存储保护信息</li>
<li>根据被选中进程恢复处理器现场</li>
</ol>
<h4 id="进程上下文："><a href="#进程上下文：" class="headerlink" title="进程上下文："></a>进程上下文：</h4><p>进程上下文：操作系统中把进程物理实体和支持进程运行的环境合称为进程上下文（context）. 进程实体+运行环境. </p>
<ul>
<li>用户级上下文：由用户程序块、用户数据块和用户堆栈组成的进程地址空间. </li>
<li>系统级上下文：由进程控制块、内存管理信息、进程环境块, 及系统堆栈等组成的进程地址空间. </li>
<li>寄存器上下文：由PSW寄存器和各类控制寄存器、地址寄存器、通用寄存器组成、用户栈指针等组成. </li>
</ul>
<h4 id="进程阻塞过程"><a href="#进程阻塞过程" class="headerlink" title="进程阻塞过程"></a>进程阻塞过程</h4><p>停止当前进程的执行；保存该进程的CPU现场信息；将进程状态改为阻塞态, 并将其PCB入相应的阻塞队列；转进程调度程序. </p>
<h4 id="进程唤醒过程"><a href="#进程唤醒过程" class="headerlink" title="进程唤醒过程"></a>进程唤醒过程</h4><p>首先把被阻塞的进程从等待该事件的阻塞队列中移出, 将其PCB中的现行状态由阻塞改为就绪, 然后再将该PCB插入到就绪队列中.<br>进程切换：中断处于运行态的进程运行, 让出处理器, 恢复新进程的状态, 使新进程投入运行.  当系统调度新进程占有处理器时, 新老进程随之发生上下文切换.  进程的运行被认为是在进程的上下文中执行的. </p>
<h3 id="2-线程"><a href="#2-线程" class="headerlink" title="2. 线程"></a>2. 线程</h3><p>线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源. 线程间通信主要通过共享内存, 上下文切换很快, 资源开销较少, 但相比进程不够稳定容易丢失数据. </p>
<p>一旦创建完线程, 你就无法决定他什么时候获得时间片, 什么时候让出时间片了, 你把它交给了内核. 因为程序的使用涉及大量的计算机资源配置, 把这活随意的交给用户程序, 非常容易让整个系统分分钟被搞跪, 资源分配也很难做到相对的公平. 所以核心的操作需要陷入内核(kernel), 切换到操作系统, 让老大帮你来做. </p>
<p>线程上下文一般只包含CPU上下文及其他的线程管理信息. 线程创建的开销主要取决于为线程堆栈的建立而分配内存的开销, 这些开销并不大. 线程上下文切换发生在两个线程需要同步的时候, 比如进入共享数据段. 切换只CPU寄存器值需要存储, 并随后用将要切换到的线程的原先存储的值重新加载到CPU寄存器中去. </p>
<p>有的时候碰着I/O访问, 阻塞了后面所有的计算. 空着也是空着, 老大就直接把CPU切换到其他进程, 让人家先用着. 当然除了I\O阻塞, 还有时钟阻塞等等.但是一切换进程得反复进入内核, 置换掉一大堆状态. 进程数一高, 大部分系统资源就被进程切换给吃掉了. 后来搞出线程的概念, 这个地方阻塞了, 但我还有其他地方的逻辑流可以计算, 这些逻辑流是共享一个地址空间的, 不用特别麻烦的切换页表、刷新TLB, 只要把寄存器刷新一遍就行, 能比切换进程开销少点. </p>
<h3 id="3-协程"><a href="#3-协程" class="headerlink" title="3. 协程"></a>3. 协程</h3><p>一个Coroutine如果处于block状态, 可以交出执行权, 让其他的coroutine继续执行(<strong>这个是由其实现调度的,对用户透明的</strong>). </p>
<p><strong>Coroutines使得开发者可以采用阻塞式的开发风格,却能够实现非阻塞I/O的效果隐式事件调度</strong></p>
<p>函数其实是协程的特例, 从Knuth老爷子的基本算法卷上看“子程序其实是协程的特例”. 子程序是什么？子程序（英语：Subroutine, procedure, function, routine, method, subprogram）, 就是函数嘛！所以协程也没什么了不起的, 就是种更一般意义的程序组件, 那你内存空间够大, 创建多少个函数还不是随你么？</p>
<p>协程可以通过yield来调用其它协程. 通过yield方式转移执行权的协程之间不是调用者与被调用者的关系, 而是彼此对称、平等的. 协程的起始处是第一个入口点, 在协程里, 返回点之后是接下来的入口点. 子例程的生命期遵循后进先出（最后一个被调用的子例程最先返回）；相反, 协程的生命期完全由他们的使用的需要决定. (continuation)</p>
<p>协程是一种用户态的轻量级线程, 协程的调度完全由用户控制. 协程拥有自己的寄存器上下文和栈. 协程调度切换时, 将寄存器上下文和栈保存到其他地方, 在切回来的时候, 恢复先前保存的寄存器上下文和栈, 直接操作栈则基本没有内核切换的开销, 可以不加锁的访问全局变量, 所以上下文的切换非常快. </p>
<p>协程可以通过yield来调用其它协程. 通过yield方式转移执行权的协程之间不是调用者与被调用者的关系, 而是彼此对称、平等的. 协程的起始处是第一个入口点, 在协程里, 返回点之后是接下来的入口点. 子例程的生命期遵循后进先出（最后一个被调用的子例程最先返回）；相反, 协程的生命期完全由他们的使用的需要决定. </p>
<p>协程和线程的区别是：协程避免了无意义的调度, 由此可以提高性能, 但也因此, 程序员必须自己承担调度的责任, 同时, 协程也失去了标准线程使用多CPU的能力</p>
<p>协程编写者可以有一是可控的切换时机, 二是很小的切换代价. 从操作系统有没有调度权上看, 协程就是因为不需要进行内核态的切换, 所以会使用它</p>
<h4 id="协程好处"><a href="#协程好处" class="headerlink" title="协程好处"></a><strong>协程好处</strong></h4><ul>
<li><p>无需线程上下文切换的开销</p>
</li>
<li><p>无需原子操作锁定及同步的开销</p>
</li>
<li><p>状态机：在一个子例程里实现状态机, 这里状态由该过程当前的出口／入口点确定；这可以产生可读性更高的代码. </p>
</li>
<li><p>角色模型：并行的角色模型, 例如计算机游戏. 每个角色有自己的过程（这又在逻辑上分离了代码）, 但他们自愿地向顺序执行各角色过程的中央调度器交出控制（这是合作式多任务的一种形式）. </p>
</li>
<li><p>产生器：它有助于输入／输出和对数据结构的通用遍历</p>
</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a><strong>缺点</strong></h4><ul>
<li>不能同时将 CPU 的多个核.不过现在使用协程的语言都用到了多调度器的架构, 单进程下的协程也能用多核了</li>
</ul>
<p>说了这么多, 无非是说明, coroutine 是从另一个方向演化而来, 它是对 continuation 概念的简化. Lua 设计者反复提到, coroutine is one-shot semi-continuation. </p>
<h1 id="其他说明"><a href="#其他说明" class="headerlink" title="其他说明"></a>其他说明</h1><ol>
<li>历史上先有协程,OS模拟多任务并发, 非抢占式</li>
<li>线程能利用多核达到真正的并行计算, 说线程性能不好是因为设计的不好, 有大量的锁/切换/等待. 同一时间只有一个协程拥有运行权, 所以相当于单线程的能力</li>
<li>说协程性能好的,<strong>真正原因是瓶颈在IO上面,这时候发挥不了线程的作用</strong></li>
<li>事件驱动, Callback也挺不错</li>
<li>协程可以用同步的代码感觉 写出异步的效果</li>
</ol>
<hr>
<p>分割线</p>
<hr>
<h2 id="进程、线程、协程的关系和区别："><a href="#进程、线程、协程的关系和区别：" class="headerlink" title="进程、线程、协程的关系和区别："></a>进程、线程、协程的关系和区别：</h2><p>方面: cpu调度, 上下文切换, 数 据共享, 多核cup利用率, 资源占用角度.</p>
<p>线程进程都是同步机制, 而协程则是异步,协程能保留上一次调用时的状态, 每次过程重入时, 就相当于进入上一次调用的状态(continuation)</p>
<p>线程和进程主要区别是在轻量级和重量级, 他们调度都是系统来的; 协程和线程的区别是在调度：协程避免了无意义的调度, 由此可以提高性能, 但也因此, 程序员必须自己承担调度的责任, 同时, 协程也失去了标准线程使用多CPU的能力. </p>
<ul>
<li><p>进程拥有自己独立的堆和栈, 既不共享堆, 亦不共享栈, 进程由操作系统调度. 一个进程死亡对其他进程没有影响</p>
</li>
<li><p>线程拥有自己独立的栈和共享的堆, 共享堆, 不共享栈, 线程亦由操作系统调度(标准线程是的), 没有自己独立的地址空间!!! 默认情况下, 线程栈的大小为1MB. <strong>线程私有栈</strong>, 一个线程挂掉将导致整个进程挂掉, 因为线程没有自己单独的内存地址空间. 当一个线程向非法地址读取或者写入(伴随栈溢出、读取或者访问了非法地址), 无法确认这个操作是否会影响同一进程中的其它线程, 所以只能是整个进程一起崩溃. (注意 这个崩溃不是 java 的异常哦, jvm 做了很多保护)</p>
</li>
<li><p>协程和线程一样共享堆, 不共享栈, 协程由程序员在协程的代码里显示调度. 栈内存（大概是4～5KB）</p>
</li>
</ul>
<ol>
<li><p>需要频繁创建销毁的优先用线程.  实例：web服务器. 来一个建立一个线程, 断了就销毁线程. 要是用进程, 创建和销毁的代价是很难承受的. </p>
</li>
<li><p>需要进行大量计算的优先使用线程.  所谓大量计算, 当然就是要消耗很多cpu, 切换频繁了, 这种情况先线程是最合适的. 实例：图像处理、算法处理. </p>
</li>
<li><p><strong>强相关的处理用线程, 弱相关的处理用进程</strong> 什么叫强相关、弱相关？理论上很难定义, 给个简单的例子就明白了.  一般的server需要完成如下任务：消息收发和消息处理. 消息收发和消息处理就是弱相关的任务, 而消息处理里面可能又分为消息解码、业务处理, 这两个任务相对来说相关性就要强多 了. 因此消息收发和消息处理可以分进程设计, 消息解码和业务处理可以分线程设计. </p>
</li>
<li><p>可能扩展到多机分布的用进程, 多核分布的用线程</p>
</li>
</ol>
<p>总结一下就是IO密集型一般使用多线程或者多进程, CPU密集型一般使用多进程, 强调非阻塞异步并发的一般都是使用协程, 当然有时候也是需要多进程线程池结合的, 或者是其他组合方式. Nginx 用一个进程可以跑满整个机器的 cpu(都是非阻塞的操作, 没有比这更快的了, Nginx 最近也引入了线程池, 对付会阻塞的任务), </p>
<hr>
<p>分割线</p>
<hr>
<h1 id="Continuation"><a href="#Continuation" class="headerlink" title="Continuation"></a>Continuation</h1><p>在计算机科学和程序设计中, 延续性（continuation）是一种对程序控制流程/状态的抽象表现形式.  延续性使程序状态信息具体化, 也可以理解为, 一个延续性以数据结构的形式表现了程序在运行过程中某一点的计算状态, 相应的数据内容可以被编程语言访问, 不被运行时环境所隐藏掉.<br>延续性包含了当前程序的栈（包括当前周期内的所有数据, 也就是本地变量）, 以及当前运行的位置. 一个延续的实例可以在将来被用做控制流, 被调用时它从所表达的状态开始恢复执行. </p>
<h1 id="协程wiki"><a href="#协程wiki" class="headerlink" title="协程wiki"></a>协程wiki</h1><p>适用于实现彼此熟悉的模块:合作式多任务, 迭代器, 无限列表和管道. </p>
<p><code>var q := new queue</code></p>
<h2 id="生产者协程"><a href="#生产者协程" class="headerlink" title="生产者协程"></a>生产者协程</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loop</span><br><span class="line">    while q is not full</span><br><span class="line">        create some new items</span><br><span class="line">        add the items to q</span><br><span class="line">    yield to consume</span><br></pre></td></tr></table></figure>

<h2 id="消费者协程"><a href="#消费者协程" class="headerlink" title="消费者协程"></a>消费者协程</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loop</span><br><span class="line">    while q is not empty</span><br><span class="line">        remove some items from q</span><br><span class="line">        use the items</span><br><span class="line">    yield to produce</span><br></pre></td></tr></table></figure>

<p><strong>每个协程在用yield命令向另一个协程交出控制时都尽可能做了更多的工作</strong>. 放弃控制使得另一个例程从这个例程停止的地方开始, 但因为现在队列被修改了所以他可以做更多事情. 尽管这个例子常用来介绍多线程, 实际没有必要用多线程实现这种动态：yield语句可以通过由一个协程向另一个协程直接分支的方式实现. </p>
<p>注意 在yield的时候,队列已经被改变了,下一个生产者(或者消费者)执行的时候,直接在这个中间状态上执行就好了.</p>
<h2 id="详细比较"><a href="#详细比较" class="headerlink" title="详细比较"></a>详细比较</h2><p>因为相对于子例程, 协程可以有多个入口和出口点, 可以用协程来实现任何的子例程. 事实上, 正如Knuth所说：“子例程是协程的特例. ”<br>每当子例程被调用时, 执行从被调用子例程的起始处开始；然而, 接下来的每次协程被调用时, 从协程返回（或yield）的位置接着执行.<br>因为子例程只返回一次, 要返回多个值就要通过集合的形式. 这在有些语言, 如Forth里很方便；而其他语言, 如C, 只允许单一的返回值, 所以就需要引用一个集合. 相反地, 因为协程可以返回多次, 返回多个值只需要在后继的协程调用中返回附加的值即可. 在后继调用中返回附加值的协程常被称为产生器.<br>子例程容易实现于堆栈之上, 因为子例程将调用的其他子例程作为下级. 相反地, 协程对等地调用其他协程, 最好的实现是用continuations（由有垃圾回收的堆实现）以跟踪控制流程. </p>
<h2 id="与subroutine-子例程-函数-比较"><a href="#与subroutine-子例程-函数-比较" class="headerlink" title="与subroutine(子例程 函数?)比较"></a>与subroutine(子例程 函数?)比较</h2><p>当一个subroutine被invoked,execution begins at the start,当它退出时就结束.一个subroutine的实例值返回一次,不持有状态between invocation.</p>
<p>coroutine可以退出通过调用其他coroutine,可能过会会回到在原coroutine调用的那个point(which may return to the point where they were invoked in the original coroutine).从coroutine的视角来看,它并没有退出,只是调用了其他的coroutine(yield).因此,一个coroutine实例holds了state,并且在调用之间会有不同.</p>
<p>两个coroutine yield to each other是对称的(平等的),而且不是调用和被调用的关系(caller-callee 像函数的调用栈?)</p>
<p>所有的subroutine都可以被认为是没有yield的coroutine</p>
<p>要实现subroutine只需要一个简单的栈,可以预先分配,当程序执行的时候.但是coroutine要对等的调用对方,最好的实现是使用continuation</p>
<h2 id="与generators相比"><a href="#与generators相比" class="headerlink" title="与generators相比"></a>与generators相比</h2><p>Generators,也叫semicoroutines.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">var q := new queue</span><br><span class="line"></span><br><span class="line">generator produce</span><br><span class="line">    loop</span><br><span class="line">        while q is not full</span><br><span class="line">            create some new items</span><br><span class="line">            add the items to q</span><br><span class="line">        yield consume</span><br><span class="line">generator consume</span><br><span class="line">    loop</span><br><span class="line">        while q is not empty</span><br><span class="line">            remove some items from q</span><br><span class="line">            use the items</span><br><span class="line">        yield produce</span><br><span class="line">subroutine dispatcher</span><br><span class="line">    var d := new dictionary(generator → iterator)</span><br><span class="line">    d[produce] := start produce</span><br><span class="line">    d[consume] := start consume</span><br><span class="line">    var current := produce</span><br><span class="line">    loop</span><br><span class="line">        current := next d[current]</span><br></pre></td></tr></table></figure>

<h2 id="与相互递归相比"><a href="#与相互递归相比" class="headerlink" title="与相互递归相比"></a>与相互递归相比</h2><p>Using coroutines for state machines or concurrency is similar to using mutual recursion with <strong>tail calls</strong></p>
<p>coroutine 使用yield而不是返回,可以复用execution,而不是重头开始运行.</p>
<p>递归必须使用共享变量或者参数传入状态,每一个递归的调用都需要一个新的栈帧.而在coroutine之间的passing control可以使用现存的contexts,可以简单的被实现为一个jump</p>
<p>coroutines yield rather than return, and then resume execution rather than restarting from the beginning, they are able to hold state, both variables (as in a closure) and execution point, and yields are not limited to being in tail position;</p>
<p>(很久以前的博客了, 不是原创, 参考了很多资料, 加着自己的理解)</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2019/07/19/Spark-DateType-Timestamp-cast-%E5%B0%8F%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/07/19/Spark-DateType-Timestamp-cast-%E5%B0%8F%E7%BB%93/" class="post-title-link" itemprop="url">Spark DateType/Timestamp cast 小结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-07-19 16:47:39" itemprop="dateCreated datePublished" datetime="2019-07-19T16:47:39+08:00">2019-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:18:47" itemprop="dateModified" datetime="2021-05-08T11:18:47+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在平时的 Spark 处理中常常会有把一个如 <code>2012-12-12</code> 这样的 date 类型转换成一个 long 的 Unix time 然后进行计算的需求.下面是一段示例代码:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">  <span class="type">Array</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">&quot;id&quot;</span>, <span class="type">IntegerType</span>, nullable = <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">&quot;birth&quot;</span>, <span class="type">DateType</span>, nullable = <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">&quot;time&quot;</span>, <span class="type">TimestampType</span>, nullable = <span class="literal">true</span>)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> data = <span class="type">Seq</span>(</span><br><span class="line">  <span class="type">Row</span>(<span class="number">1</span>, <span class="type">Date</span>.valueOf(<span class="string">&quot;2012-12-12&quot;</span>), <span class="type">Timestamp</span>.valueOf(<span class="string">&quot;2016-09-30 03:03:00&quot;</span>)),</span><br><span class="line">  <span class="type">Row</span>(<span class="number">2</span>, <span class="type">Date</span>.valueOf(<span class="string">&quot;2016-12-14&quot;</span>), <span class="type">Timestamp</span>.valueOf(<span class="string">&quot;2016-12-14 03:03:00&quot;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)</span><br></pre></td></tr></table></figure>

<h1 id="问题-amp-解决"><a href="#问题-amp-解决" class="headerlink" title="问题 &amp; 解决"></a>问题 &amp; 解决</h1><p>首先很直观的是直接把DateType cast 成 LongType, 如下:</p>
<p><code>df.select(df.col(&quot;birth&quot;).cast(LongType))</code></p>
<p>但是这样出来都是 null, 这是为什么? 答案就在 <code>org.apache.spark.sql.catalyst.expressions.Cast</code> 中, 先看 canCast 方法, 可以看到 DateType 其实是可以转成 NumericType 的, 然后再看下面castToLong的方法, 可以看到<code>case DateType =&gt; buildCast[Int](_, d =&gt; null)</code>居然直接是个 null, 看提交记录其实这边有过反复, 然后为了和 hive 统一, 所以返回最后还是返回 null 了.</p>
<p>虽然 DateType 不能直接 castToLong, 但是TimestampType可以, 所以这里的解决方案就是先把 DateType cast 成 TimestampType. 但是这里又会有一个非常坑爹的问题: <strong>时区问题</strong>.</p>
<p>首先明确一个问题, 就是这个放到了 spark 中的 2012-12-12 到底 UTC 还是我们当前时区? 答案是如果没有经过特殊配置, 这个2012-12-12代表的是 <strong>当前时区的 2012-12-12 00:00:00.</strong>, 对应 UTC 其实是: 2012-12-11 16:00:00, 少了8小时. 这里还顺便说明了Spark 入库 Date 数据的时候是带着时区的.</p>
<p>然后再看DateType cast toTimestampType  的代码, 可以看到<code>buildCast[Int](_, d =&gt; DateTimeUtils.daysToMillis(d, timeZone) * 1000)</code>, 这里是带着时区的, 但是 Spark SQL 默认会用当前机器的时区. 但是大家一般底层数据比如这个2016-09-30, 都是代表的 UTC 时间, 在用 Spark 处理数据的时候, 这个时间还是 UTC 时间, 只有通过 JDBC 出去的时间才会变成带目标时区的结果. 经过摸索, 这里有两种解决方案:</p>
<ol>
<li>配置 Spark 的默认时区<code>config(&quot;spark.sql.session.timeZone&quot;, &quot;UTC&quot;)</code>, 最直观. 这样直接写 <code>df.select(df.col(&quot;birth&quot;).cast(TimestampType).cast(LongType))</code> 就可以了.</li>
<li>不配置 conf, 正面刚: <code>df.select(from_utc_timestamp(to_utc_timestamp(df.col(&quot;birth&quot;), TimeZone.getTimeZone(&quot;UTC&quot;).getID), TimeZone.getDefault.getID).cast(LongType))</code>, 可以看到各种 cast, 这是区别:</li>
</ol>
<ul>
<li>没有配置 UTC: <code>from_utc_timestamp(to_utc_timestamp(lit(&quot;2012-12-11 16:00:00&quot;), TimeZone.getTimeZone(&quot;UTC&quot;).getID), TimeZone.getDefault.getID)</code></li>
<li>配置了 UTC: <code>from_utc_timestamp(to_utc_timestamp(lit(&quot;2012-12-12 00:00:00&quot;), TimeZone.getTimeZone(&quot;UTC&quot;).getID), TimeZone.getDefault.getID)</code> 多了8小时</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns true iff we can cast `from` type to `to` type.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">canCast</span></span>(from: <span class="type">DataType</span>, to: <span class="type">DataType</span>): <span class="type">Boolean</span> = (from, to) <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> (fromType, toType) <span class="keyword">if</span> fromType == toType =&gt; <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">NullType</span>, _) =&gt; <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> (_, <span class="type">StringType</span>) =&gt; <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">StringType</span>, <span class="type">BinaryType</span>) =&gt; <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">StringType</span>, <span class="type">BooleanType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">DateType</span>, <span class="type">BooleanType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">TimestampType</span>, <span class="type">BooleanType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  <span class="keyword">case</span> (_: <span class="type">NumericType</span>, <span class="type">BooleanType</span>) =&gt; <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">StringType</span>, <span class="type">TimestampType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">BooleanType</span>, <span class="type">TimestampType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">DateType</span>, <span class="type">TimestampType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  <span class="keyword">case</span> (_: <span class="type">NumericType</span>, <span class="type">TimestampType</span>) =&gt; <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">StringType</span>, <span class="type">DateType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">TimestampType</span>, <span class="type">DateType</span>) =&gt; <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">StringType</span>, <span class="type">CalendarIntervalType</span>) =&gt; <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">StringType</span>, _: <span class="type">NumericType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">BooleanType</span>, _: <span class="type">NumericType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">DateType</span>, _: <span class="type">NumericType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  <span class="keyword">case</span> (<span class="type">TimestampType</span>, _: <span class="type">NumericType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  <span class="keyword">case</span> (_: <span class="type">NumericType</span>, _: <span class="type">NumericType</span>) =&gt; <span class="literal">true</span></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">castToLong</span></span>(from: <span class="type">DataType</span>): <span class="type">Any</span> =&gt; <span class="type">Any</span> = from <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">StringType</span> =&gt;</span><br><span class="line">    <span class="keyword">val</span> result = <span class="keyword">new</span> <span class="type">LongWrapper</span>()</span><br><span class="line">    buildCast[<span class="type">UTF8String</span>](_, s =&gt; <span class="keyword">if</span> (s.toLong(result)) result.value <span class="keyword">else</span> <span class="literal">null</span>)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">BooleanType</span> =&gt;</span><br><span class="line">    buildCast[<span class="type">Boolean</span>](_, b =&gt; <span class="keyword">if</span> (b) <span class="number">1</span>L <span class="keyword">else</span> <span class="number">0</span>L)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">DateType</span> =&gt;</span><br><span class="line">    buildCast[<span class="type">Int</span>](_, d =&gt; <span class="literal">null</span>)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">TimestampType</span> =&gt;</span><br><span class="line">    buildCast[<span class="type">Long</span>](_, t =&gt; timestampToLong(t))</span><br><span class="line">  <span class="keyword">case</span> x: <span class="type">NumericType</span> =&gt;</span><br><span class="line">    b =&gt; x.numeric.asInstanceOf[<span class="type">Numeric</span>[<span class="type">Any</span>]].toLong(b)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TimestampConverter</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">castToTimestamp</span></span>(from: <span class="type">DataType</span>): <span class="type">Any</span> =&gt; <span class="type">Any</span> = from <span class="keyword">match</span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">case</span> <span class="type">DateType</span> =&gt;</span><br><span class="line">    buildCast[<span class="type">Int</span>](_, d =&gt; <span class="type">DateTimeUtils</span>.daysToMillis(d, timeZone) * <span class="number">1000</span>)</span><br><span class="line">  <span class="comment">// TimestampWritable.decimalToTimestamp</span></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Given a timestamp, which corresponds to a certain time of day in the given timezone, returns</span></span><br><span class="line"><span class="comment"> * another timestamp that corresponds to the same time of day in UTC.</span></span><br><span class="line"><span class="comment"> * @group datetime_funcs</span></span><br><span class="line"><span class="comment"> * @since 1.5.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_utc_timestamp</span></span>(ts: <span class="type">Column</span>, tz: <span class="type">String</span>): <span class="type">Column</span> = withExpr &#123;</span><br><span class="line">  <span class="type">ToUTCTimestamp</span>(ts.expr, <span class="type">Literal</span>(tz))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Given a timestamp, which corresponds to a certain time of day in UTC, returns another timestamp</span></span><br><span class="line"><span class="comment"> * that corresponds to the same time of day in the given timezone.</span></span><br><span class="line"><span class="comment"> * @group datetime_funcs</span></span><br><span class="line"><span class="comment"> * @since 1.5.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">from_utc_timestamp</span></span>(ts: <span class="type">Column</span>, tz: <span class="type">String</span>): <span class="type">Column</span> = withExpr &#123;</span><br><span class="line">  <span class="type">FromUTCTimestamp</span>(ts.expr, <span class="type">Literal</span>(tz))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Deep-dive"><a href="#Deep-dive" class="headerlink" title="Deep dive"></a>Deep dive</h1><p>配置源码解读:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">SESSION_LOCAL_TIMEZONE</span> = buildConf(<span class="string">&quot;spark.sql.session.timeZone&quot;</span>).stringConf.createWithDefaultFunction(() =&gt; <span class="type">TimeZone</span>.getDefault.getID)</span><br></pre></td></tr></table></figure>

<p><code>def sessionLocalTimeZone: String = getConf(SQLConf.SESSION_LOCAL_TIMEZONE)</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Replace [[TimeZoneAwareExpression]] without timezone id by its copy with session local</span></span><br><span class="line"><span class="comment"> * time zone.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ResolveTimeZone</span>(<span class="params">conf: <span class="type">SQLConf</span></span>) <span class="keyword">extends</span> <span class="title">Rule</span>[<span class="type">LogicalPlan</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> transformTimeZoneExprs: <span class="type">PartialFunction</span>[<span class="type">Expression</span>, <span class="type">Expression</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">TimeZoneAwareExpression</span> <span class="keyword">if</span> e.timeZoneId.isEmpty =&gt;</span><br><span class="line">      e.withTimeZone(conf.sessionLocalTimeZone)</span><br><span class="line">    <span class="comment">// Casts could be added in the subquery plan through the rule TypeCoercion while coercing</span></span><br><span class="line">    <span class="comment">// the types between the value expression and list query expression of IN expression.</span></span><br><span class="line">    <span class="comment">// We need to subject the subquery plan through ResolveTimeZone again to setup timezone</span></span><br><span class="line">    <span class="comment">// information for time zone aware expressions.</span></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">ListQuery</span> =&gt; e.withNewPlan(apply(e.plan))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">LogicalPlan</span> =</span><br><span class="line">    plan.transformAllExpressions(transformTimeZoneExprs)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resolveTimeZones</span></span>(e: <span class="type">Expression</span>): <span class="type">Expression</span> = e.transform(transformTimeZoneExprs)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Mix-in trait for constructing valid [[Cast]] expressions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">CastSupport</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Configuration used to create a valid cast expression.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">conf</span></span>: <span class="type">SQLConf</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Create a Cast expression with the session local time zone.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cast</span></span>(child: <span class="type">Expression</span>, dataType: <span class="type">DataType</span>): <span class="type">Cast</span> = &#123;</span><br><span class="line">    <span class="type">Cast</span>(child, dataType, <span class="type">Option</span>(conf.sessionLocalTimeZone))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>org.apache.spark.sql.catalyst.analysis.Analyzer#batches 可以看到有<code>ResolveTimeZone</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> batches: <span class="type">Seq</span>[<span class="type">Batch</span>] = <span class="type">Seq</span>(</span><br><span class="line"></span><br><span class="line">  <span class="type">Batch</span>(<span class="string">&quot;Resolution&quot;</span>, fixedPoint,</span><br><span class="line">    <span class="type">ResolveTableValuedFunctions</span> ::</span><br><span class="line">    <span class="type">ResolveRelations</span> ::</span><br><span class="line">    <span class="type">ResolveReferences</span> ::</span><br><span class="line">    ...</span><br><span class="line">    <span class="type">ResolveTimeZone</span>(conf) ::</span><br><span class="line">    <span class="type">ResolvedUuidExpressions</span> ::</span><br><span class="line">    <span class="type">TypeCoercion</span>.typeCoercionRules(conf) ++</span><br><span class="line">    extendedResolutionRules : _*),</span><br><span class="line">  <span class="type">Batch</span>(<span class="string">&quot;Post-Hoc Resolution&quot;</span>, <span class="type">Once</span>, postHocResolutionRules: _*),</span><br><span class="line">  <span class="type">Batch</span>(<span class="string">&quot;View&quot;</span>, <span class="type">Once</span>,</span><br><span class="line">    <span class="type">AliasViewChild</span>(conf)),</span><br><span class="line">  <span class="type">Batch</span>(<span class="string">&quot;Nondeterministic&quot;</span>, <span class="type">Once</span>,</span><br><span class="line">    <span class="type">PullOutNondeterministic</span>),</span><br><span class="line">  <span class="type">Batch</span>(<span class="string">&quot;UDF&quot;</span>, <span class="type">Once</span>,</span><br><span class="line">    <span class="type">HandleNullInputsForUDF</span>),</span><br><span class="line">  <span class="type">Batch</span>(<span class="string">&quot;FixNullability&quot;</span>, <span class="type">Once</span>,</span><br><span class="line">    <span class="type">FixNullability</span>),</span><br><span class="line">  <span class="type">Batch</span>(<span class="string">&quot;Subquery&quot;</span>, <span class="type">Once</span>,</span><br><span class="line">    <span class="type">UpdateOuterReferences</span>),</span><br><span class="line">  <span class="type">Batch</span>(<span class="string">&quot;Cleanup&quot;</span>, fixedPoint,</span><br><span class="line">    <span class="type">CleanupAliases</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h2 id="Test-Example"><a href="#Test-Example" class="headerlink" title="Test Example"></a>Test Example</h2><h4 id="对于时区理解"><a href="#对于时区理解" class="headerlink" title="对于时区理解"></a>对于时区理解</h4><p>在不同的时区下 sql.Timestamp 对象的表现:</p>
<p>这里是 GMT+8:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Timestamp &quot;2014-06-24 07:22:15.0&quot;</span><br><span class="line">    - fastTime = 1403565735000</span><br><span class="line">    - &quot;2014-06-24T07:22:15.000+0700&quot;</span><br></pre></td></tr></table></figure>

<p>如果是 GMT+7, 会显示如下,可以看到是同一个毫秒数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Timestamp</span> <span class="string">&quot;2014-06-24 06:22:15.0&quot;</span></span><br><span class="line">    - fastTime = <span class="number">1403565735000</span></span><br><span class="line">    - <span class="string">&quot;2014-06-24T06:22:15.000+0700&quot;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">test(<span class="string">&quot;ColumnBatch&quot;</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">    <span class="type">Array</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">&quot;id&quot;</span>, <span class="type">IntegerType</span>, nullable = <span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">&quot;birth&quot;</span>, <span class="type">DateType</span>, nullable = <span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">&quot;time&quot;</span>, <span class="type">TimestampType</span>, nullable = <span class="literal">true</span>)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> columnarBatch = <span class="type">ColumnarBatch</span>.allocate(schema, <span class="type">MemoryMode</span>.<span class="type">ON_HEAP</span>, <span class="number">1024</span>)</span><br><span class="line">  <span class="keyword">val</span> c0 = columnarBatch.column(<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">val</span> c1 = columnarBatch.column(<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">val</span> c2 = columnarBatch.column(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  c0.putInt(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">  <span class="comment">// 1355241600, /3600/24 s to days</span></span><br><span class="line">  c1.putInt(<span class="number">0</span>, <span class="number">1355241600</span> / <span class="number">3600</span> / <span class="number">24</span>)</span><br><span class="line">  <span class="comment">// microsecond</span></span><br><span class="line">  c2.putLong(<span class="number">0</span>, <span class="number">1355285532000000</span>L)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> internal0 = columnarBatch.getRow(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//a way converting internal row to unsafe row.</span></span><br><span class="line">  <span class="comment">//val convert = UnsafeProjection.create(schema)</span></span><br><span class="line">  <span class="comment">//val internal = convert.apply(internal0)</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> enc = <span class="type">RowEncoder</span>.apply(schema).resolveAndBind()</span><br><span class="line">  <span class="keyword">val</span> row = enc.fromRow(internal0)</span><br><span class="line">  <span class="keyword">val</span> df = spark.createDataFrame(<span class="type">Lists</span>.newArrayList(row), schema)</span><br><span class="line"></span><br><span class="line">  <span class="type">TimeZone</span>.setDefault(<span class="type">TimeZone</span>.getTimeZone(<span class="string">&quot;UTC&quot;</span>))</span><br><span class="line">  <span class="keyword">val</span> tsStr0 = df.select(col(<span class="string">&quot;time&quot;</span>)).head().getTimestamp(<span class="number">0</span>).toString</span><br><span class="line">  <span class="keyword">val</span> ts0 = df.select(col(<span class="string">&quot;time&quot;</span>).cast(<span class="type">LongType</span>)).head().getLong(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="type">TimeZone</span>.setDefault(<span class="type">TimeZone</span>.getTimeZone(<span class="string">&quot;GMT+8&quot;</span>))</span><br><span class="line">  <span class="keyword">val</span> tsStr1 = df.select(col(<span class="string">&quot;time&quot;</span>)).head().getTimestamp(<span class="number">0</span>).toString</span><br><span class="line">  <span class="keyword">val</span> ts1 = df.select(col(<span class="string">&quot;time&quot;</span>).cast(<span class="type">LongType</span>)).head().getLong(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  assert(<span class="literal">true</span>, <span class="string">&quot;2012-12-12 04:12:12.0&quot;</span>.equals(tsStr0))</span><br><span class="line">  assert(<span class="literal">true</span>, <span class="string">&quot;2012-12-12 12:12:12.0&quot;</span>.equals(tsStr1))</span><br><span class="line">  <span class="comment">// to long 之后毫秒数都是一样的</span></span><br><span class="line">  assert(<span class="literal">true</span>, ts0 == ts1)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="番外-ImplicitCastInputTypes"><a href="#番外-ImplicitCastInputTypes" class="headerlink" title="番外 : ImplicitCastInputTypes"></a>番外 : ImplicitCastInputTypes</h1><p>我们自己定义了一个Expr, 要求接受两个 input 为 DateType 的参数.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MockExpr</span>(<span class="params">d0: <span class="type">Expression</span>, d1: <span class="type">Expression</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">BinaryExpression</span> <span class="keyword">with</span> <span class="type">ImplicitCastInputTypes</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">left</span></span>: <span class="type">Expression</span> = d0</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">right</span></span>: <span class="type">Expression</span> = d1</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputTypes</span></span>: <span class="type">Seq</span>[<span class="type">AbstractDataType</span>] = <span class="type">Seq</span>(<span class="type">DateType</span>, <span class="type">DateType</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">IntegerType</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">nullSafeEval</span></span>(date0: <span class="type">Any</span>, date1: <span class="type">Any</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>假设我们有如下调用, 请问这个调用符合预期吗? 结论是符合的, 因为有<code>ImplicitCastInputTypes</code>.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lit(<span class="string">&quot;2012-11-12 12:12:12.0&quot;</span>).cast(<span class="type">TimestampType</span>)</span><br><span class="line">lit(<span class="string">&quot;2012-12-12 12:12:12.0&quot;</span>).cast(<span class="type">TimestampType</span>)</span><br><span class="line"><span class="type">Column</span>(<span class="type">MockExpr</span>(tsc1.expr, tsc2.expr))</span><br></pre></td></tr></table></figure>

<p><strong>org.apache.spark.sql.catalyst.analysis.TypeCoercion.ImplicitTypeCasts</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> e: <span class="type">ImplicitCastInputTypes</span> <span class="keyword">if</span> e.inputTypes.nonEmpty =&gt;</span><br><span class="line"><span class="keyword">val</span> children: <span class="type">Seq</span>[<span class="type">Expression</span>] = e.children.zip(e.inputTypes).map &#123; <span class="keyword">case</span> (in, expected) =&gt;</span><br><span class="line">  <span class="comment">// If we cannot do the implicit cast, just use the original input.</span></span><br><span class="line">  implicitCast(in, expected).getOrElse(in)</span><br><span class="line">&#125;</span><br><span class="line">e.withNewChildren(children)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">implicitCast</span></span>(e: <span class="type">Expression</span>, expectedType: <span class="type">AbstractDataType</span>): <span class="type">Option</span>[<span class="type">Expression</span>] = &#123;</span><br><span class="line">  implicitCast(e.dataType, expectedType).map &#123; dt =&gt;</span><br><span class="line">    <span class="keyword">if</span> (dt == e.dataType) e <span class="keyword">else</span> <span class="type">Cast</span>(e, dt)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>org.apache.spark.sql.catalyst.expressions.Cast#castToDate #DateConverter</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">castToDate</span></span>(from: <span class="type">DataType</span>): <span class="type">Any</span> =&gt; <span class="type">Any</span> = from <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">StringType</span> =&gt;</span><br><span class="line">    buildCast[<span class="type">UTF8String</span>](_, s =&gt; <span class="type">DateTimeUtils</span>.stringToDate(s).orNull)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">TimestampType</span> =&gt;</span><br><span class="line">    <span class="comment">// throw valid precision more than seconds, according to Hive.</span></span><br><span class="line">    <span class="comment">// Timestamp.nanos is in 0 to 999,999,999, no more than a second.</span></span><br><span class="line">    buildCast[<span class="type">Long</span>](_, t =&gt; <span class="type">DateTimeUtils</span>.millisToDays(t / <span class="number">1000</span>L, timeZone))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2019/03/29/Spark-SQL-Join-Deep-Dive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/03/29/Spark-SQL-Join-Deep-Dive/" class="post-title-link" itemprop="url">Spark SQL Join Deep Dive</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-03-29 18:20:19" itemprop="dateCreated datePublished" datetime="2019-03-29T18:20:19+08:00">2019-03-29</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:45:02" itemprop="dateModified" datetime="2021-05-08T11:45:02+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Join-基础"><a href="#Join-基础" class="headerlink" title="Join 基础"></a>Join 基础</h1><h2 id="Nested-Loop-Join"><a href="#Nested-Loop-Join" class="headerlink" title="Nested Loop Join"></a>Nested Loop Join</h2><ul>
<li>本质就是嵌套 for 循环</li>
<li>适用于被连接的数据子集较小</li>
<li>Nested Loop先扫描外表, 每读取一条记录，就去去另一张表(内表, 一般是带索引的大表)里查找. 若没有索引的话一般就不会选择 Nested Loop Join(针对传统数据库).</li>
</ul>
<h2 id="Hash-Join"><a href="#Hash-Join" class="headerlink" title="Hash Join"></a>Hash Join</h2><ul>
<li>大数据集连接时的常用方式, 可以减少一次 for 循环</li>
<li>优化器使用两个表中较小（相对较小）的表做 hash 表, 然后扫描较大的表并探测该 hash 表，找出与匹配的行。</li>
<li>适用于没有索引且较小的表完全可以放于内存中的情况</li>
<li>Hash Join只能应用于等值连接</li>
</ul>
<h2 id="Merge-Join"><a href="#Merge-Join" class="headerlink" title="Merge Join"></a>Merge Join</h2><ul>
<li>for 循环次数同 Hash Join</li>
<li>排序两个表, 然后遍历第一个表, 在第二表中找对应的 key, 由于是排序的, 查找的复杂度会小于 O(n)</li>
<li>主要开销在排序, 如果表的 key 已经是排序的话, 开销比较小</li>
</ul>
<h1 id="Spark-Join-框架"><a href="#Spark-Join-框架" class="headerlink" title="Spark Join 框架"></a>Spark Join 框架</h1><h2 id="基本执行框架"><a href="#基本执行框架" class="headerlink" title="基本执行框架"></a>基本执行框架</h2><ul>
<li>参与Join操作的两张表分别被称为流式表（StreamTable）和构建表（BuildTable）, 一般来说系统会默认将大表设定为流式表，将小表设定为构建表</li>
</ul>
<p><img src="/2019/03/29/Spark-SQL-Join-Deep-Dive/20190630105331.png" alt="Spark SQL 内核剖析"></p>
<h3 id="无-Shuffle-的-join"><a href="#无-Shuffle-的-join" class="headerlink" title="无 Shuffle 的 join"></a>无 Shuffle 的 join</h3><h4 id="BroadcastJoinExec"><a href="#BroadcastJoinExec" class="headerlink" title="BroadcastJoinExec"></a>BroadcastJoinExec</h4><p>通过将小表 broadcast 到每个 executor 节点上, 从而避免大表产生 shuffle.</p>
<p><img src="/2019/03/29/Spark-SQL-Join-Deep-Dive/20190630164046.png" alt="Spark SQL 内核剖析"></p>
<p><img src="/2019/03/29/Spark-SQL-Join-Deep-Dive/20190630164058.png" alt="Spark SQL 内核剖析"></p>
<h5 id="选择条件"><a href="#选择条件" class="headerlink" title="选择条件"></a>选择条件</h5><ol>
<li>首先看有没有 hints, 如果有 hints, 直接选用 BHJ</li>
<li>如果能够广播非构建(build) 表, <code>JoinSelection#canBroadcast&#123; plan.stats.sizeInBytes &lt;= conf.autoBroadcastJoinThreshold &#125;</code>, 默认 broadcast 的 threshold 是10Mb, 也会选用 BHJ.</li>
</ol>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ol>
<li>每个 executor 上都会有一份表的数据, 有冗余</li>
<li>进行 broadcast 会拉数据到 driver 端, 对 driver 内存造成压力</li>
</ol>
<h3 id="有-Shuffle-的-join"><a href="#有-Shuffle-的-join" class="headerlink" title="有 Shuffle 的 join"></a>有 Shuffle 的 join</h3><p>在 Spark SQL 中, 最直观的进行 join 的操作如下:</p>
<ol>
<li>对两个表分别对应 Join Key 进行 shuffle, 这样两个表上相同的 join key 的记录会在一个分区上, 方便进行 join 操作</li>
<li>对每个分区的记录进行 join(有Hash/Sort 两种方式, 下面介绍).</li>
</ol>
<h4 id="ShuffleHashJoinExec"><a href="#ShuffleHashJoinExec" class="headerlink" title="ShuffleHashJoinExec"></a>ShuffleHashJoinExec</h4><p><img src="/2019/03/29/Spark-SQL-Join-Deep-Dive/20190630164357.png"></p>
<p>在 shuffle 过后, 对每个分区中的小表构造出一张 hash 表</p>
<h5 id="选择条件-1"><a href="#选择条件-1" class="headerlink" title="选择条件"></a>选择条件</h5><ol>
<li>当不 preferSortMergeJoin 时, 才会看下面的条件, 不然直接会用 SMJ</li>
<li>一边需要 <code>canBuildLocalHashMap</code>: <code>plan.stats.sizeInBytes &lt; conf.autoBroadcastJoinThreshold * conf.numShufflePartitions</code></li>
<li>小表的数据量要比大表小很多<code>muchSmaller</code>: <code>a.stats.sizeInBytes * 3 &lt;= b.stats.sizeInBytes</code></li>
<li>join cond 的key 没有排序</li>
</ol>
<h4 id="SortMergeJoinExec"><a href="#SortMergeJoinExec" class="headerlink" title="SortMergeJoinExec"></a>SortMergeJoinExec</h4><h5 id="选择条件-2"><a href="#选择条件-2" class="headerlink" title="选择条件"></a>选择条件</h5><p><img src="/2019/03/29/Spark-SQL-Join-Deep-Dive/20190805222307.png"></p>
<p>当两个表都较大时, 会选用这种 SMJ.</p>
<h1 id="Spark-Join-Selection-Code"><a href="#Spark-Join-Selection-Code" class="headerlink" title="Spark Join Selection Code"></a>Spark Join Selection Code</h1><p><img src="/2019/03/29/Spark-SQL-Join-Deep-Dive/20190805222901.png"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">Seq</span>[<span class="type">SparkPlan</span>] = plan <span class="keyword">match</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// --- BroadcastHashJoin --------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// broadcast hints were specified</span></span><br><span class="line">  <span class="keyword">case</span> <span class="type">ExtractEquiJoinKeys</span>(joinType, leftKeys, rightKeys, condition, left, right)</span><br><span class="line">    <span class="keyword">if</span> canBroadcastByHints(joinType, left, right) =&gt;</span><br><span class="line">    <span class="keyword">val</span> buildSide = broadcastSideByHints(joinType, left, right)</span><br><span class="line">    <span class="type">Seq</span>(joins.<span class="type">BroadcastHashJoinExec</span>(</span><br><span class="line">      leftKeys, rightKeys, joinType, buildSide, condition, planLater(left), planLater(right)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// broadcast hints were not specified, so need to infer it from size and configuration.</span></span><br><span class="line">  <span class="keyword">case</span> <span class="type">ExtractEquiJoinKeys</span>(joinType, leftKeys, rightKeys, condition, left, right)</span><br><span class="line">    <span class="keyword">if</span> canBroadcastBySizes(joinType, left, right) =&gt;</span><br><span class="line">    <span class="keyword">val</span> buildSide = broadcastSideBySizes(joinType, left, right)</span><br><span class="line">    <span class="type">Seq</span>(joins.<span class="type">BroadcastHashJoinExec</span>(</span><br><span class="line">      leftKeys, rightKeys, joinType, buildSide, condition, planLater(left), planLater(right)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// --- ShuffledHashJoin ---------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="type">ExtractEquiJoinKeys</span>(joinType, leftKeys, rightKeys, condition, left, right)</span><br><span class="line">     <span class="keyword">if</span> !conf.preferSortMergeJoin &amp;&amp; canBuildRight(joinType) &amp;&amp; canBuildLocalHashMap(right)</span><br><span class="line">       &amp;&amp; muchSmaller(right, left) ||</span><br><span class="line">       !<span class="type">RowOrdering</span>.isOrderable(leftKeys) =&gt;</span><br><span class="line">    <span class="type">Seq</span>(joins.<span class="type">ShuffledHashJoinExec</span>(</span><br><span class="line">      leftKeys, rightKeys, joinType, <span class="type">BuildRight</span>, condition, planLater(left), planLater(right)))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="type">ExtractEquiJoinKeys</span>(joinType, leftKeys, rightKeys, condition, left, right)</span><br><span class="line">     <span class="keyword">if</span> !conf.preferSortMergeJoin &amp;&amp; canBuildLeft(joinType) &amp;&amp; canBuildLocalHashMap(left)</span><br><span class="line">       &amp;&amp; muchSmaller(left, right) ||</span><br><span class="line">       !<span class="type">RowOrdering</span>.isOrderable(leftKeys) =&gt;</span><br><span class="line">    <span class="type">Seq</span>(joins.<span class="type">ShuffledHashJoinExec</span>(</span><br><span class="line">      leftKeys, rightKeys, joinType, <span class="type">BuildLeft</span>, condition, planLater(left), planLater(right)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// --- SortMergeJoin ------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="type">ExtractEquiJoinKeys</span>(joinType, leftKeys, rightKeys, condition, left, right)</span><br><span class="line">    <span class="keyword">if</span> <span class="type">RowOrdering</span>.isOrderable(leftKeys) =&gt;</span><br><span class="line">    joins.<span class="type">SortMergeJoinExec</span>(</span><br><span class="line">      leftKeys, rightKeys, joinType, condition, planLater(left), planLater(right)) :: <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// --- Without joining keys ------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Pick BroadcastNestedLoopJoin if one side could be broadcast</span></span><br><span class="line">  <span class="keyword">case</span> j @ logical.<span class="type">Join</span>(left, right, joinType, condition)</span><br><span class="line">      <span class="keyword">if</span> canBroadcastByHints(joinType, left, right) =&gt;</span><br><span class="line">    <span class="keyword">val</span> buildSide = broadcastSideByHints(joinType, left, right)</span><br><span class="line">    joins.<span class="type">BroadcastNestedLoopJoinExec</span>(</span><br><span class="line">      planLater(left), planLater(right), buildSide, joinType, condition) :: <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> j @ logical.<span class="type">Join</span>(left, right, joinType, condition)</span><br><span class="line">      <span class="keyword">if</span> canBroadcastBySizes(joinType, left, right) =&gt;</span><br><span class="line">    <span class="keyword">val</span> buildSide = broadcastSideBySizes(joinType, left, right)</span><br><span class="line">    joins.<span class="type">BroadcastNestedLoopJoinExec</span>(</span><br><span class="line">      planLater(left), planLater(right), buildSide, joinType, condition) :: <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Pick CartesianProduct for InnerJoin</span></span><br><span class="line">  <span class="keyword">case</span> logical.<span class="type">Join</span>(left, right, _: <span class="type">InnerLike</span>, condition) =&gt;</span><br><span class="line">    joins.<span class="type">CartesianProductExec</span>(planLater(left), planLater(right), condition) :: <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> logical.<span class="type">Join</span>(left, right, joinType, condition) =&gt;</span><br><span class="line">    <span class="keyword">val</span> buildSide = broadcastSide(</span><br><span class="line">      left.stats.hints.broadcast, right.stats.hints.broadcast, left, right)</span><br><span class="line">    <span class="comment">// This join could be very slow or OOM</span></span><br><span class="line">    joins.<span class="type">BroadcastNestedLoopJoinExec</span>(</span><br><span class="line">      planLater(left), planLater(right), buildSide, joinType, condition) :: <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// --- Cases where this strategy does not apply ---------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> _ =&gt; <span class="type">Nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2019/01/05/Linux-%E5%88%9D%E5%85%BB%E6%88%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/01/05/Linux-%E5%88%9D%E5%85%BB%E6%88%90/" class="post-title-link" itemprop="url">Linux 初养成</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-01-05 10:32:31" itemprop="dateCreated datePublished" datetime="2019-01-05T10:32:31+08:00">2019-01-05</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2019-07-02 15:45:36" itemprop="dateModified" datetime="2019-07-02T15:45:36+08:00">2019-07-02</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="shell-基础知识"><a href="#shell-基础知识" class="headerlink" title="shell 基础知识"></a>shell 基础知识</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$@</span>  传递给脚本或函数的所有参数</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -f  file ]    如果文件存在</span><br><span class="line"><span class="keyword">if</span> [ -d ...   ]    如果目录存在</span><br><span class="line"><span class="keyword">if</span> [ -s file  ]    如果文件存在且非空 </span><br><span class="line"><span class="keyword">if</span> [ -r file  ]    如果文件存在且可读</span><br><span class="line"><span class="keyword">if</span> [ -w file  ]    如果文件存在且可写</span><br><span class="line"><span class="keyword">if</span> [ -x file  ]    如果文件存在且可执行   </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>  [ -n <span class="variable">$string</span>  ]             如果string 非空(非0），返回0(<span class="literal">true</span>)  </span><br><span class="line"><span class="keyword">if</span>  [ -z <span class="variable">$string</span>  ]             如果string 为空</span><br><span class="line"><span class="keyword">if</span>  [ <span class="variable">$sting</span> ]                  如果string 非空，返回0 (和-n类似)    </span><br><span class="line"></span><br><span class="line">``</span><br><span class="line"></span><br><span class="line">|meta字符| meta字符作用|</span><br><span class="line">|--------|-------------|</span><br><span class="line">|= |设定变量|</span><br><span class="line">|$ | 作变量或运算替换(请不要与`shell prompt`混淆)|命令</span><br><span class="line">|&gt;| 输出重定向(重定向stdout)|</span><br><span class="line">|&lt;|输入重定向(重定向stdin)|</span><br><span class="line">|\||命令管道|</span><br><span class="line">|&amp;|重定向file descriptor或将命令至于后台(<span class="built_in">bg</span>)运行|</span><br><span class="line">|()|将其内部的命令置于nested subshell执行，或用于运算或变量替换|</span><br><span class="line">|&#123;&#125;|将其内的命令置于non-named <span class="keyword">function</span>中执行，或用在变量替换的界定范围|</span><br><span class="line">|;|在前一个命令执行结束时，而忽略其返回值，继续执行下一个命令|</span><br><span class="line">|&amp;&amp;|在前一个命令执行结束时，若返回值为<span class="literal">true</span>，继续执行下一个命令|</span><br><span class="line">|\|\||在前一个命令执行结束时，若返回值为<span class="literal">false</span>，继续执行下一个命令|</span><br><span class="line">|!|执行histroy列表中的命令|</span><br><span class="line">|... | ...|</span><br><span class="line"></span><br><span class="line"><span class="comment">## snipaste</span></span><br><span class="line"></span><br><span class="line">```sh</span><br><span class="line">DIR=<span class="string">&quot;<span class="subst">$( cd <span class="string">&quot;<span class="subst">$(dirname <span class="string">&quot;<span class="variable">$0</span>&quot;</span>)</span>&quot;</span> ; pwd -P )</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$0</span> 类似于python中的sys.argv[0]等。 <span class="variable">$0</span>指的是Shell本身的文件名。类似的有如果运行脚本的时候带参数，那么<span class="variable">$1</span> 就是第一个参数，依此类推。 </span><br><span class="line">dirname 用于取指定路径所在的目录 ，如 dirname /home/ikidou 结果为 /home。 </span><br><span class="line"></span><br><span class="line">$ 返回该命令的结果 </span><br><span class="line"></span><br><span class="line"><span class="built_in">pwd</span> -P 如果目录是链接时，格式：<span class="built_in">pwd</span> -P 显示出实际路径，而非使用连接（link）路径。</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">dir=$(<span class="built_in">cd</span> -P -- <span class="string">&quot;<span class="subst">$(dirname -- <span class="string">&quot;<span class="variable">$0</span>&quot;</span>)</span>&quot;</span> &amp;&amp; <span class="built_in">pwd</span> -P)</span><br><span class="line">会 <span class="built_in">cd</span> 到脚本所在的目录</span><br><span class="line"></span><br><span class="line">Note: A double dash (--) is used <span class="keyword">in</span> commands to signify the end of <span class="built_in">command</span> options, so files containing dashes or other special characters won<span class="string">&#x27;t break the command.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Always use `-P` flag with `cd` command, alias cd=&#x27;</span><span class="built_in">cd</span> -P<span class="string">&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">若将一个文件夹自己的快捷方式放到文件夹里,这样写脚本的时候就有可能会出现无限循环,当前路径名就会变得无限长,但是加上了-P命令后就可以避免无限循环的情况.</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A</span><br><span class="line">&gt; ./myscript</span><br><span class="line"></span><br><span class="line">B</span><br><span class="line">&gt; source myscript</span><br><span class="line"></span><br><span class="line">Short answer: sourcing will run the commands in the current shell process. executing will run the commands in a new shell process. </span><br></pre></td></tr></table></figure>

<h2 id="variable"><a href="#variable" class="headerlink" title="variable"></a>variable</h2><p>若从技术的细节来看，shell会依据IFS(Internal Field Seperator) 将command line所输入的文字拆解为”字段”(word/field)。 然后再针对特殊字符(meta)先作处理，最后重组整行command line。</p>
<p>变量替换(substitution) shell 之所以强大，其中的一个因素是它可以在命令行中对变量作 替换(substitution)处理。 在命令行中使用者可以使用$符号加上变量名称(除了用=定义变量名称之外)， 将变量值给替换出来，然后再重新组建命令行。</p>
<p>比方:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ A=ls</span><br><span class="line">$ B=la</span><br><span class="line">$ C=/tmp</span><br><span class="line">$ $A -$B $C</span><br></pre></td></tr></table></figure>

<p>会得到:<code>ls -la /tmp</code></p>
<p>echo命令只单纯将其argument送至”标准输出”(stdout, 通常是我们的屏幕)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ echo $A -$B $C</span><br><span class="line">&gt;&gt; ls -la /tmp</span><br><span class="line"></span><br><span class="line">$ echo $&#123;C&#125;</span><br><span class="line">&gt;&gt; /tmp</span><br></pre></td></tr></table></figure>

<h3 id="export"><a href="#export" class="headerlink" title="export"></a>export</h3><p>严格来说，我们在当前shell中所定义的变量，均属于 “本地变量”(local variable), 只有经过export命令的 “输出”处理，才能成为”环境变量”(environment variable)：</p>
<h2 id="与-差在哪？"><a href="#与-差在哪？" class="headerlink" title="()与{}差在哪？"></a>()与{}差在哪？</h2><p>要从一些命令执行的先后次序中得到结果， 如算术运算的2*(3+4)那样, 这时候，我们就可以引入”命令群组”(command group) 的概念：将许多命令集中处理。</p>
<ul>
<li>  <code>()</code> 将<code>command group</code>置于<code>sub-shell</code>(<code>子shell</code>)中去执行，也称 <code>nested sub-shell</code>。</li>
<li>  <code>&#123;&#125;</code> 则是在同一个<code>shell</code>内完成，也称<code>non-named command group</code>。</li>
</ul>
<p>在bash shell中, $()与``(反引号)都是用来做 命令替换(command substitution)的。</p>
<h1 id="Shell-snipaste"><a href="#Shell-snipaste" class="headerlink" title="Shell-snipaste"></a>Shell-snipaste</h1><ol>
<li><p><code>!$</code> : 代表上条命令的最后一个参数.</p>
</li>
<li><p><code>ssh-copy-id -i ~/.ssh/id_rsa.pub root@host</code>: 快速与root@host建立免密连接</p>
</li>
<li><p><code>lsof -i :7070 | awk &#39;NR==2 &#123;print $2&#125;&#39; | xargs kill -9</code> : 杀死占用端口为7070的进程</p>
</li>
<li><p><code>for i in &#123;1..10&#125;; do ll | wc -l; sleep 2; done</code> : 打印当前系统中句柄的打开数量</p>
</li>
<li><p><code>ll | awk &#39;&#123;print $9&#125;&#39;</code> : 只取文件名</p>
</li>
<li><p><code>find * | grep .DS_Store |xargs rm -rf</code> : 删除所有 .DS_Store</p>
</li>
<li><p><code>ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</code> : 切换时区</p>
</li>
<li><p><code>pbcopy &lt; ~/.ssh/id_rsa.pub</code> : 将公钥内容复制到剪切板</p>
</li>
<li><p><code>du -hsx * | sort -rh | head -n</code> : 打印当前目录下占磁盘空间最多的n个文件</p>
</li>
<li><p><code>/var/log</code> : 各个组件的 log, /var/log/messages 内核日志在这里.</p>
</li>
<li><p>CPU占用最多的前10个进程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps auxw|head -1;ps auxw|sort -rn -k3|head -10</span><br><span class="line">top （然后按下M，注意大写）</span><br></pre></td></tr></table></figure></li>
<li><p>内存消耗最多的前10个进程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps auxw|head -1;ps auxw|sort -rn -k4|head -10</span><br><span class="line">top （然后按下P，注意大写）</span><br></pre></td></tr></table></figure></li>
<li><p>虚拟内存使用最多的前10个进程<br><code>ps auxw|head -1;ps auxw|sort -rn -k5|head -10</code></p>
</li>
<li><p>配置 Linux 自启动 :<br>a. 把命令加到 <code>/etc/rc.local</code><br>b. 将脚本放到目录 /etc/profile.d/ 下，系统启动后就会自动执行该目录下的所有shell脚本<br>c. cp启动文件到 /etc/init.d/或者/etc/rc.d/init.d/（前者是后者的软连接）, <code>chkconfig --add cloudera-scm-agent</code>, <code>chkconfig cloudera-scm-agent on</code>, <code>chkconfig --list cloudera-scm-agent</code></p>
</li>
<li><p>关闭防火墙 : <code>service iptables status</code>, <code>service iptables stop</code>, <code>chkconfig iptables off</code>, <code>service iptables status</code></p>
</li>
<li><p>卸载 rpm 包 :</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa|grep -i java</span><br><span class="line">rpm -e --nodeps xxx yyy zzz</span><br></pre></td></tr></table></figure></li>
<li><p>du -sh, du -h –max-depth=1, df -h</p>
</li>
<li><p><code>find / -type f -name &quot;*.log&quot; | xargs grep &quot;ERROR&quot;</code> : 统计所有的log文件中，包含Error字符的行</p>
</li>
<li><p><code>ps -efL | grep [PID] | wc -l</code> : 查看某个进程创建的线程数</p>
</li>
<li><p>nohub</p>
</li>
<li><p>mount</p>
</li>
<li><p><code>find .-name *iso &gt;/tmp/res.txt &amp;</code>, 加&amp;放到后台执行, <code>bg</code> 可以查看后台运行的任务, <code>fg  %进程id</code> 放到前台执行</p>
</li>
<li><p><code>find / -name zkCli.sh</code>: /opt/cloudera/parcels/CDH-5.11.2-1.cdh5.11.2.p0.4/lib/zookeeper/bin/zkCli.sh<br>也可以在cdh的节点上, 直接 z 然后 tab, 会发现有 zookeeper-client 这样一个命令.</p>
</li>
</ol>
<p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/WeChatWorkScreenshot_9994c078-2f8a-40e8-9659-165afcde065d(1).png"></p>
<ol start="24">
<li><p>找某个目录下是否有包含某各类的 jar 包: <code>grep -r &quot;SparkSession&quot; jars</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> grep -r <span class="string">&quot;SparkSession&quot;</span> jars</span></span><br><span class="line">Binary file jars/spark-hive_2.11-2.2.1.jar matches</span><br><span class="line">Binary file jars/spark-sql_2.11-2.2.1.jar matches</span><br><span class="line">Binary file jars/hive-exec-1.2.1.spark2.jar matches</span><br><span class="line">Binary file jars/spark-repl_2.11-2.2.1.jar matches</span><br></pre></td></tr></table></figure></li>
<li><p><code>hadoop classpath</code>, <code>/var/log</code>, <code>/usr/hdp/2.4.0.0-169/</code></p>
</li>
<li><p>hive 默认 db 放的 HDFS 路径 <code>/user/hive/warehouse</code>, HIVE 查看表元数据: <code>desc formatted TEST_TIME</code></p>
</li>
<li><p><code>./spark-shell --master yarn --executor-cores 5 --executor-memory 5g --num-executors 3</code></p>
</li>
</ol>
<h2 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h2><ol>
<li>jstat -gc [pid] : 查看gc情况</li>
<li>jmap -histo [pid] : 按照对象内存大小排序, 注意会导致full gc</li>
<li>jstack -l pid : 用于查看线程是否存在死锁</li>
</ol>
<h1 id="装机必备"><a href="#装机必备" class="headerlink" title="装机必备"></a>装机必备</h1><h3 id="git"><a href="#git" class="headerlink" title="git"></a>git</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install git-core</span><br></pre></td></tr></table></figure>

<h3 id="zsh"><a href="#zsh" class="headerlink" title="zsh"></a>zsh</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install zsh</span><br><span class="line">git clone https://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh (sh -c &quot;$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)&quot;</span><br><span class="line">)</span><br><span class="line">git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions</span><br></pre></td></tr></table></figure>

<h3 id="htop"><a href="#htop" class="headerlink" title="htop"></a>htop</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget http://pkgs.repoforge.org/htop/htop-1.0.2-1.el6.rf.x86_64.rpm</span><br><span class="line">yum install -y epel-release</span><br><span class="line">yum install -y htop</span><br></pre></td></tr></table></figure>

<h3 id="设置文件打开数目和用户最大进程数"><a href="#设置文件打开数目和用户最大进程数" class="headerlink" title="设置文件打开数目和用户最大进程数"></a>设置文件打开数目和用户最大进程数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">查看文件打开数目</span><br><span class="line">ulimit -a</span><br><span class="line">查看用户最大进程数</span><br><span class="line">ulimit -u</span><br><span class="line">设置用户最大进程数</span><br><span class="line">vim /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line">结尾添加以下内容</span><br><span class="line">*       soft    nofile          32768</span><br><span class="line">*       hard    nofile          1048576</span><br><span class="line">*       soft    nproc           65536</span><br><span class="line">*       hard    nproc           unlimited</span><br><span class="line">*       soft    memlock         unlimited</span><br><span class="line">*       hard    memlock         unlimited</span><br></pre></td></tr></table></figure>

<h3 id="percol"><a href="#percol" class="headerlink" title="percol"></a>percol</h3><p>brew install percol</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">function exists &#123; which $1 &amp;&gt; /dev/null &#125;</span><br><span class="line"></span><br><span class="line">if exists percol; then</span><br><span class="line">    function percol_select_history() &#123;</span><br><span class="line">        local tac</span><br><span class="line">        exists gtac &amp;&amp; tac=&quot;gtac&quot; || &#123; exists tac &amp;&amp; tac=&quot;tac&quot; || &#123; tac=&quot;tail -r&quot; &#125; &#125;</span><br><span class="line">        BUFFER=$(fc -l -n 1 | eval $tac | percol --query &quot;$LBUFFER&quot;)</span><br><span class="line">        CURSOR=$#BUFFER         # move cursor</span><br><span class="line">        zle -R -c               # refresh</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    zle -N percol_select_history</span><br><span class="line">    bindkey &#x27;^R&#x27; percol_select_history</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/12/30/Maven-%E6%89%93%E5%8C%85%E8%B6%9F%E5%9D%91%E4%B8%8E%E8%A7%A3%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/12/30/Maven-%E6%89%93%E5%8C%85%E8%B6%9F%E5%9D%91%E4%B8%8E%E8%A7%A3%E6%B3%95/" class="post-title-link" itemprop="url">Maven 打包趟坑与解法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-12-30 11:19:06" itemprop="dateCreated datePublished" datetime="2018-12-30T11:19:06+08:00">2018-12-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:17:53" itemprop="dateModified" datetime="2021-05-08T11:17:53+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p>首先明确:</p>
<ol start="0">
<li>当你使用 Maven 对项目打包时，你需要了解以下 3 个打包 plugin，它们分别是</li>
</ol>
<table>
<thead>
<tr>
<th>plugin</th>
<th>function</th>
</tr>
</thead>
<tbody><tr>
<td>maven-jar-plugin</td>
<td>maven 默认打包插件，用来创建 project jar</td>
</tr>
<tr>
<td>maven-shade-plugin</td>
<td>用来打可执行包，executable(fat) jar</td>
</tr>
<tr>
<td>maven-assembly-plugin</td>
<td>支持定制化打包方式，例如 apache 项目的打包方式</td>
</tr>
</tbody></table>
<p>不管你 dependences 里的 scope 设置为什么, mvn package 出来的 你 src 的 jar 包里, 只会有你的 class 文件, 不会有所依赖的 jar 包, 可以通过 maven assembly 插件来做这个事情. 但是如果打成 war 包, 是会包含 compile scope 的依赖的. 而 provided 是要容器提供, 比如说 Tomcat, 会到 Tomcat 的 <code>$liferay-tomcat-home\webapps\ROOT\WEB-INF\lib</code> 目录下找.</p>
<ol>
<li><p>mvn install 出来的 jar 包只会包含自己的 src 的 classes. 即使你是 compile 的依赖, 也不会进去, 但是如果打成 war 包, 是会包含 compile scope 的依赖的. 而 provided 是要容器提供, 比如说 Tomcat, 会到 Tomcat 的 <code>$liferay-tomcat-home\webapps\ROOT\WEB-INF\lib</code> 目录下找. 而且 compile 的依赖是传递的, provided 的不传递.</p>
</li>
<li><p>可以通过 assembly/shade 插件把依赖的 jar 包打到一个 assembly.jar 包中去. 和源码的 jar 包可以是独立的, 也可以打到一起. 如果你一个依赖(D1)有两个版本(在父/子pom 中都有定义, 但是版本不一样), 在打出的 jar 包里只会有一个版本, 因为路径里不带版本的. 所以会出现各种 NoSuchMethodError 等等问题, 因为编译的时候都是各自用的正确的 D1 编译的出的 class. 但是运行时用到的 D1 只会有一个版本, 会有不匹配.</p>
</li>
</ol>
<p>所以, 不要在一个项目里, 不同 pom 里面尝试使用不同 version 的依赖. 来看个实例:</p>
<p>parquet-column 里会 shade 一个 fasttuil, 你 jar -tf parquet-column.jar 看他 里面会有这个 fastutil.</p>
<p><img src="/2018/12/30/Maven-%E6%89%93%E5%8C%85%E8%B6%9F%E5%9D%91%E4%B8%8E%E8%A7%A3%E6%B3%95/50973815.jpg"></p>
<p>可以看到有两个 jar 包, 一个 origin 不带 shade 的 fastutil, 另外一个是带着的, 也是放到 maven 仓库的 jar 包.<br><img src="/2018/12/30/Maven-%E6%89%93%E5%8C%85%E8%B6%9F%E5%9D%91%E4%B8%8E%E8%A7%A3%E6%B3%95/29736635.jpg"></p>
<p>jar -tf 确认<br><img src="/2018/12/30/Maven-%E6%89%93%E5%8C%85%E8%B6%9F%E5%9D%91%E4%B8%8E%E8%A7%A3%E6%B3%95/57385130.jpg"></p>
<h3 id="问题定位一般方法"><a href="#问题定位一般方法" class="headerlink" title="问题定位一般方法"></a>问题定位一般方法</h3><ol start="0">
<li><p>当你遇到 <code>java.lang.NoClassDefFoundError</code> 等错误的时候, 如果是在 IDEA 里运行的, 很有可能是 provided 依赖. 具体可以先看 IDEA 中打出的 classpath 里有没有依赖的包,<img src="/2018/12/30/Maven-%E6%89%93%E5%8C%85%E8%B6%9F%E5%9D%91%E4%B8%8E%E8%A7%A3%E6%B3%95/5.png"></p>
</li>
<li><p>如果有遇到什么 NoSuchMethodError, ClassNotFoundException 等等的, 先看看打印出来的 classpath. IDEA 里可以直接看, ClassNotFoundException 是真的没有这个 class:<br><img src="/2018/12/30/Maven-%E6%89%93%E5%8C%85%E8%B6%9F%E5%9D%91%E4%B8%8E%E8%A7%A3%E6%B3%95/76336665.jpg"></p>
</li>
<li><p>然后可以 double shift, 搜下出问题的类, 一般会跳出来多个:<br><img src="/2018/12/30/Maven-%E6%89%93%E5%8C%85%E8%B6%9F%E5%9D%91%E4%B8%8E%E8%A7%A3%E6%B3%95/65076854.jpg"></p>
</li>
<li><p>然后再用 <code>mvn dependency:tree</code> 看下当前 model 用的哪个版本的依赖</p>
</li>
</ol>
<p>然后就可以做相应的操作, 一般有以下几种:</p>
<pre><code>1. exclusive 相应依赖
2. 写死用一个版本的
3. 把 dependency 的依赖做 external 模块, 然后 shade + reloaction, 可以参见 spark 的 external model.
</code></pre>
<h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h3><p>程序里报错<code>Caused by: java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.canSerialize(Ljava/lang/Class;Ljava/util/concurrent/atomic/AtomicReference;)Z</code></p>
<p>但是无论从<code>mvn dependency:tree</code>, 还是运行时加载的 jar 包来看, 都是用了正确的 <code>jackson-databind-2.6.5.jar</code>. 问题就刁钻在它用的这个类, 其实不是 <code>jackson-databind</code> 里的, 而是其他的包里 shaed 但是又没有 relocation 的. 除非你把这个包给从依赖李去掉, 在这个包的里面的依赖里去掉, 或者最外面加正确版本的<code>jackson-databind-2.6.5.jar</code>都是没有用的, 见下图:</p>
<p><img src="/2018/12/30/Maven-%E6%89%93%E5%8C%85%E8%B6%9F%E5%9D%91%E4%B8%8E%E8%A7%A3%E6%B3%95/40098244.jpg"></p>
<p>所以画框里他 exclusive 也是没有用的. 解决方法就是我们做成 external 的, 并且 exclude 掉.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>external-influxdb<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>External-InfluxDB<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://kyligence.io<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Curator for KAP<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>apache<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kylin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">relativePath</span>&gt;</span>../../../pom.xml<span class="tag">&lt;/<span class="name">relativePath</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shadeBase</span>&gt;</span>org.apache.kylin.shaded.influxdb<span class="tag">&lt;/<span class="name">shadeBase</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shaded.curator.version</span>&gt;</span>2.12.0<span class="tag">&lt;/<span class="name">shaded.curator.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.influxdb<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>influxdb-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>20.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- cover log4j from parent pom--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jcl-over-slf4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--overwrite parent, need to upgrade this when upgrade grpc--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">include</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>log4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.slf4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">relocations</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>org.influxdb<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.org.influxdb<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.squareup.moshi<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.com.squareup.moshi<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>okhttp3<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.okhttp3<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>okio<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.okio<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>retrofit2<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.retrofit2<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.google<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.com.google.common<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">relocations</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="番外"><a href="#番外" class="headerlink" title="番外"></a>番外</h4><p>maven 的不同 scope 的官方定义:</p>
<ul>
<li><p>compile</p>
<p>  This is the default scope. Compile dependencies are available in all classpaths of a project. Furthermore, those dependencies are propagated to dependent projects(会有依赖传递).</p>
</li>
<li><p>provided</p>
<p>  This is much like compile, but indicates you expect the JDK or a container to provide the dependency at runtime. For example, when building a web application for the Java Enterprise Edition, you would set the dependency on the Servlet API and related Java EE APIs to scope provided because the web container provides those classes. This scope is only available on the compilation and test classpath, and is not transitive.</p>
</li>
</ul>
<p>我们经常回用到 <code>-pl :moduleName</code>, 看着很奇怪, 其实:前面省略的是 groupId.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/12/19/Spark-PRC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/12/19/Spark-PRC/" class="post-title-link" itemprop="url">Spark PRC</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-12-19 10:30:33" itemprop="dateCreated datePublished" datetime="2018-12-19T10:30:33+08:00">2018-12-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:45:10" itemprop="dateModified" datetime="2021-05-08T11:45:10+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li>receive：接收消息并处理，但不需要给客户端回复。</li>
<li>receiveAndReply：接收消息并处理，需要给客户端回复。回复是通过 RpcCall-Context来实现的。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * An end point for the RPC that defines what functions to trigger given a message.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * It is guaranteed that `onStart`, `receive` and `onStop` will be called in sequence.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The life-cycle of an endpoint is:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * constructor -&gt; onStart -&gt; receive* -&gt; onStop</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Note: `receive` can be called concurrently. If you want `receive` to be thread-safe, please use</span></span><br><span class="line"><span class="comment"> * [[ThreadSafeRpcEndpoint]]</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * If any error is thrown from one of [[RpcEndpoint]] methods except `onError`, `onError` will be</span></span><br><span class="line"><span class="comment"> * invoked with the cause. If `onError` throws an error, [[RpcEnv]] will ignore it.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">RpcEndpoint</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The [[RpcEnv]] that this [[RpcEndpoint]] is registered to.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">val</span> rpcEnv: <span class="type">RpcEnv</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The [[RpcEndpointRef]] of this [[RpcEndpoint]]. `self` will become valid when `onStart` is</span></span><br><span class="line"><span class="comment">   * called. And `self` will become `null` when `onStop` is called.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Note: Because before `onStart`, [[RpcEndpoint]] has not yet been registered and there is not</span></span><br><span class="line"><span class="comment">   * valid [[RpcEndpointRef]] for it. So don&#x27;t call `self` before `onStart` is called.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">self</span></span>: <span class="type">RpcEndpointRef</span> = &#123;</span><br><span class="line">    require(rpcEnv != <span class="literal">null</span>, <span class="string">&quot;rpcEnv has not been initialized&quot;</span>)</span><br><span class="line">    rpcEnv.endpointRef(<span class="keyword">this</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Process messages from [[RpcEndpointRef.send]] or [[RpcCallContext.reply)]]. If receiving a</span></span><br><span class="line"><span class="comment">   * unmatched message, [[SparkException]] will be thrown and sent to `onError`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(self + <span class="string">&quot; does not implement &#x27;receive&#x27;&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Process messages from [[RpcEndpointRef.ask]]. If receiving a unmatched message,</span></span><br><span class="line"><span class="comment">   * [[SparkException]] will be thrown and sent to `onError`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receiveAndReply</span></span>(context: <span class="type">RpcCallContext</span>): <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; context.sendFailure(<span class="keyword">new</span> <span class="type">SparkException</span>(self + <span class="string">&quot; won&#x27;t reply anything&quot;</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Invoked when any exception is thrown during handling messages.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onError</span></span>(cause: <span class="type">Throwable</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// By default, throw e and let RpcEnv handle it</span></span><br><span class="line">    <span class="keyword">throw</span> cause</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Invoked when `remoteAddress` is connected to the current node.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onConnected</span></span>(remoteAddress: <span class="type">RpcAddress</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// By default, do nothing.</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Invoked when `remoteAddress` is lost.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onDisconnected</span></span>(remoteAddress: <span class="type">RpcAddress</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// By default, do nothing.</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Invoked when some network error happens in the connection between the current node and</span></span><br><span class="line"><span class="comment">   * `remoteAddress`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onNetworkError</span></span>(cause: <span class="type">Throwable</span>, remoteAddress: <span class="type">RpcAddress</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// By default, do nothing.</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Invoked before [[RpcEndpoint]] starts to handle any message.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// By default, do nothing.</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Invoked when [[RpcEndpoint]] is stopping. `self` will be `null` in this method and you cannot</span></span><br><span class="line"><span class="comment">   * use it to send or ask messages.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// By default, do nothing.</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * A convenient method to stop [[RpcEndpoint]].</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> _self = self</span><br><span class="line">    <span class="keyword">if</span> (_self != <span class="literal">null</span>) &#123;</span><br><span class="line">      rpcEnv.stop(_self)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Akka中只要你持有了⼀个Actor的引⽤ ActorRef，那么你就可以使⽤此ActorRef向远端的Actor发起请求。 RpcEndpointRef也具有同等的效⽤，要向⼀个远端的RpcEndpoint发起请 求，你就必须持有这个RpcEndpoint的RpcEndpointRef。</p>
<p><img src="/2018/12/19/Spark-PRC/20181219104726.png"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A reference for a remote [[RpcEndpoint]]. [[RpcEndpointRef]] is thread-safe.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RpcEndpointRef</span>(<span class="params">conf: <span class="type">SparkConf</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Serializable</span> <span class="keyword">with</span> <span class="type">Logging</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> maxRetries = <span class="type">RpcUtils</span>.numRetries(conf)</span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> retryWaitMs = <span class="type">RpcUtils</span>.retryWaitMs(conf)</span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> defaultAskTimeout = <span class="type">RpcUtils</span>.askRpcTimeout(conf)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * return the address for the [[RpcEndpointRef]]</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">address</span></span>: <span class="type">RpcAddress</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Sends a one-way asynchronous message. Fire-and-forget semantics.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">send</span></span>(message: <span class="type">Any</span>): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Send a message to the corresponding [[RpcEndpoint.receiveAndReply)]] and return a [[Future]] to</span></span><br><span class="line"><span class="comment">   * receive the reply within the specified timeout.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * This method only sends the message once and never retries.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">ask</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](message: <span class="type">Any</span>, timeout: <span class="type">RpcTimeout</span>): <span class="type">Future</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Send a message to the corresponding [[RpcEndpoint.receiveAndReply)]] and return a [[Future]] to</span></span><br><span class="line"><span class="comment">   * receive the reply within a default timeout.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * This method only sends the message once and never retries.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">ask</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](message: <span class="type">Any</span>): <span class="type">Future</span>[<span class="type">T</span>] = ask(message, defaultAskTimeout)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Send a message to the corresponding [[RpcEndpoint]] and get its result within a default</span></span><br><span class="line"><span class="comment">   * timeout, or throw a SparkException if this fails even after the default number of retries.</span></span><br><span class="line"><span class="comment">   * The default `timeout` will be used in every trial of calling `sendWithReply`. Because this</span></span><br><span class="line"><span class="comment">   * method retries, the message handling in the receiver side should be idempotent.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Note: this is a blocking action which may cost a lot of time,  so don&#x27;t call it in a message</span></span><br><span class="line"><span class="comment">   * loop of [[RpcEndpoint]].</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param message the message to send</span></span><br><span class="line"><span class="comment">   * @tparam T type of the reply message</span></span><br><span class="line"><span class="comment">   * @return the reply message from the corresponding [[RpcEndpoint]]</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">askWithRetry</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](message: <span class="type">Any</span>): <span class="type">T</span> = askWithRetry(message, defaultAskTimeout)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Send a message to the corresponding [[RpcEndpoint.receive]] and get its result within a</span></span><br><span class="line"><span class="comment">   * specified timeout, throw a SparkException if this fails even after the specified number of</span></span><br><span class="line"><span class="comment">   * retries. `timeout` will be used in every trial of calling `sendWithReply`. Because this method</span></span><br><span class="line"><span class="comment">   * retries, the message handling in the receiver side should be idempotent.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Note: this is a blocking action which may cost a lot of time, so don&#x27;t call it in a message</span></span><br><span class="line"><span class="comment">   * loop of [[RpcEndpoint]].</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param message the message to send</span></span><br><span class="line"><span class="comment">   * @param timeout the timeout duration</span></span><br><span class="line"><span class="comment">   * @tparam T type of the reply message</span></span><br><span class="line"><span class="comment">   * @return the reply message from the corresponding [[RpcEndpoint]]</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">askWithRetry</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](message: <span class="type">Any</span>, timeout: <span class="type">RpcTimeout</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> Consider removing multiple attempts</span></span><br><span class="line">    <span class="keyword">var</span> attempts = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> lastException: <span class="type">Exception</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">while</span> (attempts &lt; maxRetries) &#123;</span><br><span class="line">      attempts += <span class="number">1</span></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> future = ask[<span class="type">T</span>](message, timeout)</span><br><span class="line">        <span class="keyword">val</span> result = timeout.awaitResult(future)</span><br><span class="line">        <span class="keyword">if</span> (result == <span class="literal">null</span>) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">&quot;RpcEndpoint returned null&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="keyword">throw</span> ie</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">          lastException = e</span><br><span class="line">          logWarning(<span class="string">s&quot;Error sending message [message = <span class="subst">$message</span>] in <span class="subst">$attempts</span> attempts&quot;</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (attempts &lt; maxRetries) &#123;</span><br><span class="line">        <span class="type">Thread</span>.sleep(retryWaitMs)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</span><br><span class="line">      <span class="string">s&quot;Error sending message [message = <span class="subst">$message</span>]&quot;</span>, lastException)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>RpcEndpoint：RPC端点，即RPC分布式环境中⼀个具体的实例，其可 以对指定的消息进⾏处理。由于RpcEndpoint是⼀个特质，所以需要提供 RpcEndpoint的实现类。特质RpcEndpoint已在前⽂详细介绍，此处不再赘 述。</p>
</li>
<li><p>RpcEndpointRef：RPC端点引⽤，即RPC分布式环境中⼀个具体实体 的引⽤，所谓引⽤实际是“spark://host:port/name”这种格式的地址。其中， host为端点所在RPC服务所在的主机IP，port是端点所在RPC服务的端⼜， name是端点实例的名称。抽象类RpcEndpointRef已在前⽂详细介绍，此处 不再赘述。</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E8%BF%B7%E6%80%9D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E8%BF%B7%E6%80%9D/" class="post-title-link" itemprop="url">大数据处理迷思</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-11-02 15:30:43" itemprop="dateCreated datePublished" datetime="2018-11-02T15:30:43+08:00">2018-11-02</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-07 22:40:46" itemprop="dateModified" datetime="2021-05-07T22:40:46+08:00">2021-05-07</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="OLAP-系统"><a href="#OLAP-系统" class="headerlink" title="OLAP 系统"></a>OLAP 系统</h3><p>No-SQL, New-SQL, 到了最后还是 SQL 好</p>
<ol>
<li>性能和稳定性的考量, 性能优先(MPP)? or 稳定性优先(DAG/MR)?</li>
<li>让数据更适合被计算<ol>
<li>将计算放在数据附近: Move compute logic, not data. 矛盾: 云, 存储和计算分离.</li>
<li>做大宽表, 减少 join 开销.</li>
<li>提前对数据进行分区/分桶/排序, 减少后续计算时候的 shuffle 以及 sort.</li>
<li>对数据进行预计算, 减少后续计算的聚合开销. 并且预计算等于说自己 own 了数据(多了一步构建 cube 的操作), 且由于是预计算的, 还可以进一步把查询结果 cache 到 Redis 这样的系统中, 下次一模一样的查询来直接用 cache 的 result 就行了, 这个 QPS 超级高.</li>
</ol>
</li>
<li>QPS 考量</li>
<li>很重要一点, 客户/业务方要怎么使用我们的系统? 换句话说, 我们能让客户按我们推荐的方式使用我们系统吗? 举例来说, 时间分区列我们推荐使用 date, 但是用户使用到了 timestamp, 造成了很多小文件, 怎么避免这种错误使用?</li>
<li>Schema change</li>
<li>数据点更新</li>
<li>如何调配资源</li>
<li>上/下层生态?</li>
<li>客户环境? 自己是否拥有集群环境</li>
</ol>
<h4 id="什么是一个好的-OLAP-系统"><a href="#什么是一个好的-OLAP-系统" class="headerlink" title="什么是一个好的 OLAP 系统"></a>什么是一个好的 OLAP 系统</h4><ol start="0">
<li>最最重要的: 稳定性</li>
<li>最重要的: 标准 SQL 支持</li>
<li>低延迟, 高 QPS</li>
<li>部署运维简单</li>
<li>支持 schema change</li>
<li>既能查聚合数据, 又可以查明细数据(此条对于预计算系统)</li>
</ol>
<h4 id="数据分析师之痛"><a href="#数据分析师之痛" class="headerlink" title="数据分析师之痛"></a>数据分析师之痛</h4><ol>
<li>精确去重</li>
<li>TopN</li>
<li>多 join/子查询</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/11/02/Hadoop-MR-%E5%92%8C-Spark-%E5%AF%B9%E6%AF%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/11/02/Hadoop-MR-%E5%92%8C-Spark-%E5%AF%B9%E6%AF%94/" class="post-title-link" itemprop="url">Hadoop MR 和 Spark 对比</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-11-02 15:21:13" itemprop="dateCreated datePublished" datetime="2018-11-02T15:21:13+08:00">2018-11-02</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:18:14" itemprop="dateModified" datetime="2021-05-08T11:18:14+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="0-启动开销"><a href="#0-启动开销" class="headerlink" title="0. 启动开销"></a>0. 启动开销</h3><p>总结: Spark 计算比 MapReduce 快的根本原因在于DAG计算模型, 但 MR 真正的缺点是抽象层次太低, 大量底层逻辑需要开发者手工完成. 但是也不是说 MR 就已经没用了, 没有最好的技术, 只有合适你需求的技术.</p>
<h3 id="1-启动开销"><a href="#1-启动开销" class="headerlink" title="1. 启动开销"></a>1. 启动开销</h3><p>Hadoop MapReduce 采用了多进程模型, 而Spark采用了多线程模型. </p>
<p>Hadoop MapReduce 每个 Mpa task/Reduce Task 都是一个 JVM, 是基于进程的, task 的启动时间在秒级, 然后用完后又立即释放, 不能被其他任务重用; Spark executor 是常驻的, taks 是基于线程的, 而且由于在一个 JVM 中, 方便数据共享.</p>
<p>但是基于进程的好处是每个 task 都可以控制自己的资源粒度, 而线程的资源隔离并没有保证.</p>
<p> MR 稳定是真的. 现在越来越觉得稳定性比性能重要很多, MR 虽然慢, 但是基本上能保证跑出结果. MR 真正的缺点是MR抽象层次太低, 大量底层逻辑需要开发者手工完成.</p>
<h3 id="2-DAG-优化"><a href="#2-DAG-优化" class="headerlink" title="2. DAG 优化"></a>2. DAG 优化</h3><h4 id="消除了冗余的-HDFS-读写"><a href="#消除了冗余的-HDFS-读写" class="headerlink" title="消除了冗余的 HDFS 读写"></a>消除了冗余的 HDFS 读写</h4><p>单个 MR job 和 Spark 其实可能也没啥区别, 差异主要在多个MR组成的复杂Job来和Spark比</p>
<p>对于一个 job, 会启动很多轮 MR 组合计算, MR 每次都会从 HDFS 读, 再写回到 HDFS, 下轮 MR 任务又要从 HDFS 读, 但是 Spark 只需要一个 job, 只读写 HDFS 一次. 中间只落本地磁盘.</p>
<h4 id="消除了冗余的-MapReduce-阶段"><a href="#消除了冗余的-MapReduce-阶段" class="headerlink" title="消除了冗余的 MapReduce 阶段"></a>消除了冗余的 MapReduce 阶段</h4><p>Spark lazy evaluation, 减少不必要的 stage, 可以减少 shuffle 次数, 要是没有 shuffle, Spark可以在内存中一次性完成这些操作.</p>
<h3 id="3-Cache"><a href="#3-Cache" class="headerlink" title="3. Cache"></a>3. Cache</h3><p>Spark 可以指定 Cache 某个 RDD, 以加速后面计算.</p>
<p>TBD: 从更高维度剖析 Hadoop MR 和 SQL 系统的区别.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/11/01/Spark-Read-Deep-Dive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/11/01/Spark-Read-Deep-Dive/" class="post-title-link" itemprop="url">Spark Paruqet Read Deep Dive</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-11-01 11:05:52" itemprop="dateCreated datePublished" datetime="2018-11-01T11:05:52+08:00">2018-11-01</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:19:31" itemprop="dateModified" datetime="2021-05-08T11:19:31+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="读取流程"><a href="#读取流程" class="headerlink" title="读取流程"></a>读取流程</h2><h3 id="入口点"><a href="#入口点" class="headerlink" title="入口点"></a>入口点</h3><p>我们以读取 parquet 文件为例, 使用 <code>ss.read.parquet(path)</code> 读取文件的时候, 入口点是<code>sparkSession.baseRelationToDataFrame</code>, 该方法接受一个 relation 返回一个 DataFrame. </p>
<p>问题的关键在于如何得到这个 relation(具体来说是 <code>HadoopFsRelation</code>), 其代码在 <code>DataSource#resolveRelation</code>, 里面代码比较多,因为要 infer schema 什么的, 所有代码最后只是要构造一个 <code>HadoopFsRelation</code>, 我们先来看 <code>HadoopFsRelation</code> 的定义:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">HadoopFsRelation</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    location: <span class="type">FileIndex</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    partitionSchema: <span class="type">StructType</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    dataSchema: <span class="type">StructType</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    bucketSpec: <span class="type">Option</span>[<span class="type">BucketSpec</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">    fileFormat: <span class="type">FileFormat</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]</span>)(<span class="params">val sparkSession: <span class="type">SparkSession</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">BaseRelation</span> <span class="keyword">with</span> <span class="type">FileRelation</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>下面我们来说说每个属性:</p>
<ol>
<li><p>location, 它的类型是 <code>FileIndex</code>, 其实这个 <code>FileIndex</code> 单独拿出来说内容也不少(我们可以使用这个来实现自己的 parquet index ), 这边只简单的提下(填个坑 之后详细讲下这个和各种 pruning), <code>FileIndex</code> 接口提供了一个 <code>listFiles</code> 的方法, 就是通过这个方法 Spark 知道要去哪里扫文件, 观察这个方法我们可以看到它传入了两个 filter, 可以用来做后续的 pruning, 如果有 partitionBy 的列, 那么就会多一级的目录, 所以这边返回的是一个 PartitionDirectory ( PartitionDirectory 是一个分区列的值带着一串文件列表).</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">listFiles</span></span>(partitionFilters: <span class="type">Seq</span>[<span class="type">Expression</span>], dataFilters: <span class="type">Seq</span>[<span class="type">Expression</span>]): <span class="type">Seq</span>[<span class="type">PartitionDirectory</span>]</span><br></pre></td></tr></table></figure></li>
<li><p>partitionSchema: partitionBy 的列</p>
</li>
<li><p>dataSchema: 数据文件本身的 schema, Spark 是从文件中 infer 出来的(如果用户没传的话)</p>
</li>
<li><p>bucketSpec: 如果有 bucketBy 的话, bucketBy的信息, 详细可以见我之前的博客 <a href="https://aaaaaaron.github.io/2019/10/09/Spark-Bucketing-Deep-Dive/">Spark bucketing 机制</a></p>
</li>
<li><p>fileFormat: 如果是 parquet 的话, 就是 ParquetFileFormat.</p>
</li>
<li><p>其他可选项</p>
</li>
</ol>
<p>我们要读取一个文件, 得到一个 DF, 其实最主要的就是要知道文件的 schema 和 location, 如果有这两个属性, 我们完全可以自己 new 一个 <code>HadoopFsRelation</code>, 传给 <code>baseRelationToDataFrame</code> (我们也可以写自己的 relation, 里面可以带我们自己需要的信息).</p>
<h3 id="Strategy-部分"><a href="#Strategy-部分" class="headerlink" title="Strategy 部分"></a>Strategy 部分</h3><p>Spark 靠各个 Strategy把 Logic plan 转换成 SparkPlan(以 Exec 结尾), . 读取文件会用到的 Strategy 是 <code>FileSourceStrategy</code>.</p>
<p>何时这个 Strategy 会被触发到? 主体代码在它的 apply 方法中, 在 apply 中可以看到, 当 match 到一个 <code>HadoopFsRelation</code> 时, 会触发到这个 Strategy. </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">PhysicalOperation</span>(projects, filters,</span><br><span class="line">      l @ <span class="type">LogicalRelation</span>(fsRelation: <span class="type">HadoopFsRelation</span>, _, table, _)) =&gt;</span><br></pre></td></tr></table></figure>

<p><code>FileSourceStrategy</code> 主要是用来生成 <code>FileSourceScanExec</code>(如果有 project 和 filter, 会生成 <code>FilterExec</code>/<code>ProjectExec</code>, 并把 <code>FileSourceScanExec</code> 作为他们的 <code>child</code>), 我们结合最后生成的 exec 的属性一起看看它到底做了什么.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> scan = <span class="type">FileSourceScanExec</span>(</span><br><span class="line">    fsRelation,</span><br><span class="line">    outputAttributes,</span><br><span class="line">    outputSchema,</span><br><span class="line">    partitionKeyFilters.toSeq,</span><br><span class="line">    bucketSet,</span><br><span class="line">    dataFilters,</span><br><span class="line">    table.map(_.identifier))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> afterScanFilter = afterScanFilters.toSeq.reduceOption(expressions.<span class="type">And</span>)</span><br><span class="line"><span class="keyword">val</span> withFilter = afterScanFilter.map(execution.<span class="type">FilterExec</span>(_, scan)).getOrElse(scan)</span><br><span class="line"><span class="keyword">val</span> withProjections = <span class="keyword">if</span> (projects == withFilter.output) &#123;</span><br><span class="line">  withFilter</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  execution.<span class="type">ProjectExec</span>(projects, withFilter)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="0">
<li>fsRelation: 上面的 <code>HadoopFsRelation</code></li>
<li>outputAttributes/outputSchema: 从logic plan上拿到 project 和 filter, 得到具体会用到的的是哪些列(列存, 不用扫描所有列)</li>
<li>partitionKeyFilters: 如果有 partition by 的列, 提取与其相关的 filter, 保存为 <code>partitionKeyFilters</code>.</li>
<li>bucketSet: 如果有 bucket by, 得到具体要扫哪些 buckets, 具体看我另外一篇关于 Spark bucket 的博客</li>
<li>dataFilters: 在没有 partition by 的列上的 filters</li>
<li>tableIdentifier: table 在 metastore 的 identify.</li>
</ol>
<h3 id="Exec-部分"><a href="#Exec-部分" class="headerlink" title="Exec 部分"></a>Exec 部分</h3><p>负责数据文件扫描的 spark plan 是 <code>FileSourceScanExec</code>, 借一张图来简要说明怎么会走到这个 Exec.</p>
<img src="/2018/11/01/Spark-Read-Deep-Dive/00014.jpeg" alt="Spark SQL 内核剖析" style="zoom:67%;">

<p>从图中可以看到, 从 SQL 生成了一颗逻辑执行计划树. 每个逻辑执行计划的节点都会转换成对应的一个物理执行计划(也是一颗树状结构). </p>
<img src="/2018/11/01/Spark-Read-Deep-Dive/20181118165922.png" style="zoom: 50%;">

<p>由于 Spark 会使用 Codegen,  <code>WholeStageCodegenExec</code> 是所有 Exec 的入口点. <code>child.asInstanceOf[CodegenSupport].inputRDDs()</code> 处于所有的 exec 的最头部, 会沿着 exec 树一层层调用下去最后会走到:<code>FileSourceScanExec#inputRDD</code>, 如图:</p>
<p>先到 <code>WholeStageCodegenExec</code> 的 <code>doExecute</code>, 这里要下面会提到的有两个地方, 一个是 <code>inputRDDs</code>, 另一个是 <code>buffer.hasNext</code> :</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doExecute</span></span>(): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">val</span> (ctx, cleanedSource) = doCodeGen()</span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">val</span> rdds = child.asInstanceOf[<span class="type">CodegenSupport</span>].inputRDDs()</span><br><span class="line">  assert(rdds.size &lt;= <span class="number">2</span>, <span class="string">&quot;Up to two input RDDs can be supported&quot;</span>)</span><br><span class="line">  <span class="keyword">if</span> (rdds.length == <span class="number">1</span>) &#123;</span><br><span class="line">    rdds.head.mapPartitionsWithIndex &#123; (index, iter) =&gt;</span><br><span class="line">      <span class="keyword">val</span> (clazz, _) = <span class="type">CodeGenerator</span>.compile(cleanedSource)</span><br><span class="line">      <span class="keyword">val</span> buffer = clazz.generate(references).asInstanceOf[<span class="type">BufferedRowIterator</span>]</span><br><span class="line">      buffer.init(index, <span class="type">Array</span>(iter))</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">InternalRow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> v = buffer.hasNext</span><br><span class="line">          <span class="keyword">if</span> (!v) durationMs += buffer.durationMs()</span><br><span class="line">          v</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>: <span class="type">InternalRow</span> = buffer.next()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="FileSourceScanExec"><a href="#FileSourceScanExec" class="headerlink" title="FileSourceScanExec"></a>FileSourceScanExec</h3><p>inputRDD 首先会调用具体 <code>FileFormat</code> 实现类的 <code>buildReaderWithPartitionValues</code> ,因为我们用的是 parquet 作为例子, 所以这里会是 <code>ParquetFileFormat</code>, build 出来一个 <code>readFunction : ((PartitionedFile) =&gt; Iterator[InternalRow])</code>, 顾名思义后面会用这个 func 来读取文件.</p>
<p>下一步的 <code>createBucketedReadRDD</code> 我们之前的博客分析过( <a href="https://aaaaaaron.github.io/2018/10/22/Spark-Parquet-file-split">Spark-Parquet-file-split</a> ), 主要是用来切分 partitions, 并且返回一个 FileScanRDD.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">lazy</span> <span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> readFile: (<span class="type">PartitionedFile</span>) =&gt; <span class="type">Iterator</span>[<span class="type">InternalRow</span>] =</span><br><span class="line">    relation.fileFormat.buildReaderWithPartitionValues(</span><br><span class="line">      sparkSession = relation.sparkSession,</span><br><span class="line">      dataSchema = relation.dataSchema,</span><br><span class="line">      partitionSchema = relation.partitionSchema,</span><br><span class="line">      requiredSchema = requiredSchema,</span><br><span class="line">      filters = pushedDownFilters,</span><br><span class="line">      options = relation.options,</span><br><span class="line">      hadoopConf = relation.sparkSession.sessionState.newHadoopConfWithOptions(relation.options))</span><br><span class="line"></span><br><span class="line">  relation.bucketSpec <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(bucketing) <span class="keyword">if</span> relation.sparkSession.sessionState.conf.bucketingEnabled =&gt;</span><br><span class="line">      createBucketedReadRDD(bucketing, readFile, selectedPartitions, relation)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      createNonBucketedReadRDD(readFile, selectedPartitions, relation)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Pushdown-filter"><a href="#Pushdown-filter" class="headerlink" title="Pushdown filter"></a>Pushdown filter</h4><p>首先在 <code>FileSourceStrategy.apply</code>, 拿到 logic plan 上的 Filter, 处理逻辑在 <code>PhysicalOperation#collectProjectsAndFilters</code>, 这里看个方法<code>splitConjunctivePredicates</code>, 这个方法会把 Filter 按照 And 切开, 但是也不是遍历着切, 如果你外面套着个 And, 才会切, 所以会得到一个 Seq:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">splitConjunctivePredicates</span></span>(condition: <span class="type">Expression</span>): <span class="type">Seq</span>[<span class="type">Expression</span>] = &#123;</span><br><span class="line">  condition <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">And</span>(cond1, cond2) =&gt;</span><br><span class="line">      splitConjunctivePredicates(cond1) ++ splitConjunctivePredicates(cond2)</span><br><span class="line">    <span class="keyword">case</span> other =&gt; other :: <span class="type">Nil</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在 <code>FileSourceStrategy</code> 中, 会用这个 filter 构造 dataFilters:<code>val dataFilters = normalizedFilters.filter(_.references.intersect(partitionSet).isEmpty)</code>, 这里会过滤掉 partition by 的列的 filter, 这个后面会单独传给 exec.</p>
<p>在 <code>ScanExec</code> 中, 会构造 pushedDownFilters: <code>dataFilters.flatMap(DataSourceStrategy.translateFilter)</code>, 它把 Expression 转成 sources.Filter.</p>
<p>注意这个 translateFilter 不支持任何的函数, 例如 cast, substr 等, filter 表达式中带着函数的, 会被直接跳过, 在最后的 <code>pushedDownFilters</code> 中, 也不会有这个 Expression 的 Filter.</p>
<h4 id="FileIndex"><a href="#FileIndex" class="headerlink" title="FileIndex"></a>FileIndex</h4><h2 id="TBD"><a href="#TBD" class="headerlink" title="TBD"></a>TBD</h2><h3 id="FileScanRDD"><a href="#FileScanRDD" class="headerlink" title="FileScanRDD"></a>FileScanRDD</h3><p>比较重要的是 <code>currentIterator:Iterator[Object]</code> 这个东西, <code>compute</code> 方法吐出去的就是这个 iter. 可以看到这个 iter 通过之前 FileFormat 里生成的 readFunction 来生成的.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">readCurrentFile</span></span>(): <span class="type">Iterator</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    readFunction(currentFile)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">FileNotFoundException</span> =&gt;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">FileNotFoundException</span>(</span><br><span class="line">        e.getMessage + <span class="string">&quot;\n&quot;</span> +</span><br><span class="line">          <span class="string">&quot;It is possible the underlying files have been updated. &quot;</span> +</span><br><span class="line">          <span class="string">&quot;You can explicitly invalidate the cache in Spark by &quot;</span> +</span><br><span class="line">          <span class="string">&quot;running &#x27;REFRESH TABLE tableName&#x27; command in SQL or &quot;</span> +</span><br><span class="line">          <span class="string">&quot;by recreating the Dataset/DataFrame involved.&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意这个 rdd.iterator<br>codegen</p>
<p>scan_mutableStateArray_0[0].hasNext() 调用到了 rdd 的 iter</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> void scan_nextBatch_0() <span class="keyword">throws</span> java.io.<span class="type">IOException</span> &#123;</span><br><span class="line">    long getBatchStart = <span class="type">System</span>.nanoTime();</span><br><span class="line">    <span class="keyword">if</span> (scan_mutableStateArray_0[<span class="number">0</span>].hasNext()) &#123;</span><br><span class="line">        scan_mutableStateArray_1[<span class="number">0</span>] = (org.apache.spark.sql.vectorized.<span class="type">ColumnarBatch</span>)scan_mutableStateArray_0[<span class="number">0</span>].next();</span><br><span class="line">        ((org.apache.spark.sql.execution.metric.<span class="type">SQLMetric</span>) references[<span class="number">0</span>] <span class="comment">/* numOutputRows */</span>).add(scan_mutableStateArray_1[<span class="number">0</span>].numRows());</span><br><span class="line">        scan_batchIdx_0 = <span class="number">0</span>;</span><br><span class="line">        scan_mutableStateArray_2[<span class="number">0</span>] = (org.apache.spark.sql.execution.vectorized.<span class="type">OnHeapColumnVector</span>) scan_mutableStateArray_1[<span class="number">0</span>].column(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    scan_scanTime_0 += <span class="type">System</span>.nanoTime() - getBatchStart;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="RecordReaderIterator"><a href="#RecordReaderIterator" class="headerlink" title="RecordReaderIterator"></a>RecordReaderIterator</h4><p>上面的 currentIterator 其实是一个 RecordReaderIterator, 里面包装了 RecordReader. 对于 Parquet 的实现是 <code>VectorizedParquetRecordReader</code>.</p>
<h5 id="initialize"><a href="#initialize" class="headerlink" title="initialize"></a>initialize</h5><pre><code>  blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);
  this.reader = new ParquetFileReader(
    configuration, footer.getFileMetaData(), file, blocks, requestedSchema.getColumns())
</code></pre>
<h5 id="nextKeyValue"><a href="#nextKeyValue" class="headerlink" title="nextKeyValue"></a>nextKeyValue</h5><h5 id="getCurrentValue"><a href="#getCurrentValue" class="headerlink" title="getCurrentValue"></a>getCurrentValue</h5><h5 id="checkEndOfRowGroup"><a href="#checkEndOfRowGroup" class="headerlink" title="checkEndOfRowGroup"></a>checkEndOfRowGroup</h5>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/10/30/Learning-Parquet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/10/30/Learning-Parquet/" class="post-title-link" itemprop="url">Learning Parquet</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-10-30 19:09:46" itemprop="dateCreated datePublished" datetime="2018-10-30T19:09:46+08:00">2018-10-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:11:48" itemprop="dateModified" datetime="2021-05-08T11:11:48+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Glossary"><a href="#Glossary" class="headerlink" title="Glossary"></a>Glossary</h1><ul>
<li><p>Row group: A logical horizontal partitioning of the data into rows. There is no physical structure that is guaranteed for a row group. A row group consists of a column chunk for each column in the dataset.</p>
</li>
<li><p>Column chunk: A chunk of the data for a particular column. These live in a particular row group and is guaranteed to be contiguous in the file.</p>
</li>
<li><p>Page: Column chunks are divided up into pages. A page is conceptually an indivisible unit (in terms of compression and encoding). There can be multiple page types which is interleaved in a column chunk.</p>
</li>
</ul>
<p>一个文件由一个或者多个 row groups 组成, 一个 row group 包含一个 column chunk per column. column chunk 包含一个或者多个 pages.</p>
<h2 id="Unit-of-parallelization"><a href="#Unit-of-parallelization" class="headerlink" title="Unit of parallelization"></a>Unit of parallelization</h2><ul>
<li>MapReduce - File/Row Group</li>
<li>IO - Column chunk</li>
<li>Encoding/Compression - Page</li>
</ul>
<p><img src="/2018/10/30/Learning-Parquet/FileLayout.gif"></p>
<p><img src="/2018/10/30/Learning-Parquet/FileFormat.gif"></p>
<p>![](Learning-Parquet/Parquet 文件格式.png)</p>
<h1 id="Deep"><a href="#Deep" class="headerlink" title="Deep"></a>Deep</h1><p>正是因为有 row group, 才使得 parquet 文件 splitable, 且每个 split 都有完整的 record(Spark 中也是根据 row group 来切 split 的). </p>
<p>Row groups are used to keep all the columns of each record in the same HDFS block so records can be reassembled from a single block.</p>
<p>你只可以 sort 一个 column, 所以 sort 你最常 filter 的列.</p>
<p>RowGroups are typically chosen to be pretty large to avoid the cost of random i/o</p>
<p>在 parquet 里，经常混用 block 和 row group 的概念，可以认为两者相等。</p>
<p>BlockMetaData</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> List&lt;ColumnChunkMetaData&gt; columns = <span class="keyword">new</span> ArrayList&lt;ColumnChunkMetaData&gt;();</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> rowCount;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> totalByteSize;</span><br><span class="line"><span class="keyword">private</span> String path;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * PageReader for a single column chunk. A column chunk contains</span></span><br><span class="line"><span class="comment"> * several pages, which are yielded one by one in order.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This implementation is provided with a list of pages, each of which</span></span><br><span class="line"><span class="comment"> * is decompressed and passed through.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ColumnChunkPageReader</span> <span class="keyword">implements</span> <span class="title">PageReader</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Reader for a sequence a page from a given column chunk</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">PageReader</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@return</span> the dictionary page in that chunk or null if none</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="function">DictionaryPage <span class="title">readDictionaryPage</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@return</span> the total number of values in the column chunk</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">long</span> <span class="title">getTotalValueCount</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@return</span> the next page in that chunk or null if after the last page</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">DataPage <span class="title">readPage</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The data for a column chunk</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">Chunk</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * Builder to concatenate the buffers of the discontinuous parts for the same column. These parts are generated as a</span></span><br><span class="line"><span class="comment">   * result of the column-index based filtering when some pages might be skipped at reading.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">ChunkListBuilder</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">ChunkData</span> </span>&#123;</span><br><span class="line">      <span class="keyword">final</span> List&lt;ByteBuffer&gt; buffers = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">      OffsetIndex offsetIndex;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;ChunkDescriptor, ChunkData&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> ChunkDescriptor lastDescriptor;</span><br><span class="line">    <span class="keyword">private</span> SeekableInputStream f;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * PageReader for a single column chunk. A column chunk contains</span></span><br><span class="line"><span class="comment">   * several pages, which are yielded one by one in order.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * This implementation is provided with a list of pages, each of which</span></span><br><span class="line"><span class="comment">   * is decompressed and passed through.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ColumnChunkPageReader</span> <span class="keyword">implements</span> <span class="title">PageReader</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> BytesInputDecompressor decompressor;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> valueCount;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;DataPage&gt; compressedPages;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> DictionaryPage compressedDictionaryPage;</span><br><span class="line">    <span class="comment">// null means no page synchronization is required; firstRowIndex will not be returned by the pages</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> OffsetIndex offsetIndex;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> rowCount;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> pageIndex = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    ColumnChunkPageReader(BytesInputDecompressor decompressor, List&lt;DataPage&gt; compressedPages,</span><br><span class="line">        DictionaryPage compressedDictionaryPage, OffsetIndex offsetIndex, <span class="keyword">long</span> rowCount) &#123;</span><br><span class="line">      <span class="keyword">this</span>.decompressor = decompressor;</span><br><span class="line">      <span class="keyword">this</span>.compressedPages = <span class="keyword">new</span> LinkedList&lt;DataPage&gt;(compressedPages);</span><br><span class="line">      <span class="keyword">this</span>.compressedDictionaryPage = compressedDictionaryPage;</span><br><span class="line">      <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (DataPage p : compressedPages) &#123;</span><br><span class="line">        count += p.getValueCount();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">this</span>.valueCount = count;</span><br><span class="line">      <span class="keyword">this</span>.offsetIndex = offsetIndex;</span><br><span class="line">      <span class="keyword">this</span>.rowCount = rowCount;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/10/24/Spark-executor-log/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/10/24/Spark-executor-log/" class="post-title-link" itemprop="url">寻找 Spark executor 日志</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-10-24 22:21:28" itemprop="dateCreated datePublished" datetime="2018-10-24T22:21:28+08:00">2018-10-24</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2019-06-23 20:26:38" itemprop="dateModified" datetime="2019-06-23T20:26:38+08:00">2019-06-23</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>spark on yarn 应用在运行时和完成后日志的存放位置是不同的，一般运行时是存放在各个运行节点，完成后会归集到 hdfs。无论哪种情况，都可以通过 spark 的页面跳转找到 executor 的日志，但是在大多数的生产环境中，对端口的开放是有严格的限制，也就是说根本无法正常跳转到日志页面进行查看的，这种情况下，就需要通过后台查询。</p>
<h2 id="运行时"><a href="#运行时" class="headerlink" title="运行时"></a>运行时</h2><p>spark on yarn 模式下一个 executor 对应 yarn 的一个 container，所以在 executor 的节点运行<code>ps -ef|grep spark.yarn.app.container.log.dir</code>，如果这个节点上可能运行多个 application，那么再通过 application id 进一步过滤。上面的命令会查到 executor 的进程信息，并且包含了日志路径，例如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> -Djava.io.tmpdir=/data1/hadoop/yarn/local/usercache/ocdp/appcache/application_1521424748238_0051/container_e07_1521424748238_0051_01_000002/tmp &#x27;</span><br><span class="line">-Dspark.history.ui.port=18080&#x27; &#x27;-Dspark.driver.port=59555&#x27; </span><br><span class="line">-Dspark.yarn.app.container.log.dir=/data1/hadoop/yarn/log/application_1521424748238_0051/container_e07_1521424748238_0051_01_000002 </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>也就是说这个 executor 的日志就在<code>/data1/hadoop/yarn/log/application_1521424748238_0051/container_e07_1521424748238_0051_01_000002</code>目录里。至此，我们就找到了运行时的 executor 日志。</p>
<h2 id="完成后"><a href="#完成后" class="headerlink" title="完成后"></a>完成后</h2><p>当这个 application 正常或者由于某种原因异常结束后，yarn 默认会将所有日志归集到 hdfs 上，所以 yarn 也提供了一个查询已结束 application 日志的方法，即<br> <code>yarn logs -applicationId application_1521424748238_0057</code>，结果里面会包含所有 executor 的日志，可能会比较多，建议将结果重定向到一个文件再详细查看。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>无论对于 spark 应用程序的开发者还是运维人员，日志对于排查问题是至关重要的，所以本文介绍了找到日志的方法。</p>
<h2 id="画外-log4j-配置-spark-on-yarn-client-mode"><a href="#画外-log4j-配置-spark-on-yarn-client-mode" class="headerlink" title="画外:log4j 配置 - spark on yarn client mode"></a>画外:log4j 配置 - spark on yarn client mode</h2><p>spark streaming 的程序如果运行方式是 yarn <strong><em>client</em></strong> mode，那么如何指定 driver 和 executor 的 log4j 配置文件？</p>
<h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>添加参数<code>--driver-java-options</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --driver-java-options &quot;-Dlog4j.configuration=file:/data1/conf/log4j-driver.properties&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>由于 executor 是运行在 yarn 的集群中的，所以先要将配置文件通过<code>--files</code>上传</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --files /data1/conf/log4j.properties --conf spark.executor.extraJavaOptions=&quot;-Dlog4j.configuration=log4j.properties&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在 log4j.properties 中要注意配置<code>spark.yarn.app.container.log.dir</code>例如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, file</span><br><span class="line">log4j.appender.file=org.apache.log4j.RollingFileAppender</span><br><span class="line">log4j.appender.file.append=true</span><br><span class="line">log4j.appender.file.file=$&#123;spark.yarn.app.container.log.dir&#125;/stdout</span><br><span class="line">log4j.appender.file.MaxFileSize=256MB</span><br><span class="line">log4j.appender.file.MaxBackupIndex=20</span><br><span class="line"></span><br><span class="line">log4j.appender.file.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.file.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %p [%t] %c&#123;1&#125;:%L - %m%n</span><br><span class="line"></span><br><span class="line"># Settings to quiet third party logs that are too verbose</span><br><span class="line">log4j.logger.org.spark-project.jetty=WARN</span><br><span class="line">log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ERROR</span><br><span class="line">log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO</span><br><span class="line">log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这样就可以在 spark 的 Web UI 中直接查看日志:executor tab 下的 Logs.</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>如果是通过<code>java -cp</code>命令运行自己的 jar 包，可以通过下面的方式添加 log4j 的配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -cp -Dlog4j.configuration=file:$&#123;APP_HOME&#125;/conf/log4j.properties</span><br></pre></td></tr></table></figure>

<p>作者：Woople, 链接：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/06a630618f19">https://www.jianshu.com/p/06a630618f19</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/10/24/Spark-Shuffle/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/10/24/Spark-Shuffle/" class="post-title-link" itemprop="url">Spark/MR Shuffle 对比</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-10-24 14:09:16" itemprop="dateCreated datePublished" datetime="2018-10-24T14:09:16+08:00">2018-10-24</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 00:31:30" itemprop="dateModified" datetime="2021-05-08T00:31:30+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Shuffle-过程介绍"><a href="#Shuffle-过程介绍" class="headerlink" title="Shuffle 过程介绍"></a><strong>Shuffle 过程介绍</strong></h1><h2 id="MapReduce-的-Shuffle-过程介绍"><a href="#MapReduce-的-Shuffle-过程介绍" class="headerlink" title="MapReduce 的 Shuffle 过程介绍"></a><strong>MapReduce 的 Shuffle 过程介绍</strong></h2><p>Shuffle 的本义是洗牌、混洗，把一组有一定规则的数据尽量转换成一组无规则的数据，越随机越好。MapReduce 中的 shuffle 更像是洗牌的逆过程，把一组无规则的数据尽量转换成一组具有一定规则的数据。</p>
<p>为什么 MapReduce 计算模型需要 shuffle 过程？我们都知道 MapReduce 计算模型一般包括两个重要的阶段：map 是映射，负责数据的过滤分发；reduce 是规约，负责数据的计算归并。Reduce 的数据来源于 map，map 的输出即是 reduce 的输入，reduce 需要通过 shuffle 来获取数据。</p>
<p>从 map 输出到 reduce 输入的整个过程可以广义地称为 shuffle。Shuffle 横跨 map 端和 reduce 端，在 map 端包括 spill 过程，在 reduce 端包括 copy 和 sort 过程，如图所示：</p>
<p><img src="/2018/10/24/Spark-Shuffle/image001.jpg" alt="image001"></p>
<h3 id="Map-Shuffle-Spill-过程"><a href="#Map-Shuffle-Spill-过程" class="headerlink" title="Map Shuffle (Spill 过程)"></a>Map Shuffle (Spill 过程)</h3><p>Spill 过程包括输出、排序、溢写、合并等步骤，如图所示：</p>
<p><img src="/2018/10/24/Spark-Shuffle/image002.png" alt="image002"></p>
<h4 id="Collect"><a href="#Collect" class="headerlink" title="Collect"></a><strong>Collect</strong></h4><p>每个 Map 任务不断地以 &lt;key, value&gt; 对的形式把数据输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内存空间，在内存中放置尽可能多的数据。</p>
<p>这个数据结构其实就是个字节数组，叫 kvbuffer，名如其义，但是这里面不光放置了 &lt;key, value&gt; 数据，还放置了一些索引数据，给放置索引数据的区域起了一个 kvmeta 的别名，在 kvbuffer 的一块区域上穿了一个 IntBuffer（字节序采用的是平台自身的字节序）的马甲。&lt;key, value&gt; 数据区域和索引数据区域在 kvbuffer 中是相邻不重叠的两个区域，用一个分界点来划分两者，分界点不是亘古不变的，而是每次 spill 之后都会更新一次。初始的分界点是 0，&lt;key, value&gt; 数据的存储方向是向上增长，索引数据的存储方向是向下增长，如图所示：</p>
<p><img src="/2018/10/24/Spark-Shuffle/image003.png" alt="image003"></p>
<p>Kvbuffer 的存放指针 bufindex 是一直闷着头地向上增长，比如 bufindex 初始值为 0，一个 Int 型的 key 写完之后，bufindex 增长为 4，一个 Int 型的 value 写完之后，bufindex 增长为 8。</p>
<p>索引是对 &lt;key, value&gt; 在 kvbuffer 中的索引，是个四元组，包括：value 的起始位置、key 的起始位置、partition 值、value 的长度，占用四个 Int 长度，kvmeta 的存放指针 kvindex 每次都是向下跳四个 “格子”，然后再向上一个格子一个格子地填充四元组的数据。比如 kvindex 初始位置是 - 4，当第一个 &lt;key, value&gt; 写完之后，(kvindex+0) 的位置存放 value 的起始位置、(kvindex+1)的位置存放 key 的起始位置、(kvindex+2)的位置存放 partition 的值、(kvindex+3)的位置存放 value 的长度，然后 kvindex 跳到 - 8 位置，等第二个 &lt;key, value&gt; 和索引写完之后，kvindex 跳到 - 32 位置。</p>
<p>Kvbuffer 的大小虽然可以通过参数设置，但是总共就那么大，&lt;key, value&gt; 和索引不断地增加，加着加着，kvbuffer 总有不够用的那天，那怎么办？把数据从内存刷到磁盘上再接着往内存写数据，把 kvbuffer 中的数据刷到磁盘上的过程就叫 spill，多么明了的叫法，内存中的数据满了就自动地 spill 到具有更大空间的磁盘。</p>
<p>关于 spill 触发的条件，也就是 kvbuffer 用到什么程度开始 spill，还是要讲究一下的。如果把 kvbuffer 用得死死得，一点缝都不剩的时候再开始 spill，那 map 任务就需要等 spill 完成腾出空间之后才能继续写数据；如果 kvbuffer 只是满到一定程度，比如 80% 的时候就开始 spill，那在 spill 的同时，map 任务还能继续写数据，如果 spill 够快，map 可能都不需要为空闲空间而发愁。两利相衡取其大，一般选择后者。</p>
<p>Spill 这个重要的过程是由 spill 线程承担，spill 线程从 map 任务接到 “命令” 之后就开始正式干活，干的活叫 sortAndSpill，原来不仅仅是 spill，在 spill 之前还有个颇具争议性的 sort。</p>
<h4 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a><strong>Sort</strong></h4><p>先把 kvbuffer 中的数据按照 partition 值和 key 两个关键字升序排序，移动的只是索引数据，排序结果是 kvmeta 中数据按照 partition 为单位聚集在一起，同一 partition 内的按照 key 有序。</p>
<h4 id="Spill"><a href="#Spill" class="headerlink" title="Spill"></a><strong>Spill</strong></h4><p>Spill 线程为这次 spill 过程创建一个磁盘文件：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于 “spill12.out” 的文件。Spill 线程根据排过序的 kvmeta 挨个 partition 的把 &lt;key, value&gt; 数据吐到这个文件中，一个 partition 对应的数据吐完之后顺序地吐下个 partition，直到把所有的 partition 遍历完。一个 partition 在文件中对应的数据也叫段(segment)。</p>
<p>所有的 partition 对应的数据都放在这个文件里，虽然是顺序存放的，但是怎么直接知道某个 partition 在这个文件中存放的起始位置呢？强大的索引又出场了。有一个三元组记录某个 partition 对应的数据在这个文件中的索引：起始位置、原始数据长度、压缩之后的数据长度，一个 partition 对应一个三元组。然后把这些索引信息存放在内存中，如果内存中放不下了，后续的索引信息就需要写到磁盘文件中了：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于 “spill12.out.index” 的文件，文件中不光存储了索引数据，还存储了 crc32 的校验数据。(spill12.out.index 不一定在磁盘上创建，如果内存（默认 1M 空间）中能放得下就放在内存中，即使在磁盘上创建了，和 spill12.out 文件也不一定在同一个目录下。)</p>
<p>每一次 spill 过程就会最少生成一个 out 文件，有时还会生成 index 文件，spill 的次数也烙印在文件名中。索引文件和数据文件的对应关系如下图所示：</p>
<p><img src="/2018/10/24/Spark-Shuffle/image004.png" alt="image004"></p>
<p>话分两端，在 spill 线程如火如荼的进行 sortAndSpill 工作的同时，map 任务不会因此而停歇，而是一无既往地进行着数据输出。Map 还是把数据写到 kvbuffer 中，那问题就来了：&lt;key, value&gt; 只顾着闷头按照 bufindex 指针向上增长，kvmeta 只顾着按照 kvindex 向下增长，是保持指针起始位置不变继续跑呢，还是另谋它路？如果保持指针起始位置不变，很快 bufindex 和 kvindex 就碰头了，碰头之后再重新开始或者移动内存都比较麻烦，不可取。Map 取 kvbuffer 中剩余空间的中间位置，用这个位置设置为新的分界点，bufindex 指针移动到这个分界点，kvindex 移动到这个分界点的 - 16 位置，然后两者就可以和谐地按照自己既定的轨迹放置数据了，当 spill 完成，空间腾出之后，不需要做任何改动继续前进。分界点的转换如下图所示：</p>
<p><img src="/2018/10/24/Spark-Shuffle/image005.png" alt="image005"></p>
<p>Map 任务总要把输出的数据写到磁盘上，即使输出数据量很小在内存中全部能装得下，在最后也会把数据刷到磁盘上。</p>
<h4 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a><strong>Merge</strong></h4><p>Map 任务如果输出数据量很大，可能会进行好几次 spill，out 文件和 index 文件会产生很多，分布在不同的磁盘上。最后把这些文件进行合并的 merge 过程闪亮登场。</p>
<p>Merge 过程怎么知道产生的 spill 文件都在哪了呢？从所有的本地目录上扫描得到产生的 spill 文件，然后把路径存储在一个数组里。Merge 过程又怎么知道 spill 的索引信息呢？没错，也是从所有的本地目录上扫描得到 index 文件，然后把索引信息存储在一个列表里。到这里，又遇到了一个值得纳闷的地方。在之前 spill 过程中的时候为什么不直接把这些信息存储在内存中呢，何必又多了这步扫描的操作？特别是 spill 的索引数据，之前当内存超限之后就把数据写到磁盘，现在又要从磁盘把这些数据读出来，还是需要装到更多的内存中。之所以多此一举，是因为这时 kvbuffer 这个内存大户已经不再使用可以回收，有内存空间来装这些数据了。（对于内存空间较大的土豪来说，用内存来省却这两个 io 步骤还是值得考虑的。）</p>
<p>然后为 merge 过程创建一个叫 file.out 的文件和一个叫 file.out.index 的文件用来存储最终的输出和索引。</p>
<p>一个 partition 一个 partition 的进行合并输出。对于某个 partition 来说，从索引列表中查询这个 partition 对应的所有索引信息，每个对应一个段插入到段列表中。也就是这个 partition 对应一个段列表，记录所有的 spill 文件中对应的这个 partition 那段数据的文件名、起始位置、长度等等。</p>
<p>然后对这个 partition 对应的所有的 segment 进行合并，目标是合并成一个 segment。当这个 partition 对应很多个 segment 时，会分批地进行合并：先从 segment 列表中把第一批取出来，以 key 为关键字放置成最小堆，然后从最小堆中每次取出最小的 &lt;key, value&gt; 输出到一个临时文件中，这样就把这一批段合并成一个临时的段，把它加回到 segment 列表中；再从 segment 列表中把第二批取出来合并输出到一个临时 segment，把其加入到列表中；这样往复执行，直到剩下的段是一批，输出到最终的文件中。</p>
<p>最终的索引数据仍然输出到 index 文件中。</p>
<p><img src="/2018/10/24/Spark-Shuffle/image006.png" alt="image006"></p>
<p>Map 端的 shuffle 过程到此结束。</p>
<h3 id="Reduce-shuffle"><a href="#Reduce-shuffle" class="headerlink" title="Reduce shuffle"></a>Reduce shuffle</h3><h4 id="Copy"><a href="#Copy" class="headerlink" title="Copy"></a><strong>Copy</strong></h4><p>Reduce 任务通过 http 向各个 map 任务拖取它所需要的数据。每个节点都会启动一个常驻的 http server，其中一项服务就是响应 reduce 拖取 map 数据。当有 mapOutput 的 http 请求过来的时候，http server 就读取相应的 map 输出文件中对应这个 reduce 部分的数据通过网络流输出给 reduce。</p>
<p>Reduce 任务拖取某个 map 对应的数据，如果在内存中能放得下这次数据的话就直接把数据写到内存中。Reduce 要向每个 map 去拖取数据，在内存中每个 map 对应一块数据，当内存中存储的 map 数据占用空间达到一定程度的时候，开始启动内存中 merge，把内存中的数据 merge 输出到磁盘上一个文件中。</p>
<p>如果在内存中不能放得下这个 map 的数据的话，直接把 map 数据写到磁盘上，在本地目录创建一个文件，从 http 流中读取数据然后写到磁盘，使用的缓存区大小是 64K。拖一个 map 数据过来就会创建一个文件，当文件数量达到一定阈值时，开始启动磁盘文件 merge，把这些文件合并输出到一个文件。</p>
<p>有些 map 的数据较小是可以放在内存中的，有些 map 的数据较大需要放在磁盘上，这样最后 reduce 任务拖过来的数据有些放在内存中了有些放在磁盘上，最后会对这些来一个全局合并。</p>
<h4 id="Merge-Sort"><a href="#Merge-Sort" class="headerlink" title="Merge Sort"></a><strong>Merge Sort</strong></h4><p>这里使用的 merge 和 map 端使用的 merge 过程一样。Map 的输出数据已经是有序的，merge 进行一次合并排序，所谓 reduce 端的 sort 过程就是这个合并的过程。一般 reduce 是一边 copy 一边 sort，即 copy 和 sort 两个阶段是重叠而不是完全分开的。</p>
<p>Reduce 端的 shuffle 过程至此结束。</p>
<h2 id="Spark-的-Shuffle-过程介绍"><a href="#Spark-的-Shuffle-过程介绍" class="headerlink" title="Spark 的 Shuffle 过程介绍"></a><strong>Spark 的 Shuffle 过程介绍</strong></h2><h3 id="Shuffle-Writer"><a href="#Shuffle-Writer" class="headerlink" title="Shuffle Writer"></a><strong>Shuffle Writer</strong></h3><p>Spark 丰富了任务类型，有些任务之间数据流转不需要通过 shuffle，但是有些任务之间还是需要通过 shuffle 来传递数据，比如 wide dependency 的 group by key。</p>
<p>Spark 中需要 shuffle 输出的 map 任务会为每个 reduce 创建对应的 bucket，map 产生的结果会根据设置的 partitioner 得到对应的 bucketId，然后填充到相应的 bucket 中去。每个 map 的输出结果可能包含所有的 reduce 所需要的数据，所以每个 map 会创建 R 个 bucket（R 是 reduce 的个数），M 个 map 总共会创建 M*R 个 bucket。</p>
<p>Map 创建的 bucket 其实对应磁盘上的一个文件，map 的结果写到每个 bucket 中其实就是写到那个磁盘文件中，这个文件也被称为 blockFile，是 DiskBlockManager 管理器通过文件名的 hash 值对应到本地目录的子目录中创建的。每个 map 要在节点上创建 R 个磁盘文件用于结果输出，map 的结果是直接输出到磁盘文件上的，100KB 的内存缓冲是用来创建 FastBufferedOutputStream 输出流。这种方式一个问题就是 shuffle 文件过多。</p>
<p><img src="/2018/10/24/Spark-Shuffle/image007.png" alt="image007"></p>
<p>针对上述 shuffle 过程产生的文件过多问题，Spark 有另外一种改进的 shuffle 过程：consolidation shuffle，以期显著减少 shuffle 文件的数量。在 consolidation shuffle 中每个 bucket 并非对应一个文件，而是对应文件中的一个 segment 部分。Job 的 map 在某个节点上第一次执行，为每个 reduce 创建 bucket 对应的输出文件，把这些文件组织成 ShuffleFileGroup，当这次 map 执行完之后，这个 ShuffleFileGroup 可以释放为下次循环利用；当又有 map 在这个节点上执行时，不需要创建新的 bucket 文件，而是在上次的 ShuffleFileGroup 中取得已经创建的文件继续追加写一个 segment；当前次 map 还没执行完，ShuffleFileGroup 还没有释放，这时如果有新的 map 在这个节点上执行，无法循环利用这个 ShuffleFileGroup，而是只能创建新的 bucket 文件组成新的 ShuffleFileGroup 来写输出。</p>
<p><img src="/2018/10/24/Spark-Shuffle/image008.png" alt="image008"></p>
<p>比如一个 job 有 3 个 map 和 2 个 reduce：(1) 如果此时集群有 3 个节点有空槽，每个节点空闲了一个 core，则 3 个 map 会调度到这 3 个节点上执行，每个 map 都会创建 2 个 shuffle 文件，总共创建 6 个 shuffle 文件；(2) 如果此时集群有 2 个节点有空槽，每个节点空闲了一个 core，则 2 个 map 先调度到这 2 个节点上执行，每个 map 都会创建 2 个 shuffle 文件，然后其中一个节点执行完 map 之后又调度执行另一个 map，则这个 map 不会创建新的 shuffle 文件，而是把结果输出追加到之前 map 创建的 shuffle 文件中；总共创建 4 个 shuffle 文件；(3) 如果此时集群有 2 个节点有空槽，一个节点有 2 个空 core 一个节点有 1 个空 core，则一个节点调度 2 个 map 一个节点调度 1 个 map，调度 2 个 map 的节点上，一个 map 创建了 shuffle 文件，后面的 map 还是会创建新的 shuffle 文件，因为上一个 map 还正在写，它创建的 ShuffleFileGroup 还没有释放；总共创建 6 个 shuffle 文件。</p>
<h3 id="Shuffle-Fetcher"><a href="#Shuffle-Fetcher" class="headerlink" title="Shuffle Fetcher"></a><strong>Shuffle Fetcher</strong></h3><p>Reduce 去拖 map 的输出数据，Spark 提供了两套不同的拉取数据框架：通过 socket 连接去取数据；使用 netty 框架去取数据。</p>
<p>每个节点的 Executor 会创建一个 BlockManager，其中会创建一个 BlockManagerWorker 用于响应请求。当 reduce 的 GET_BLOCK 的请求过来时，读取本地文件将这个 blockId 的数据返回给 reduce。如果使用的是 Netty 框架，BlockManager 会创建 ShuffleSender 用于发送 shuffle 数据。</p>
<p>并不是所有的数据都是通过网络读取，对于在本节点的 map 数据，reduce 直接去磁盘上读取而不再通过网络框架。</p>
<p>Reduce 拖过来数据之后以什么方式存储呢？Spark map 输出的数据没有经过排序，spark shuffle 过来的数据也不会进行排序，spark 认为 shuffle 过程中的排序不是必须的，并不是所有类型的 reduce 需要的数据都需要排序，强制地进行排序只会增加 shuffle 的负担。Reduce 拖过来的数据会放在一个 HashMap 中，HashMap 中存储的也是 &lt;key, value&gt; 对，key 是 map 输出的 key，map 输出对应这个 key 的所有 value 组成 HashMap 的 value。Spark 将 shuffle 取过来的每一个 &lt;key, value&gt; 对插入或者更新到 HashMap 中，来一个处理一个。HashMap 全部放在内存中。</p>
<p>Shuffle 取过来的数据全部存放在内存中，对于数据量比较小或者已经在 map 端做过合并处理的 shuffle 数据，占用内存空间不会太大，但是对于比如 group by key 这样的操作，reduce 需要得到 key 对应的所有 value，并将这些 value 组一个数组放在内存中，这样当数据量较大时，就需要较多内存。</p>
<p>当内存不够时，要不就失败，要不就用老办法把内存中的数据移到磁盘上放着。Spark 意识到在处理数据规模远远大于内存空间时所带来的不足，引入了一个具有外部排序的方案。Shuffle 过来的数据先放在内存中，当内存中存储的 &lt;key, value&gt; 对超过 1000 并且内存使用超过 70% 时，判断节点上可用内存如果还足够，则把内存缓冲区大小翻倍，如果可用内存不再够了，则把内存中的 &lt;key, value&gt; 对排序然后写到磁盘文件中。最后把内存缓冲区中的数据排序之后和那些磁盘文件组成一个最小堆，每次从最小堆中读取最小的数据，这个和 MapReduce 中的 merge 过程类似。</p>
<h2 id="MapReduce-和-Spark-的-Shuffle-过程对比"><a href="#MapReduce-和-Spark-的-Shuffle-过程对比" class="headerlink" title="MapReduce 和 Spark 的 Shuffle 过程对比"></a><strong>MapReduce 和 Spark 的 Shuffle 过程对比</strong></h2><table>
<thead>
<tr>
<th></th>
<th>MapReduce</th>
<th>Spark</th>
</tr>
</thead>
<tbody><tr>
<td>collect</td>
<td>在内存中构造了一块数据结构用于 map 输出的缓冲</td>
<td>没有在内存中构造一块数据结构用于 map 输出的缓冲，而是直接把输出写到磁盘文件</td>
</tr>
<tr>
<td>sort</td>
<td>map 输出的数据有排序</td>
<td>map 输出的数据没有排序</td>
</tr>
<tr>
<td>merge</td>
<td>对磁盘上的多个 spill 文件最后进行合并成一个输出文件</td>
<td>在 map 端没有 merge 过程，在输出时直接是对应一个 reduce 的数据写到一个文件中，这些文件同时存在并发写，最后不需要合并成一个</td>
</tr>
<tr>
<td>copy 框架</td>
<td>jetty</td>
<td>netty 或者直接 socket 流</td>
</tr>
<tr>
<td>对于本节点上的文件</td>
<td>仍然是通过网络框架拖取数据</td>
<td>不通过网络框架，对于在本节点上的 map 输出文件，采用本地读取的方式</td>
</tr>
<tr>
<td>copy 过来的数据存放位置</td>
<td>先放在内存，内存放不下时写到磁盘</td>
<td>一种方式全部放在内存；另一种方式先放在内存</td>
</tr>
<tr>
<td>merge sort</td>
<td>最后会对磁盘文件和内存中的数据进行合并排序</td>
<td>对于采用另一种方式时也会有合并排序的过程</td>
</tr>
</tbody></table>
<h2 id="Shuffle-后续优化方向"><a href="#Shuffle-后续优化方向" class="headerlink" title="Shuffle 后续优化方向"></a><strong>Shuffle 后续优化方向</strong></h2><p>通过上面的介绍，我们了解到，shuffle 过程的主要存储介质是磁盘，尽量的减少 io 是 shuffle 的主要优化方向。我们脑海中都有那个经典的存储金字塔体系，shuffle 过程为什么把结果都放在磁盘上，那是因为现在内存再大也大不过磁盘，内存就那么大，还这么多张嘴吃，当然是分配给最需要的了。如果具有 “土豪” 内存节点，减少 shuffle io 的最有效方式无疑是尽量把数据放在内存中。下面列举一些现在看可以优化的方面，期待经过我们不断的努力，TDW 计算引擎运行地更好。</p>
<h3 id="MapReduce-Shuffle-后续优化方向"><a href="#MapReduce-Shuffle-后续优化方向" class="headerlink" title="MapReduce Shuffle 后续优化方向"></a><strong>MapReduce Shuffle 后续优化方向</strong></h3><ul>
<li><p>  压缩：对数据进行压缩，减少写读数据量；</p>
</li>
<li><p>  减少不必要的排序：并不是所有类型的 reduce 需要的数据都是需要排序的，排序这个 nb 的过程如果不需要最好还是不要的好；</p>
</li>
<li><p>  内存化：shuffle 的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招；</p>
</li>
<li><p>  网络框架：netty 的性能据说要占优了；</p>
</li>
<li><p>  本节点上的数据不走网络框架：对于本节点上的 map 输出，reduce 直接去读吧，不需要绕道网络框架。</p>
</li>
</ul>
<h3 id="Spark-Shuffle-后续优化方向"><a href="#Spark-Shuffle-后续优化方向" class="headerlink" title="Spark Shuffle 后续优化方向"></a><strong>Spark Shuffle 后续优化方向</strong></h3><p>Spark 作为 MapReduce 的进阶架构，对于 shuffle 过程已经是优化了的，特别是对于那些具有争议的步骤已经做了优化，但是 Spark 的 shuffle 对于我们来说在一些方面还是需要优化的。</p>
<ul>
<li><p>  压缩：对数据进行压缩，减少写读数据量；</p>
</li>
<li><p>  内存化：Spark 历史版本中是有这样设计的：map 写数据先把数据全部写到内存中，写完之后再把数据刷到磁盘上；考虑内存是紧缺资源，后来修改成把数据直接写到磁盘了；对于具有较大内存的集群来讲，还是尽量地往内存上写吧，内存放不下了再放磁盘。</p>
</li>
</ul>
<p>摘自: <a target="_blank" rel="noopener" href="http://data.qq.com/article?id=543">腾讯大数据博客</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/10/16/Spark-map-vs-mapPartitions/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/10/16/Spark-map-vs-mapPartitions/" class="post-title-link" itemprop="url">Spark map vs mapPartitions</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-10-16 10:43:54" itemprop="dateCreated datePublished" datetime="2018-10-16T10:43:54+08:00">2018-10-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2018-10-25 23:00:00" itemprop="dateModified" datetime="2018-10-25T23:00:00+08:00">2018-10-25</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>官方定义:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new RDD by applying a function to all elements of this RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new RDD by applying a function to each partition of this RDD.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * `preservesPartitioning` indicates whether the input function preserves the partitioner, which</span></span><br><span class="line"><span class="comment"> * should be `false` unless this is a pair RDD and the input function doesn&#x27;t modify the keys.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(</span><br><span class="line">    <span class="keyword">this</span>,</span><br><span class="line">    (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(iter),</span><br><span class="line">    preservesPartitioning)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到的是 mapPartitions 需要的函数参数传入的是一个 iter, 返回的也是一个 iter, 而 map 仅仅是一个元素.</p>
<p>假设我们有 10k 个元素, 10个 partitions, 数据均匀分布:</p>
<ol>
<li>map:调用10k 次 map 方法</li>
<li>mapPartitions:调用10次 mapPartitions 方法, 每次传入1k个(一个 partition 的数据量)进行计算. 结果先存到 memory 中, 直到可以返回.</li>
<li>flatMap在 单个元素(map)上工作，生成结果是多个元素(mapPartitions)</li>
</ol>
<p>结论:mapPartitions 转换比 map 快，因为它调用你的函数 一次/分区，而不是 一次/元素, 像有一些高开销的 init 的时候, 如数据库连接, 使用 mapPartitions, 每个分区就只需要一次 init.</p>
<p>来看一个使用 mapPartitions 报错的例子, 运行这段代码, 会报 already closed exception, 因为 spark 的计算都是 lazy 的, 下面的 partition.map 到真的触发计算的时候, conn 已经 close 了.解决方法就是 <code>val newPartition = partition.map(...&#125;).toList</code> 触发计算.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> newDF = myDF.mapPartitions(</span><br><span class="line">  partition =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> conn = <span class="keyword">new</span> <span class="type">DbConnection</span></span><br><span class="line">    <span class="keyword">val</span> newPartition = partition.map(record =&gt; &#123; readMatchingFromDB(record, connection) &#125;)</span><br><span class="line">    conn.close()</span><br><span class="line">    newPartition</span><br><span class="line">  &#125;).toDF()</span><br></pre></td></tr></table></figure>

<p>map mapPartitions 对外部引用的更新都没法作用出来: 下面的 flag 还是0.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> flag = <span class="number">0</span></span><br><span class="line"><span class="keyword">val</span> test = rdd.map &#123;</span><br><span class="line">  row =&gt; flag += <span class="number">1</span></span><br><span class="line">&#125;.collect()</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/10/06/Druid-Storage-%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/10/06/Druid-Storage-%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">Druid Storage 原理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-10-06 20:49:18" itemprop="dateCreated datePublished" datetime="2018-10-06T20:49:18+08:00">2018-10-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:11:20" itemprop="dateModified" datetime="2021-05-08T11:11:20+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>转载自<a target="_blank" rel="noopener" href="https://blog.bcmeng.com/post/druid-storage.html">编程小梦</a></p>
<h3 id="What-is-Druid"><a href="#What-is-Druid" class="headerlink" title="What is Druid"></a>What is Druid</h3><p>Druid 是一个开源的实时 OLAP 系统，可以对超大规模数据提供亚秒级查询，其具有以下特点：</p>
<ol>
<li> 列式存储</li>
<li> 倒排索引 （基于 Bitmap 实现）</li>
<li> 分布式的 Shared-Nothing 架构 （高可用，易扩展是 Druid 的设计目标）</li>
<li> 实时摄入 （数据被 Druid 实时摄入后便可以立即查询）</li>
</ol>
<h3 id="Why-Druid"><a href="#Why-Druid" class="headerlink" title="Why Druid"></a>Why Druid</h3><p>为了能够提取利用大数据的商业价值，我们必然需要对数据进行分析，尤其是多维分析， 但是在几年前，整个业界并没有一款很好的 OLAP 工具，各种多维分析的方式如下图所示：</p>
<p><img src="/2018/10/06/Druid-Storage-%E5%8E%9F%E7%90%86/55283969.jpg"></p>
<p>其中直接基于 Hive，MR，Spark 的方式查询速度一般十分慢，并发低；而传统的关系型数据库无法支撑大规模数据；以 HBase 为代表的 NoSQL 数据库也无法提供高效的过滤，聚合能力。正因为现有工具有着各种各样的痛点，Druid 应运而生，以下几点自然是其设计目标：</p>
<ol>
<li> 快速查询</li>
<li> 可以支撑大规模数据集</li>
<li> 高效的过滤和聚合</li>
<li> 实时摄入</li>
</ol>
<h3 id="Druid-架构"><a href="#Druid-架构" class="headerlink" title="Druid 架构"></a>Druid 架构</h3><p><img src="/2018/10/06/Druid-Storage-%E5%8E%9F%E7%90%86/98456054.jpg" alt="image.png-181kB"></p>
<p>Druid 的整体架构如上图所示，其中主要有 3 条路线：</p>
<ol>
<li><p> 实时摄入的过程： 实时数据会首先按行摄入 Real-time Nodes，Real-time Nodes 会先将每行的数据加入到 1 个 map 中，等达到一定的行数或者大小限制时，Real-time Nodes 就会将内存中的 map 持久化到磁盘中，Real-time Nodes 会按照 segmentGranularity 将一定时间段内的小文件 merge 为一个大文件，生成 Segment，然后将 Segment 上传到 Deep Storage（HDFS，S3）中，Coordinator 知道有 Segment 生成后，会通知相应的 Historical Node 下载对应的 Segment，并负责该 Segment 的查询。</p>
</li>
<li><p> 离线摄入的过程： 离线摄入的过程比较简单，就是直接通过 MR job 生成 Segment，剩下的逻辑和实时摄入相同：</p>
</li>
<li><p> 用户查询过程： 用户的查询都是直接发送到 Broker Node，Broker Node 会将查询分发到 Real-time 节点和 Historical 节点，然后将结果合并后返回给用户。</p>
</li>
</ol>
<p>各节点的主要职责如下：</p>
<h4 id="Historical-Nodes"><a href="#Historical-Nodes" class="headerlink" title="Historical Nodes"></a>Historical Nodes</h4><p>Historical 节点是整个 Druid 集群的骨干，主要负责加载不可变的 segment，并负责 Segment 的查询（注意，Segment 必须加载到 Historical 的内存中才可以提供查询）。Historical 节点是无状态的，所以可以轻易的横向扩展和快速恢复。Historical 节点 load 和 un-load segment 是依赖 ZK 的，但是即使 ZK 挂掉，Historical 依然可以对已经加载的 Segment 提供查询，只是不能再 load 新 segment，drop 旧 segment。</p>
<h4 id="Broker-Nodes"><a href="#Broker-Nodes" class="headerlink" title="Broker Nodes"></a>Broker Nodes</h4><p>Broker 节点是 Druid 查询的入口，主要负责查询的分发和 Merge。 之外，Broker 还会对不可变的 Segment 的查询结果进行 LRU 缓存。</p>
<h4 id="Coordinator-Nodes"><a href="#Coordinator-Nodes" class="headerlink" title="Coordinator Nodes"></a>Coordinator Nodes</h4><p>Coordinator 节点主要负责 Segment 的管理。Coordinator 节点会通知 Historical 节点加载新 Segment，删除旧 Segment，复制 Segment，以及 Segment 间的复杂均衡。</p>
<p>Coordinator 节点依赖 ZK 确定 Historical 的存活和集群 Segment 的分布。</p>
<h4 id="Real-time-Node"><a href="#Real-time-Node" class="headerlink" title="Real-time Node"></a>Real-time Node</h4><p>实时节点主要负责数据的实时摄入，实时数据的查询，将实时数据转为 Segment，将 Segment Hand off 给 Historical 节点。</p>
<h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h4><p>Druid 依赖 ZK 实现服务发现，数据拓扑的感知，以及 Coordinator 的选主。</p>
<h4 id="Metadata-Storage"><a href="#Metadata-Storage" class="headerlink" title="Metadata Storage"></a>Metadata Storage</h4><p>Metadata storage（Mysql） 主要用来存储 Segment 和配置的元数据。当有新 Segment 生成时，就会将 Segment 的元信息写入 metadata store, Coordinator 节点会监控 Metadata store 从而知道何时 load 新 Segment，何时 drop 旧 Segment。注意，查询时不会涉及 Metadata store。</p>
<h4 id="Deep-Storage"><a href="#Deep-Storage" class="headerlink" title="Deep Storage"></a>Deep Storage</h4><p>Deep storage (S3 and HDFS) 是作为 Segment 的永久备份，查询时同样不会涉及 Deep storage。</p>
<h3 id="Column"><a href="#Column" class="headerlink" title="Column"></a>Column</h3><p><img src="/2018/10/06/Druid-Storage-%E5%8E%9F%E7%90%86/34009295.jpg"></p>
<p>Druid 中的列主要分为 3 类：时间列，维度列，指标列。Druid 在数据摄入和查询时都依赖时间列，这也是合理的，因为多维分析一般都带有时间维度。维度和指标是 OLAP 系统中常见的概念，维度主要是事件的属性，在查询时一般用来 filtering 和 group by，指标是用来聚合和计算的，一般是数值类型，像 count,sum，min，max 等。</p>
<p>Druid 中的维度列支持 String，Long，Float，不过只有 String 类型支持倒排索引；指标列支持 Long，Float，Complex， 其中 Complex 指标包含 HyperUnique，Cardinality，Histogram，Sketch 等复杂指标。强类型的好处是可以更好的对每 1 列进行编码和压缩， 也可以保证数据索引的高效性和查询性能。</p>
<h3 id="Segment"><a href="#Segment" class="headerlink" title="Segment"></a>Segment</h3><p>前面提到过，Druid 中会按时间段生成不可变的带倒排索引的列式文件，这个文件就称之为 Segment，Segment 是 Druid 中数据存储、复制、均衡、以及计算的基本单元， Segment 由 dataSource_beginTime_endTime_version_shardNumber 唯一标识，1 个 segment 一般包含 5–10 million 行记录，大小一般在 300~700mb。</p>
<h3 id="Segment-的存储格式"><a href="#Segment-的存储格式" class="headerlink" title="Segment 的存储格式"></a>Segment 的存储格式</h3><img src="/2018/10/06/Druid-Storage-%E5%8E%9F%E7%90%86/29577015.jpg" alt="image.png-90kB" style="zoom:50%;">

<p>Druid segment 的存储格式如上图所示，包含 3 部分：</p>
<ul>
<li>  version 文件</li>
<li>  meta 文件</li>
<li>  数据文件</li>
</ul>
<p>其中 meta 文件主要包含每 1 列的文件名和文件的偏移量。（注，druid 为了减少文件描述符，将 1 个 segment 的所有列都合并到 1 个大的 smoosh 中，由于 druid 访问 segment 文件的时候采用 MMap 的方式，所以单个 smoosh 文件的大小不能超过 2G，如果超过 2G，就会写到下一个 smoosh 文件）。</p>
<p>在 smoosh 文件中，数据是按列存储中，包含时间列，维度列和指标列，其中每 1 列会包含 2 部分：ColumnDescriptor 和 binary 数据。其中 ColumnDescriptor 主要保存每 1 列的数据类型和 Serde 的方式。</p>
<p>smoosh 文件中还有 index.drd 文件和 metadata.drd 文件，其中 index.drd 主要包含该 segment 有哪些列，哪些维度，该 Segment 的时间范围以及使用哪种 bitmap；metadata.drd 主要包含是否需要聚合，指标的聚合函数，查询粒度，时间戳字段的配置等。</p>
<h3 id="指标列的存储格式"><a href="#指标列的存储格式" class="headerlink" title="指标列的存储格式"></a>指标列的存储格式</h3><p>我们先来看指标列的存储格式：</p>
<img src="/2018/10/06/Druid-Storage-%E5%8E%9F%E7%90%86/8107893.jpg" alt="image.png-35.9kB" style="zoom:50%;">

<p>指标列的存储格式如上图所示：</p>
<ul>
<li>  version</li>
<li>  value 个数</li>
<li>  每个 block 的 value 的个数（druid 对 Long 和 Float 类型会按 block 进行压缩，block 的大小是 64K）</li>
<li>  压缩类型 （druid 目前主要有 LZ4 和 LZF 俩种压缩算法）</li>
<li>  编码类型 （druid 对 Long 类型支持差分编码和 Table 编码两种方式，Table 编码就是将 long 值映射到 int，当指标列的基数小于 256 时，druid 会选择 Table 编码，否则会选择差分编码）</li>
<li>  编码的 header （以差分编码为例，header 中会记录版本号，base value，每个 value 用几个 bit 表示）</li>
<li>  每个 block 的 header （主要记录版本号，是否允许反向查找，value 的数量，列名长度和列名）</li>
<li>  每 1 列具体的值</li>
</ul>
<h4 id="Long-型指标"><a href="#Long-型指标" class="headerlink" title="Long 型指标"></a>Long 型指标</h4><p>Druid 中对 Long 型指标会先进行编码，然后按 block 进行压缩。编码算法包含差分编码和 table 编码，压缩算法包含 LZ4 和 LZF。</p>
<h4 id="Float-型指标"><a href="#Float-型指标" class="headerlink" title="Float 型指标"></a>Float 型指标</h4><p>Druid 对于 Float 类型的指标不会进行编码，只会按 block 进行压缩。</p>
<h4 id="Complex-型指标"><a href="#Complex-型指标" class="headerlink" title="Complex 型指标"></a>Complex 型指标</h4><p>Druid 对于 HyperUnique，Cardinality，Histogram，Sketch 等复杂指标不会进行编码和压缩处理，每种复杂指标的 Serde 方式由每种指标自己的 ComplexMetricSerde 实现类实现。</p>
<h3 id="String-维度的存储格式"><a href="#String-维度的存储格式" class="headerlink" title="String 维度的存储格式"></a>String 维度的存储格式</h3><p><img src="/2018/10/06/Druid-Storage-%E5%8E%9F%E7%90%86/79137485.jpg" alt="image.png-81.2kB"></p>
<p>String 维度的存储格式如上图所示，前面提到过，时间列，维度列，指标列由两部分组成：ColumnDescriptor 和 binary 数据。 String 维度的 binary 数据主要由 3 部分组成：dict，字典编码后的 id 数组，用于倒排索引的 bitmap。</p>
<p>以上图中的 D2 维度列为例，总共有 4 行，前 3 行的值是 meituan，第 4 行的值是 dianing。Druid 中 dict 的实现十分简单，就是一个 hashmap。图中 dict 的内容就是将 meituan 编码为 0，dianping 编码为 1。 Id 数组的内容就是用编码后的 ID 替换掉原始值，所以就是 [1,1,1,0]。第 3 部分的倒排索引就是用 bitmap 表示某个值是否出现在某行中，如果出现了，bitmap 对应的位置就会置为 1，如图：meituan 在前 3 行中都有出现，所以倒排索引 1：[1,1,1,0] 就表示 meituan 在前 3 行中出现。</p>
<p>显然，倒排索引的大小是列的基数 * 总的行数，如果没有处理的话结果必然会很大。不过好在如果维度列如果基数很高的话，bitmap 就会比较稀疏，而稀疏的 bitmap 可以进行高效的压缩。</p>
<h3 id="Segment-生成过程"><a href="#Segment-生成过程" class="headerlink" title="Segment 生成过程"></a>Segment 生成过程</h3><ol>
<li> Add Row to Map</li>
<li> Begin persist to disk</li>
<li> Write version file</li>
<li> Merge and write dimension dict</li>
<li> Write time column</li>
<li> Write metric column</li>
<li> Write dimension column</li>
<li> Write index.drd</li>
<li> Merge and write bitmaps</li>
<li> Write metadata.drd</li>
</ol>
<h3 id="Segment-load-过程"><a href="#Segment-load-过程" class="headerlink" title="Segment load 过程"></a>Segment load 过程</h3><img src="/2018/10/06/Druid-Storage-%E5%8E%9F%E7%90%86/34569900.jpg" alt="meta.png-44.3kB" style="zoom:50%;">

<ol>
<li> Read version</li>
<li> Load segment to MappedByteBuffer</li>
<li> Get column offset from meta</li>
<li> Deserialize each column from ByteBuffer</li>
</ol>
<h3 id="Segment-Query-过程"><a href="#Segment-Query-过程" class="headerlink" title="Segment Query 过程"></a>Segment Query 过程</h3><p>Druid 查询的最小单位是 Segment，Segment 在查询之前必须先 load 到内存，load 过程如上一步所述。如果没有索引的话，我们的查询过程就只能 Scan 的，遇到符合条件的行选择出来，但是所有查询都进行全表 Scan 肯定是不可行的，所以我们需要索引来快速过滤不需要的行。Druid 的 Segmenet 查询过程如下：</p>
<ol>
<li> 构造 1 个 Cursor 进行迭代</li>
<li> 查询之前构造出 Fliter</li>
<li> 根据 Index 匹配 Fliter，得到满足条件的 Row 的 Offset</li>
<li> 根据每列的 ColumnSelector 去指定 Row 读取需要的列。</li>
</ol>
<h3 id="Druid-的编码和压缩"><a href="#Druid-的编码和压缩" class="headerlink" title="Druid 的编码和压缩"></a>Druid 的编码和压缩</h3><p>前面已经提到了，Druid 对 Long 型的指标进行了差分编码和 Table 编码，Long 型和 Float 型的指标进行了 LZ4 或者 LZF 压缩。</p>
<p>其实编码和压缩本质上是一个东西，一切熵增的编码都是压缩。 在计算机领域，我们一般把针对特定类型的编码称之为编码，针对任意类型的通用编码称之为压缩。</p>
<p>编码和压缩的本质就是让每一个 bit 尽可能带有更多的信息。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/08/26/Designing-Data-Intensive-Applications-Storage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/08/26/Designing-Data-Intensive-Applications-Storage/" class="post-title-link" itemprop="url">Designing Data-Intensive Applications(Storage)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-08-26 19:30:40" itemprop="dateCreated datePublished" datetime="2018-08-26T19:30:40+08:00">2018-08-26</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:09:45" itemprop="dateModified" datetime="2021-05-08T11:09:45+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Index-基础"><a href="#Index-基础" class="headerlink" title="Index 基础"></a>Index 基础</h1><p>log:泛指 append only 的记录序列.不一定要人可读, 可以是 binary 的.</p>
<p>log 文件是 append only, 而且一旦生成就不可变, 所以对并发和崩溃恢复很友好. 如果用 update 方式来修改值的话, 崩溃时会有很多不可预期的行为.</p>
<p>核心, 就是要减少扫的数据量. full scan (O(n)) 肯定最慢, 扫的时间和数据量成正比.</p>
<p>index 的也就是存出一些额外的信息, 这样的话帮助你定位到你的数据. 管理额外的信息是有开销的:存储(写入) 管理(更新), 所以 db 不会索引所有值.</p>
<h2 id="Hash-Index"><a href="#Hash-Index" class="headerlink" title="Hash Index"></a>Hash Index</h2><p>要找 key 为123456的 content, 只需要先在内存 map 中找到它的 byte offset , 只要一次 seek 过去,读到和下一个 key 的 offset相减的 content. 读的很精准.</p>
<p><img src="/2018/08/26/Designing-Data-Intensive-Applications-Storage/893370.jpg"></p>
<blockquote>
<p>Storing a log of key-value pairs in a CSV-like format, indexed with an in-memory hash map.</p>
</blockquote>
<p>通过追加的方式, 可能会造成一个文件太大了, 解决方案是把当 log 到一定 size, 就拆分一个新 segment 文件. 后面我们可以对这些 segments 进行 compaction. compact 会去除 dup 的 key, 留下最新的版本.</p>
<p>还可以对多个 seg 进行 merge.<br><img src="/2018/08/26/Designing-Data-Intensive-Applications-Storage/39145451.jpg"></p>
<blockquote>
<p>对一个 seg 进行 compact</p>
</blockquote>
<p><img src="/2018/08/26/Designing-Data-Intensive-Applications-Storage/13193271.jpg"></p>
<blockquote>
<p>对 seg 进行 compact, 同时进行 merge.</p>
</blockquote>
<h3 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h3><ol>
<li>散列表必须能放进内存. 原则上可以在磁盘上保留一个哈希映射, 但是磁盘哈希映射很难表现优秀, 它需要大量的随机访问I/O, 当它变满时增长也是很昂贵的,并且散列冲突需要很多的逻辑</li>
<li>范围查询效率不高, 必须在散列映射中单独查找每个键.</li>
</ol>
<h2 id="SSLTables-and-LSM-Trees"><a href="#SSLTables-and-LSM-Trees" class="headerlink" title="SSLTables and LSM-Trees"></a>SSLTables and LSM-Trees</h2><p>之前的 log 文件里 record 的顺序是他们写入的顺序.我们现在有个新的要求, 就是写入的 k-v 对要根据 key 排序, 且每个 key 在每个 seg 中只出现一次, 这种 format 叫做 Sorted String Table(SSTable)</p>
<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ol>
<li><p>可以 merge 比内存还大的多的 seg(使用 merge sort), 对于多个 seg 中都出现的值, 只需要留最新的 seg 中的值就可以了.<br><img src="/2018/08/26/Designing-Data-Intensive-Applications-Storage/65990847.jpg"></p>
</li>
<li><p>index 不用保留所有的 key, 因为你所有的 key 都是保序的, 所以只要有几个作为base, 其他的可以在这几个 base 之间去找. 假设你正在内存中寻找键 handiwork，但是你不知道段文件中该关键字的确切偏移量. 然而，你知道 handbag 和 handsome 的偏移，而且由于排序特性，你知道 handiwork 必须出现在这两者之间. 这意味着您可以跳到 handbag 的偏移位置并从那里扫描，直到您找到 handiwork(或没找到，如果该文件中没有该键)</p>
</li>
</ol>
<p>这样, 你内存中的索引就可以很稀疏, 每几千字节的段文件就有一个键就足够了，因为几千字节可以很快被扫描.</p>
<p>如果所有的 k-v 都是 fix length 的, 你可以用 binary search. 这样可以省去整个内存中的索引.不过 in practice 一般都是变长的.</p>
<p><img src="/2018/08/26/Designing-Data-Intensive-Applications-Storage/28391204.jpg"></p>
<h3 id="维护和构建-SSTables"><a href="#维护和构建-SSTables" class="headerlink" title="维护和构建 SSTables"></a>维护和构建 SSTables</h3><p>在磁盘上维护一个有序的数据结构是可能的(见 B Tree), 但是在内存中维护会更加简单.类似的结构有 red-black trees, AVL tress.</p>
<h4 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h4><ol>
<li>写入时将其添加到内存中的平衡树数据结构(例如, 红黑树). 这个内存树有时被称为内存表(mem table).</li>
<li>当内存表大于某个阈值(通常为几兆字节)时, 将其作为 SSTable 文件写入磁盘.新的 SSTable 文件成为数据库的最新部分.当 SSTable 被写入磁盘时, 写入可以继续到一个新的内存表实例.</li>
<li>当读的时候, 首先尝试在内存表中找到关键字, 然后在最近的磁盘段中, 然后在下一个较旧的段中找到该关键字.</li>
<li>可以在后台运行合并和压缩过程以组合段文件并丢弃覆盖或删除的值.</li>
</ol>
<p>基于这种合并和压缩排序文件原理的存储引擎通常被称为 LSM 存储引擎(LevelDB, RocksDB, HBase, BigTable)</p>
<h5 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h5><ol>
<li>当查找数据库中不存在的值时, LSM 会慢, 因为要找到最老的 seg, 解决方法是用 Bloom Filter(粗糙集理论也可以了解下). 节省为不存在的键浪费的 IO.</li>
<li>还有不同的策略来确定 SSTables 如何被压缩和合并的顺序和时间. 最常见的选择是 size-tiered 和 leveled compaction. LevelDB 和 RocksDB 使用平坦压缩(LevelDB 因此得名), HBase 使用size-tiered , Cassandra 同时支持.</li>
</ol>
<p>即使有很多可选优化, 但是 LSM-trees 的基本 idea:保存一系列在后台合并的 SSTables, 简单且有效. 即使数据集比可用内存大的多, 也可以支持高效的范围查询, 而且支持非常高的写入.</p>
<h5 id="问题-amp-解决"><a href="#问题-amp-解决" class="headerlink" title="问题&amp;解决"></a>问题&amp;解决</h5><p>如果数据库崩溃, 则最近的写入(在内存表中, 但尚未写入磁盘)将丢失, 解决方案: WAL(write ahead log), 和前面的一样, 这也是个 log 文件, 写入的时候先写到磁盘上. 但 mem table =&gt; SSTable 的时候, WAL 可以被丢弃.</p>
<h2 id="B-Trees"><a href="#B-Trees" class="headerlink" title="B-Trees"></a>B-Trees</h2><p>像 SSTables 一样, B 树保持按键排序的键值对, 这允许高效的键值查找和范围查询, 但B 树有着非常不同的设计理念.</p>
<p>我们前面看到的日志结构索引将数据库分解为可变大小的段, 通常是几兆字节或更大的大小; B 树将数据库分解成固定大小的块或页面, 传统上大小为 4KB.并且一次只能读取或写入一个页面. 这种设计更接近于底层硬件.因为磁盘也被安排在固定大小的块中.</p>
<p>每个页面都可以使用地址或位置来标识, 这允许一个页面引用另一个页面 —— 类似于指针, 但在磁盘而不是在内存中. 我们可以使用这些页面引用来构建一个页面树.</p>
<p>我们要找251, 于是在[200, 300] 中间找<br><img src="/2018/08/26/Designing-Data-Intensive-Applications-Storage/89272936.jpg"></p>
<p>在 B 树的一个页面中对子页面的引用的数量称为分支因子, 在上面图中, 分支因子是6. 在实践中, 分支因子取决于存储页面参考和范围边界所需的空间量, 但通常是几百个</p>
<p>如果要更新 B 树中现有键的值, 则搜索包含该键的叶页, 更改该页中的值, 并将该页写回到磁盘(对该页的任何引用保持有效). 如果你想添加一个新的键, 你需要找到其范围包含新键的页面, 并将其添加到该页面.如果页面中没有足够的可用空间容纳新键, 则将其分成两个半满页面, 并更新父页面以解释键范围的新分区</p>
<p>该算法确保树保持平衡:具有 n 个键的 B 树总是具有 O(log n)的深度.大多数数据库可以放入一个三到四层的 B 树, 所以你不需要遵追踪多页面引用来找到你正在查找的页面. 分支因子为 500 的 4KB 页面的四级树可以存储多达 256TB.</p>
<p><img src="/2018/08/26/Designing-Data-Intensive-Applications-Storage/30223544.jpg"></p>
<p>B 树的基本底层写操作是用新数据覆盖磁盘上的页面, 但是引用不变, 这个和 LSM Tree 正好相反(只附加, 从不修改文件).</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li>需要拆分页面的时候, 要更新父页面对两个子页面的引用. 因为数据库随时可能崩溃. 解决方案: WAL(redo log), 这个日志被用来使 B 树恢复到一致的状态.</li>
<li>如果多个线程要同时访问 B 树, 则需要仔细的并发控制(latches).</li>
<li>B 树索引必须至少两次写入每一段数据:一次写入预先写入日志, 一次写入树页面本身（也许再次分页）. 即使在该页面中只有几个字节发生了变化, 也需要一次编写整个页面的开销. 有些存储引擎甚至会覆盖同一个页面两次, 以免在电源故障的情况下导致页面部分更新.</li>
</ol>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><ol>
<li>一些数据库(如 LMDB)使用写时复制方案, 而不是覆盖页面并维护 WAL 进行崩溃恢复.修改的页面被写入到不同的位置, 并且树中的父页面的新版本被创建, 指向新的位置. 这种方法对于并发控制也很有用.</li>
<li>可以通过不存储整个 key 来节省空间. key 只需要提供足够的信息来充当 key range 的边界. 一个 page 有更多的 key, 运行树有更高的分支因子, 减少层数.</li>
<li>通常, page 可以放置在磁盘上的任何位置, 如果查询需要按照排序顺序扫描大部分关键字范围, 那么每个页面的布局可能会非常不方便, 因为每个读取的页面都可能需要磁盘查找. 因此许多 B 树实现尝试叶子页面按顺序出现在磁盘上, 但是随着树的增长,维持这个顺序是很困难的. 由于 LSM 树在合并过程中一次又一次地重写存储的大部分, 所以它们更容易使顺序键在磁盘上彼此靠近.</li>
<li>添加额外的指针, eg:每个叶子页面可以在左边和右边具有对其兄弟页面的引用, 这样就能顺序扫描 key 而不跳回父页面.</li>
</ol>
<h3 id="LSM-Tree-VS-B-Tree"><a href="#LSM-Tree-VS-B-Tree" class="headerlink" title="LSM Tree VS B Tree"></a>LSM Tree VS B Tree</h3><h4 id="LSM"><a href="#LSM" class="headerlink" title="LSM"></a>LSM</h4><ol>
<li>LSM 写快, B 读快. LSM 顺序地写入紧凑的 SSTable 文件而不是必须覆盖树中的几个页面.</li>
<li>LSM 树可以被压缩得更好, B 树存储引擎会由于分割而留下一些未使用的磁盘空间.</li>
<li>在许多 SSD 上, 固件内部使用日志结构化算法, 将随机写入转变为顺序写入底层存储芯片. 但是较低的写入放大率和减少的碎片对 SSD 仍然有利: 更紧凑地表示数据可在可用的 I/O 带宽内提供更多的读取和写入请求.</li>
</ol>
<h4 id="B-Tree"><a href="#B-Tree" class="headerlink" title="B Tree"></a>B Tree</h4><ol>
<li>日志结构存储的缺点是压缩过程有时会干扰正在进行的读写操作, 很容易发生请求需要等待而磁盘完成昂贵的压缩操作. 对吞吐量和平均响应时间的影响通常很小, 但是在更高百分比的情况下（参阅 “描述性能”）, 对日志结构化存储引擎的查询响应时间有时会相当长, 而 B 树的行为则相对更具可预测性.</li>
<li>压缩的另一个问题出现在高写入吞吐量: 磁盘的有限写入带宽需要在初始写入(记录和刷新内存表到磁盘)和在后台运行的压缩线程之间共享. 写入空数据库时, 可以使用全磁盘带宽进行初始写入, 但数据库越大, 压缩所需的磁盘带宽就越多. 如果写入吞吐量很高, 并且压缩没有仔细配置, 压缩跟不上写入速率. 在这种情况下, 磁盘上未合并段的数量不断增加, 直到磁盘空间用完, 读取速度也会减慢, 因为它们需要检查更多段文件. 通常情况下, 即使压缩无法跟上, 基于 SSTable 的存储引擎也不会限制传入写入的速率, 所以需要进行明确的监控来检测这种情况</li>
<li>B 树的一个优点是每个键只存在于索引中的一个位置, 而日志结构化的存储引擎可能在不同的段中有相同键的多个副本. 这个方面使得 B 树在想要提供强大的事务语义的数据库中很有吸引力. 在许多关系数据库中, 事务隔离是通过在键范围上使用锁来实现的, 在 B 树索引中, 这些锁可以直接连接到它.</li>
</ol>
<h1 id="列存储"><a href="#列存储" class="headerlink" title="列存储"></a>列存储</h1><p>如果一列的值, count 很大, 但是基数不大, 可以使用 bit map. 可以很高效的压缩.<br><img src="/2018/08/26/Designing-Data-Intensive-Applications-Storage/37827590.jpg"></p>
<h2 id="vectorized-process"><a href="#vectorized-process" class="headerlink" title="vectorized process"></a>vectorized process</h2><p>除了减少需要从磁盘加载的数据量以外, 面向列的存储布局也可以有效利用 CPU 周期. 例如 query engine 可以把一块(chunk) 的  compressed column data(为了单位信息密度更大) 放到 CPU 的 L1 cache. 前面说的按位与/或可以直接 apply 到这些 chunk 上.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/08/22/Parquet-encoding-definitions-official/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/08/22/Parquet-encoding-definitions-official/" class="post-title-link" itemprop="url">Parquet encoding definitions(official)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-08-22 20:49:53" itemprop="dateCreated datePublished" datetime="2018-08-22T20:49:53+08:00">2018-08-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:04:59" itemprop="dateModified" datetime="2021-05-08T11:04:59+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Parquet-encoding-definitions"><a href="#Parquet-encoding-definitions" class="headerlink" title="Parquet encoding definitions"></a><a target="_blank" rel="noopener" href="https://github.com/apache/parquet-format/blob/master/Encodings.md">Parquet encoding definitions</a></h1><p>This file contains the specification of all supported encodings.</p>
<h3 id="Plain-PLAIN-0"><a href="#Plain-PLAIN-0" class="headerlink" title="Plain: (PLAIN = 0)"></a>Plain: (PLAIN = 0)</h3><p>Supported Types: all</p>
<p>This is the plain encoding that must be supported for types.  It is<br>intended to be the simplest encoding.  Values are encoded back to back.</p>
<p>The plain encoding is used whenever a more efficient encoding can not be used. It<br>stores the data in the following format:</p>
<ul>
<li>BOOLEAN: <a href="#RLE">Bit Packed</a>, LSB first</li>
<li>INT32: 4 bytes little endian</li>
<li>INT64: 8 bytes little endian</li>
<li>INT96: 12 bytes little endian (deprecated)</li>
<li>FLOAT: 4 bytes IEEE little endian</li>
<li>DOUBLE: 8 bytes IEEE little endian</li>
<li>BYTE_ARRAY: length in 4 bytes little endian followed by the bytes contained in the array</li>
<li>FIXED_LEN_BYTE_ARRAY: the bytes contained in the array</li>
</ul>
<p>For native types, this outputs the data as little endian. Floating<br>    point types are encoded in IEEE.</p>
<p>For the byte array type, it encodes the length as a 4 byte little<br>endian, followed by the bytes.</p>
<h3 id="Dictionary-Encoding-PLAIN-DICTIONARY-2-and-RLE-DICTIONARY-8"><a href="#Dictionary-Encoding-PLAIN-DICTIONARY-2-and-RLE-DICTIONARY-8" class="headerlink" title="Dictionary Encoding (PLAIN_DICTIONARY = 2 and RLE_DICTIONARY = 8)"></a>Dictionary Encoding (PLAIN_DICTIONARY = 2 and RLE_DICTIONARY = 8)</h3><p>The dictionary encoding builds a dictionary of values encountered in a given column. The<br>dictionary will be stored in a dictionary page per column chunk. The values are stored as integers<br>using the <a href="#RLE">RLE/Bit-Packing Hybrid</a> encoding. If the dictionary grows too big, whether in size<br>or number of distinct values, the encoding will fall back to the plain encoding. The dictionary page is<br>written first, before the data pages of the column chunk.</p>
<p>Dictionary page format: the entries in the dictionary - in dictionary order - using the <a href="#PLAIN">plain</a> encoding.</p>
<p>Data page format: the bit width used to encode the entry ids stored as 1 byte (max bit width = 32),<br>followed by the values encoded using RLE/Bit packed described above (with the given bit width).</p>
<p>Using the PLAIN_DICTIONARY enum value is deprecated in the Parquet 2.0 specification. Prefer using RLE_DICTIONARY<br>in a data page and PLAIN in a dictionary page for Parquet 2.0+ files.</p>
<h3 id="Run-Length-Encoding-Bit-Packing-Hybrid-RLE-3"><a href="#Run-Length-Encoding-Bit-Packing-Hybrid-RLE-3" class="headerlink" title="Run Length Encoding / Bit-Packing Hybrid (RLE = 3)"></a><a name="RLE"></a>Run Length Encoding / Bit-Packing Hybrid (RLE = 3)</h3><p>This encoding uses a combination of bit-packing and run length encoding to more efficiently store repeated values.</p>
<p>The grammar for this encoding looks like this, given a fixed bit-width known in advance:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">rle-bit-packed-hybrid: &lt;length&gt; &lt;encoded-data&gt;</span><br><span class="line">length := length of the &lt;encoded-data&gt; in bytes stored as 4 bytes little endian (unsigned int32)</span><br><span class="line">encoded-data := &lt;run&gt;*</span><br><span class="line">run := &lt;bit-packed-run&gt; | &lt;rle-run&gt;</span><br><span class="line">bit-packed-run := &lt;bit-packed-header&gt; &lt;bit-packed-values&gt;</span><br><span class="line">bit-packed-header := varint-encode(&lt;bit-pack-scaled-run-len&gt; &lt;&lt; 1 | 1)</span><br><span class="line">// we always bit-pack a multiple of 8 values at a time, so we only store the number of values / 8</span><br><span class="line">bit-pack-scaled-run-len := (bit-packed-run-len) / 8</span><br><span class="line">bit-packed-run-len := *see 3 below*</span><br><span class="line">bit-packed-values := *see 1 below*</span><br><span class="line">rle-run := &lt;rle-header&gt; &lt;repeated-value&gt;</span><br><span class="line">rle-header := varint-encode( (rle-run-len) &lt;&lt; 1)</span><br><span class="line">rle-run-len := *see 3 below*</span><br><span class="line">repeated-value := value that is repeated, using a fixed-width of round-up-to-next-byte(bit-width)</span><br></pre></td></tr></table></figure>

<ol>
<li><p>The bit-packing here is done in a different order than the one in the <a href="#BITPACKED">deprecated bit-packing</a> encoding.<br>The values are packed from the least significant bit of each byte to the most significant bit,<br>though the order of the bits in each value remains in the usual order of most significant to least<br>significant. For example, to pack the same values as the example in the deprecated encoding above:</p>
<p>The numbers 1 through 7 using bit width 3:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dec value: 0   1   2   3   4   5   6   7</span><br><span class="line">bit value: 000 001 010 011 100 101 110 111</span><br><span class="line">bit label: ABC DEF GHI JKL MNO PQR STU VWX</span><br></pre></td></tr></table></figure>

<p>would be encoded like this where spaces mark byte boundaries (3 bytes):</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bit value: 10001000 11000110 11111010</span><br><span class="line">bit label: HIDEFABC RMNOJKLG VWXSTUPQ</span><br></pre></td></tr></table></figure>

<p>The reason for this packing order is to have fewer word-boundaries on little-endian hardware<br>when deserializing more than one byte at at time. This is because 4 bytes can be read into a<br>32 bit register (or 8 bytes into a 64 bit register) and values can be unpacked just by<br>shifting and ORing with a mask. (to make this optimization work on a big-endian machine,<br>you would have to use the ordering used in the <a href="#BITPACKED">deprecated bit-packing</a> encoding)</p>
</li>
<li><p>varint-encode() is ULEB-128 encoding, see <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/LEB128">https://en.wikipedia.org/wiki/LEB128</a></p>
</li>
<li><p>bit-packed-run-len and rle-run-len must be in the range [1, 2<sup>31</sup> - 1].<br>This means that a Parquet implementation can always store the run length in a signed<br>32-bit integer. This length restriction was not part of the Parquet 2.5.0 and earlier<br>specifications, but longer runs were not readable by the most common Parquet<br>implementations so, in practice, were not safe for Parquet writers to emit.</p>
</li>
</ol>
<p>Note that the RLE encoding method is only supported for the following types of<br>data:</p>
<ul>
<li>Repetition and definition levels</li>
<li>Dictionary indices</li>
<li>Boolean values in data pages, as an alternative to PLAIN encoding</li>
</ul>
<h3 id="Bit-packed-Deprecated-BIT-PACKED-4"><a href="#Bit-packed-Deprecated-BIT-PACKED-4" class="headerlink" title="Bit-packed (Deprecated) (BIT_PACKED = 4)"></a><a name="BITPACKED"></a>Bit-packed (Deprecated) (BIT_PACKED = 4)</h3><p>This is a bit-packed only encoding, which is deprecated and will be replaced by the <a href="#RLE">RLE/bit-packing</a> hybrid encoding.<br>Each value is encoded back to back using a fixed width.<br>There is no padding between values (except for the last byte) which is padded with 0s.<br>For example, if the max repetition level was 3 (2 bits) and the max definition level as 3<br>(2 bits), to encode 30 values, we would have 30 * 2 = 60 bits = 8 bytes.</p>
<p>This implementation is deprecated because the <a href="#RLE">RLE/bit-packing</a> hybrid is a superset of this implementation.<br>For compatibility reasons, this implementation packs values from the most significant bit to the least significant bit,<br>which is not the same as the <a href="#RLE">RLE/bit-packing</a> hybrid.</p>
<p>For example, the numbers 1 through 7 using bit width 3:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dec value: 0   1   2   3   4   5   6   7</span><br><span class="line">bit value: 000 001 010 011 100 101 110 111</span><br><span class="line">bit label: ABC DEF GHI JKL MNO PQR STU VWX</span><br></pre></td></tr></table></figure>
<p>would be encoded like this where spaces mark byte boundaries (3 bytes):</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bit value: 00000101 00111001 01110111</span><br><span class="line">bit label: ABCDEFGH IJKLMNOP QRSTUVWX</span><br></pre></td></tr></table></figure>

<p>Note that the BIT_PACKED encoding method is only supported for encoding<br>repetition and definition levels.</p>
<h3 id="Delta-Encoding-DELTA-BINARY-PACKED-5"><a href="#Delta-Encoding-DELTA-BINARY-PACKED-5" class="headerlink" title="Delta Encoding (DELTA_BINARY_PACKED = 5)"></a><a name="DELTAENC"></a>Delta Encoding (DELTA_BINARY_PACKED = 5)</h3><p>Supported Types: INT32, INT64</p>
<p>This encoding is adapted from the Binary packing described in <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1209.2137v5.pdf">“Decoding billions of integers per second through vectorization”</a> by D. Lemire and L. Boytsov</p>
<p>Delta encoding consists of a header followed by blocks of delta encoded values binary packed. Each block is made of miniblocks, each of them binary packed with its own bit width. When there are not enough values to encode a full block we pad with zeros (added to the frame of reference).<br>The header is defined as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;block size in values&gt; &lt;number of miniblocks in a block&gt; &lt;total value count&gt; &lt;first value&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>the block size is a multiple of 128 stored as VLQ int</li>
<li>the miniblock count per block is a diviser of the block size stored as VLQ int the number of values in the miniblock is a multiple of 32.</li>
<li>the total value count is stored as a VLQ int</li>
<li>the first value is stored as a zigzag VLQ int</li>
</ul>
<p>Each block contains</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;min delta&gt; &lt;list of bitwidths of miniblocks&gt; &lt;miniblocks&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>the min delta is a VLQ int (we compute a minimum as we need positive integers for bit packing)</li>
<li>the bitwidth of each block is stored as a byte</li>
<li>each miniblock is a list of bit packed ints according to the bit width stored at the begining of the block</li>
</ul>
<p>Having multiple blocks allows us to escape values and restart from a new base value.</p>
<p>To encode each delta block, we will:</p>
<ol>
<li><p>Compute the deltas</p>
</li>
<li><p>Encode the first value as zigzag VLQ int</p>
</li>
<li><p>For each block, compute the frame of reference(minimum of the deltas) for the deltas. This guarantees<br>all deltas are positive.</p>
</li>
<li><p>encode the frame of reference delta as VLQ int followed by the delta values (minus the minimum) encoded as bit packed per miniblock.</p>
</li>
</ol>
<p>Steps 2 and 3 are skipped if the number of values in the block is 1.</p>
<h4 id="Example-1"><a href="#Example-1" class="headerlink" title="Example 1"></a>Example 1</h4><p>1, 2, 3, 4, 5</p>
<p>After step 1), we compute the deltas as:</p>
<p>1, 1, 1, 1</p>
<p>The minimum delta is 1 and after step 2, the deltas become</p>
<p>0, 0, 0, 0</p>
<p>The final encoded data is:</p>
<p> header:<br>8 (block size), 1 (miniblock count), 5 (value count), 1 (first value)</p>
<p> block<br>1 (minimum delta), 0 (bitwidth), (no data needed for bitwidth 0)</p>
<h4 id="Example-2"><a href="#Example-2" class="headerlink" title="Example 2"></a>Example 2</h4><p>7, 5, 3, 1, 2, 3, 4, 5, the deltas would be</p>
<p>-2, -2, -2, 1, 1, 1, 1</p>
<p>The minimum is -2, so the relative deltas are:</p>
<p>0, 0, 0, 3, 3, 3, 3</p>
<p>The encoded data is</p>
<p> header:<br>8 (block size), 1 (miniblock count), 8 (value count), 7 (first value)</p>
<p> block<br>-2 (minimum delta), 2 (bitwidth), 00000011111111b (0,0,0,3,3,3,3 packed on 2 bits)</p>
<h4 id="Characteristics"><a href="#Characteristics" class="headerlink" title="Characteristics"></a>Characteristics</h4><p>This encoding is similar to the <a href="#RLE">RLE/bit-packing</a> encoding. However the <a href="#RLE">RLE/bit-packing</a> encoding is specifically used when the range of ints is small over the entire page, as is true of repetition and definition levels. It uses a single bit width for the whole page.<br>The delta encoding algorithm described above stores a bit width per mini block and is less sensitive to variations in the size of encoded integers. It is also somewhat doing RLE encoding as a block containing all the same values will be bit packed to a zero bit width thus being only a header.</p>
<h3 id="Delta-length-byte-array-DELTA-LENGTH-BYTE-ARRAY-6"><a href="#Delta-length-byte-array-DELTA-LENGTH-BYTE-ARRAY-6" class="headerlink" title="Delta-length byte array: (DELTA_LENGTH_BYTE_ARRAY = 6)"></a>Delta-length byte array: (DELTA_LENGTH_BYTE_ARRAY = 6)</h3><p>Supported Types: BYTE_ARRAY</p>
<p>This encoding is always preferred over PLAIN for byte array columns.</p>
<p>For this encoding, we will take all the byte array lengths and encode them using delta<br>encoding (DELTA_BINARY_PACKED). The byte array data follows all of the length data just<br>concatenated back to back. The expected savings is from the cost of encoding the lengths<br>and possibly better compression in the data (it is no longer interleaved with the lengths).</p>
<p>The data stream looks like:</p>
<p><Delta encoded lengths> <Byte array data></Byte></Delta></p>
<p>For example, if the data was “Hello”, “World”, “Foobar”, “ABCDEF”:</p>
<p>The encoded data would be DeltaEncoding(5, 5, 6, 6) “HelloWorldFoobarABCDEF”</p>
<h3 id="Delta-Strings-DELTA-BYTE-ARRAY-7"><a href="#Delta-Strings-DELTA-BYTE-ARRAY-7" class="headerlink" title="Delta Strings: (DELTA_BYTE_ARRAY = 7)"></a>Delta Strings: (DELTA_BYTE_ARRAY = 7)</h3><p>Supported Types: BYTE_ARRAY</p>
<p>This is also known as incremental encoding or front compression: for each element in a<br>sequence of strings, store the prefix length of the previous entry plus the suffix.</p>
<p>For a longer description, see <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Incremental_encoding">https://en.wikipedia.org/wiki/Incremental_encoding</a>.</p>
<p>This is stored as a sequence of delta-encoded prefix lengths (DELTA_BINARY_PACKED), followed by<br>the suffixes encoded as delta length byte arrays (DELTA_LENGTH_BYTE_ARRAY).</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/07/19/Elixir/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/07/19/Elixir/" class="post-title-link" itemprop="url">Elixir 基础部分</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-07-19 20:44:00" itemprop="dateCreated datePublished" datetime="2018-07-19T20:44:00+08:00">2018-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2018-10-24 22:49:18" itemprop="dateModified" datetime="2018-10-24T22:49:18+08:00">2018-10-24</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="0-写在前面"><a href="#0-写在前面" class="headerlink" title="0. 写在前面"></a>0. 写在前面</h1><h2 id="编程时应该关注数据装换"><a href="#编程时应该关注数据装换" class="headerlink" title="编程时应该关注数据装换"></a>编程时应该关注数据装换</h2><p>用类和对象思考问题:类定义了行为,实例保存着状态.开发者构造类层次结构,为问题建模.OOP的时候,我们考虑的是状态,调用对象的方法和向某个对象传递其他对象.在这些调用中,对象更新自己或者其他对象的状态.类规范了每一个实例可以干什么,它是统治者,控制着实例的数据状态,目标是隐藏数据.</p>
<p>面向对象的一个缺点是 你想传入一个功能块(方法,函数),但是一定是要包在一个类里面,虽然有匿名函数,但是写法还是很丑,把函数当做一等公民(基本数据类型),直接传入函数的方式是最佳的.</p>
<p>真实世界里,并没有多少真正的层次结构,我们想要把事情搞定,而不是<strong>维护状态</strong>.我不要隐藏数据,我要转换数据.</p>
<h2 id="借助管道来组合转换"><a href="#借助管道来组合转换" class="headerlink" title="借助管道来组合转换"></a>借助管道来组合转换</h2><p>UNIX的设计理念是将每个工具设计的小巧,功能单一,易于组合.每个工具都是获取输入,转换输入内容,并以下一个工具能使用的格式输出.这使得UNIX工具能一设计者预想不到的方式组合,而且这种模式还是高可靠的,每种工具之做好一件事,使之更容易测试.而且命令管道可以并行工作.</p>
<h2 id="函数是数据转换器"><a href="#函数是数据转换器" class="headerlink" title="函数是数据转换器"></a>函数是数据转换器</h2><p>函数越小巧,功能越单一,组合后的灵活性越大.(OO的一个设计理念是类之做好一件事,感觉都是共通的)</p>
<p><strong>数据转换的理念是函数式编程的核心</strong>,当你不再肩负维护数据状态的责任,开始关注如何把事情做好的时候,你就会看到一个新世界</p>
<h2 id="换一种方式思考"><a href="#换一种方式思考" class="headerlink" title="换一种方式思考"></a>换一种方式思考</h2><ol>
<li>程序的基础不是赋值,循环和if</li>
<li>并发不一定需要锁,信号量,监视器等东西</li>
<li>进行不一定要消耗大量的资源</li>
<li>元编程不一定是语言的附属品</li>
<li>编程即使是你的工作,也可以是充满乐趣的.记住<strong>乐而为之</strong></li>
</ol>
<h1 id="1-常规编程"><a href="#1-常规编程" class="headerlink" title="1. 常规编程"></a>1. 常规编程</h1><h2 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h2><p>在Elixir中,<code>=</code>不是赋值,而更像是一种断言(assertion).如果Elixir<strong>可以找到一种方式</strong>让等号的左边等于右边,则执行成功.Elixir把<code>=</code>称之为<strong>匹配运算符</strong><br>如<code>list = [ 1, 2, 3 ]</code>,为了让匹配为真,Elixir将变量list绑定到列表[1,2,3].<br>如 <code>[a, b, c ] = list</code>,Elixir会试图让等号的左边等于右边.左边列表包含三个变量,右边列表包含3个值,所以将值设置到相应变量,等号两边才会相等.如:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; list = [1, 2, 3]</span><br><span class="line">[1, 2, 3]</span><br><span class="line">iex&gt; [a, 2, b ] = list</span><br><span class="line">[1, 2, 3]</span><br><span class="line">iex&gt; a</span><br><span class="line">1</span><br><span class="line">iex&gt; b</span><br><span class="line">3</span><br></pre></td></tr></table></figure>

<p>这个很好的说明了是断言,而不是赋值,因为求出了a,b.这个叫做<strong>模式匹配</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; x = 1</span><br><span class="line">iex&gt; 1 = x</span><br></pre></td></tr></table></figure>

<p><code>1 = x</code>也是成立的,这个在其他语言中是不行的,因为左右两边都是1,所以匹配.(elixir判断是否相等是用<code>===</code>和<code>==</code>)</p>
<p>模式匹配使得开发者能够简单地解构例如元组和列表的数据类型。在之后的章节中我们将看到这是Elixir中递归的基础，且其适用于其它类型，例如映射与二进制。</p>
<h3 id="用-下划线-忽略匹配值"><a href="#用-下划线-忽略匹配值" class="headerlink" title="用_(下划线)忽略匹配值"></a>用_(下划线)忽略匹配值</h3><p><code>_</code>就像一个通配符,可以接受任何值.变量_的特别之处在于它永远不可以被读取</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; [1, _, _] = [1, 2, 3]</span><br><span class="line">[1, 2, 3]</span><br><span class="line">iex&gt; [1, _, _] = [1, &quot;cat&quot;, &quot;dog&quot;]</span><br><span class="line">[1, &quot;cat&quot;, &quot;dog&quot;]</span><br></pre></td></tr></table></figure>

<p>强制让变量的已有值参与匹配,而不是变成绑定新值(当你想要对变量值进行模式匹配，而不是重新赋值时)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; a = 1</span><br><span class="line">1</span><br><span class="line">iex&gt; [^a, 2, 3 ] = [ 1, 2, 3 ] # use existing value of a</span><br><span class="line">[1, 2, 3]</span><br><span class="line">iex&gt; a = 2</span><br><span class="line">2</span><br><span class="line">iex&gt; [ ^a, 2 ] = [ 1, 2 ] #因为这里a用的是2,不能被赋值成2</span><br></pre></td></tr></table></figure>

<h2 id="从另一个角度看等号"><a href="#从另一个角度看等号" class="headerlink" title="从另一个角度看等号"></a>从另一个角度看等号</h2><p>Erlang的等号可以看成是代数里的等号,方程x = a + 1,不是将a + 1 赋值给x,而是断言x 和 a + 1 相等.一旦知道x,就能求出a,反之亦然.</p>
<p>模式匹配是Elixir的核心,将用它做条件判断,函数调用(function call),函数被调用(function invocation)</p>
<h2 id="不可变性"><a href="#不可变性" class="headerlink" title="不可变性"></a>不可变性</h2><p><strong>GOTO是邪恶的,因为我们会问:”我如何获得执行过程的入口点?”,而可变性带给我们的问题则是:”我怎么样得到这个状态?”</strong></p>
<p>看下面的这个例子:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">count = 99</span><br><span class="line">doSometingWith( count )</span><br><span class="line">print( count )</span><br></pre></td></tr></table></figure>

<p>此时你还能确信你能输出99吗?不确定性是幸福感的最大杀手啊!更糟的是还有多线程,他们都修改这个count的话,画面太美不敢想象.</p>
<h2 id="不可变数据才是已知的"><a href="#不可变数据才是已知的" class="headerlink" title="不可变数据才是已知的"></a>不可变数据才是已知的</h2><p><strong>编程就是进行数据转换</strong>,当更新[ 1, 2, 3 ]时,我们不是在原地修改它,而是将它转换成新数据.</p>
<h2 id="不可变性对性能的影响"><a href="#不可变性对性能的影响" class="headerlink" title="不可变性对性能的影响"></a>不可变性对性能的影响</h2><p>大家可能有个直觉复制是低效的,还会留下许多垃圾.事实上:<strong>正因为某个数据永远都不会改变,所以可以简单的用来重用</strong></p>
<h3 id="复制数据"><a href="#复制数据" class="headerlink" title="复制数据"></a>复制数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; list1 = [ 3, 2, 1 ]</span><br><span class="line">[3, 2, 1]</span><br><span class="line">iex&gt; list2 = [ 4 | list1 ]</span><br><span class="line">[4, 3, 2, 1]</span><br></pre></td></tr></table></figure>

<p>在可变的语言里,list2会新建一个列表,并把list1的值拷贝过来,但是在elixir中,它知道list1永远都不会改变,所以它简单的用4作为首项,把list1作为尾部创建一个新的列表.</p>
<h3 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h3><p>垃圾回收器影响性能,这个是共识了,但是在elixir中,他是以进程为基本单位的,而每个进程都有自己的堆,应用程序的数据由这些堆分摊,跟所以把所有数据放到一个堆里的情况比,每个单独的堆是很小的.因此,垃圾回收的速度会更快.并且,进程终止时,所有数据会被删除,没有必要垃圾回收.</p>
<h2 id="elixir基础"><a href="#elixir基础" class="headerlink" title="elixir基础"></a>elixir基础</h2><h3 id="基本类型"><a href="#基本类型" class="headerlink" title="基本类型"></a>基本类型</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; 1          # integer</span><br><span class="line">iex&gt; 0x1F       # integer</span><br><span class="line">iex&gt; 1.0        # float</span><br><span class="line">iex&gt; true       # boolean</span><br><span class="line">iex&gt; :atom      # atom / symbol,可以认为是常量,名字就是值,两个同名原子任何情况下都相等</span><br><span class="line">iex&gt; &quot;elixir&quot;   # string</span><br><span class="line">iex&gt; [1, 2, 3]  # list</span><br><span class="line">iex&gt; &#123;1, 2, 3&#125;  # tuple</span><br></pre></td></tr></table></figure>

<p><strong>字符串</strong>在Elixir内部被表示为二进制数值（binaries），也就是一连串的字节（bytes）：<code>iex&gt; is_binary(&quot;hellö&quot;) &gt;ture</code>; Elixir支持字符串插值（和ruby一样使用#{ … }）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; &quot;hellö #&#123;:world&#125;&quot;</span><br><span class="line">&quot;hellö world&quot;</span><br></pre></td></tr></table></figure>

<p><strong>列表与元组的区别</strong>：</p>
<p><strong>列表</strong>在内存中是以链表的形式存储的,元素值和指向下一个元素的指针为列表的一个单元(cons cell,就是有点类似 [ head | tail ] ).所以列表的前置拼接很快<code>[0] ++ list</code>,后置拼接就比较慢,但是要遍历就慢,比如获取长度,访问某个元素.</p>
<p>递归定义的列表是elixir编程的核心部分之一.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; list = [1|[2|[3|[]]]]</span><br><span class="line">[1, 2, 3]</span><br></pre></td></tr></table></figure>

<p>可以用<code>hd</code>和<code>tl</code>取出头尾,尝试从一个空列表中取出头或尾将会报错.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; list = [1,2,3]</span><br><span class="line">iex&gt; hd(list)</span><br><span class="line">1</span><br><span class="line">iex&gt; tl(list)</span><br><span class="line">[2, 3]</span><br></pre></td></tr></table></figure>

<p>列表还有一个性能优势,要剔除列表的首部,只保留尾部,不需要拷贝列表,只要返回尾部的指针即可.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; [ 1, 2, 3 ] ++ [ 4, 5, 6 ] # concatenation</span><br><span class="line">[1, 2, 3, 4, 5, 6]</span><br><span class="line">iex&gt; [1, 2, 3, 4] -- [2, 4] # difference</span><br><span class="line">[1, 3]</span><br><span class="line">iex&gt; 1 in [1,2,3,4] # membership</span><br><span class="line">true</span><br><span class="line">iex&gt; &quot;wombat&quot; in [1, 2, 3, 4]</span><br><span class="line">false</span><br></pre></td></tr></table></figure>

<p><strong>元组</strong><br>通常元组由两到四个元素组成,如果有更多元素,可以用散列表或者结构体<br>连续空间(数组),获取元组大小,或者使用索引访问元组元素的操作十分快速,添加修改元素开销大,因为这些操作会在内存中对元组的进行整体复制.</p>
<p>当需要计算某数据结构包含的元素个数时，Elixir遵循一个简单的规则： 如果操作在常数时间内完成（答案是提前算好的），这样的函数通常被命名为 *size。 而如果操作需要显式计数，那么该函数通常命名为 *length。</p>
<p>通常函数在不出错的情况下会返回一个元组</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; &#123;status, file&#125; = File.open(&quot;mix.exs&quot;)</span><br><span class="line">&#123;:ok, #PID&lt;0.39.0&gt;&#125;</span><br></pre></td></tr></table></figure>

<p>所以有个惯用法是假定会成功的匹配</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; &#123; :ok, file &#125; = File.open(&quot;Rakefile&quot;)</span><br><span class="line">&#123;:ok, #PID&lt;0.39.0&gt;&#125; # 成功</span><br><span class="line">** (MatchError) no match of right hand side value: &#123;:error, :enoent&#125; # 失败</span><br></pre></td></tr></table></figure>

<p><strong>关键字列表</strong>:k-v</p>
<p>散列表:map,<code>%&#123; key =&gt; value, key =&gt; value &#125;%</code></p>
<p>散列字典:HashDict,<code>&lt;[ fg: &quot;black&quot;, bg: &quot;white&quot;, font: &quot;Merriweather&quot; ]&gt;</code></p>
<p>关键字列表:keyword, <code>[ fg: &quot;black&quot;, bg: &quot;white&quot;, font: &quot;Merriweather&quot; ]</code>,只有他允许一对多(k-v)</p>
<p><code>[ name: &quot;Dave&quot;, city: &quot;Dallas&quot;, likes: &quot;Programming&quot; ]</code> elixir会转换成 <code>[ &#123;:name, &quot;Dave&quot;&#125;, &#123;:city, &quot;Dallas&quot;&#125;, &#123;:likes, &quot;Programming&quot;&#125; ]</code></p>
<p>作为函数调用的最后一个参数的时候可以省略<code>[]</code> .例如<code>DB.save record, [ &#123;:use_transaction, true&#125;, &#123;:logging, &quot;HIGH&quot;&#125; ]</code>可以写成<code>DB.save record, use_transaction: true, logging: &quot;HIGH&quot;</code></p>
<p>当我们有了一个元组（不一定仅有两个元素的元组）的列表，并且每个元组的第一个元素是个 原子， 那就称之为键值列表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; list = [&#123;:a, 1&#125;, &#123;:b, 2&#125;]</span><br><span class="line">[a: 1, b: 2]</span><br><span class="line">iex&gt; list == [a: 1, b: 2]</span><br><span class="line">true</span><br><span class="line">iex&gt; list[:a]</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<p>实际上,这就是一个简单的列表而已,列表的操作都可以干.</p>
<p>关键字列表在任意期望列表值的上下文的最后一项出现,可以省略列表的括号.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; [1, fred: 1, dave: 2]</span><br><span class="line">[1, &#123;:fred, 1&#125;, &#123;:dave, 2&#125;]</span><br><span class="line">iex&gt; &#123;1, fred: 1, dave: 2&#125;</span><br><span class="line">&#123;1, [fred: 1, dave: 2]&#125;</span><br></pre></td></tr></table></figure>

<p>特点:</p>
<ol>
<li>有序</li>
<li>key可以重复,重复key取值时，取回来的是第一个找到的（因为有序）</li>
</ol>
<p>关键字列表在Elixir中一般就作为函数调用的可选项,或者传递一下参数,而想获得关联数组的话还是使用散列表.</p>
<p><strong>散列表</strong>:k-v<br>和键值列表对比，散列表有两主要区别：</p>
<ul>
<li>图允许任何类型值作为键</li>
<li>图的键没有顺序</li>
</ul>
<p>如果你向散列表添加一个已有的键，将会覆盖之前的键-值对,一些例子:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; states = %&#123; &quot;AL&quot; =&gt; &quot;Alabama&quot;, &quot;WI&quot; =&gt; &quot;Wisconsin&quot; &#125;</span><br><span class="line">%&#123;&quot;AL&quot; =&gt; &quot;Alabama&quot;, &quot;WI&quot; =&gt; &quot;Wisconsin&quot;&#125;</span><br><span class="line"># 用元组做key</span><br><span class="line">iex&gt; responses = %&#123; &#123; :error, :enoent &#125; =&gt; :fatal, &#123; :error, :busy &#125; =&gt; :retry &#125;</span><br><span class="line">%&#123;&#123;:error, :busy&#125; =&gt; :retry, &#123;:error, :enoent&#125; =&gt; :fatal&#125;</span><br><span class="line"># 如果key是atom类型,可以像关键字列表那样的简略写法</span><br><span class="line">iex&gt; colors = %&#123; red: 0xff0000, green: 0x00ff00, blue: 0x0000ff &#125;</span><br><span class="line">%&#123;blue: 255, green: 65280, red: 16711680&#125;</span><br><span class="line"># 访问散列表</span><br><span class="line">iex&gt; colors[:red]</span><br><span class="line">16711680</span><br><span class="line">iex&gt; colors.green # 要是键是原子类型,还可以使用点符号.</span><br><span class="line">6528</span><br><span class="line"># key可以为不同的类型</span><br><span class="line">iex&gt; %&#123; &quot;one&quot; =&gt; 1, :two =&gt; 2, &#123;1,1,1&#125; =&gt; 3 &#125;</span><br><span class="line">%&#123;:two =&gt; 2, &#123;1, 1, 1&#125; =&gt; 3, &quot;one&quot; =&gt; 1&#125;</span><br></pre></td></tr></table></figure>

<p><strong>还有系统类型</strong>:PID,端口,引用,这些用于进程交互</p>
<h2 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h2><p>匿名函数用fn创建,funName.(parm1, parm2, …),点符号代表函数调用.</p>
<h3 id="一个函数-多个函数体"><a href="#一个函数-多个函数体" class="headerlink" title="一个函数,多个函数体"></a>一个函数,多个函数体</h3><p>单个函数定义中,定义不同的的实现,取决于传入的参数类型和内容,但不能根据参数数目进行选择,函数定义中的每个子句必须有相同数目的参数</p>
<p>简单来说,可以用模式匹配来选择要运行的子句(模式匹配很重要,有点难理解,但是一定要慢慢的理解),感觉就是if else的缩略版本?</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; handle_open = fn Line 1</span><br><span class="line">...&gt; &#123;:ok, file&#125; -&gt; &quot;Read data: #&#123;IO.read(file, :line)&#125;&quot; 2</span><br><span class="line">...&gt; &#123;_, error&#125; -&gt; &quot;Error: #&#123;:file.format_error(error)&#125;&quot; 3</span><br><span class="line">...&gt; end 4</span><br><span class="line"></span><br><span class="line">iex&gt; handle_open.(File.open(&quot;code/intro/hello.exs&quot;)) # this file exists 6</span><br><span class="line">&quot;Read data: IO.puts \&quot;Hello, World!\&quot;\n&quot; 7</span><br><span class="line"></span><br><span class="line">iex&gt; handle_open.(File.open(&quot;nonexistent&quot;)) # this one doesn&#x27;t 8</span><br><span class="line">&quot;Error: no such file or directory&quot; </span><br></pre></td></tr></table></figure>

<h3 id="返回函数的函数"><a href="#返回函数的函数" class="headerlink" title="返回函数的函数"></a>返回函数的函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; fun1 = fn -&gt; (fn -&gt; &quot;Hello&quot; end) end</span><br><span class="line">#Function&lt;12.17052888 in :erl_eval.expr/5&gt;</span><br><span class="line"></span><br><span class="line">iex&gt; other = fun1.()</span><br><span class="line">#Function&lt;12.17052888 in :erl_eval.expr/5&gt;</span><br><span class="line"></span><br><span class="line">iex&gt; other.()</span><br><span class="line">&quot;Hello&quot;</span><br></pre></td></tr></table></figure>

<h3 id="记住原始环境的函数-闭包"><a href="#记住原始环境的函数-闭包" class="headerlink" title="记住原始环境的函数(闭包)"></a>记住原始环境的函数(闭包)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; greeter = fn name -&gt; (fn -&gt; &quot;Hello #&#123;name&#125;&quot; end) end</span><br><span class="line">#Function&lt;12.17052888 in :erl_eval.expr/5&gt;</span><br><span class="line"></span><br><span class="line">iex&gt; dave_greeter = greeter.(&quot;Dave&quot;)</span><br><span class="line">#Function&lt;12.17052888 in :erl_eval.expr/5&gt;</span><br><span class="line"></span><br><span class="line">iex&gt; dave_greeter.()</span><br><span class="line">&quot;Hello Dave&quot;</span><br></pre></td></tr></table></figure>

<p>在返回<code>greeter.(&quot;Dave&quot;)</code>时,只是返回了一个函数,没有把name代换进去(所以这里不是把Dave放到name的时候),这里只返回了一个内部函数的定义(你就当做第一个fn就是返回了一个什么元素 只不过这个元素是个函数,没有深入到那个元素里面).当我们调用内部函数的时候,外部函数已经返回,参数的生命周期也已经终结.为啥内部函数还能取用到这个变量是因为这个作用域被绑定到外部函数上,当内部函数被定义的时候,这个绑定了name的作用域继承到了内部函数.这就是闭包-<strong>作用于将其中的变量绑定封闭起来,并将它们打包到稍后能被保存并且使用的东西上</strong></p>
<h3 id="参数化函数"><a href="#参数化函数" class="headerlink" title="参数化函数"></a>参数化函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; add_n = fn n -&gt; (fn other -&gt; n + other end) end</span><br><span class="line">iex&gt; add_two = add_n.(2)</span><br><span class="line">iex&gt; add_five = add_n.(5)</span><br><span class="line"></span><br><span class="line">iex&gt; add_two.(3)</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">iex&gt; add_five.(7)</span><br><span class="line">12</span><br></pre></td></tr></table></figure>

<h3 id="将函数作为参数传递"><a href="#将函数作为参数传递" class="headerlink" title="将函数作为参数传递"></a>将函数作为参数传递</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; list = [1, 3, 5, 7, 9]</span><br><span class="line">[1, 3, 5, 7, 9]</span><br><span class="line"></span><br><span class="line">iex&gt; Enum.map list, fn elem -&gt; elem * 2 end</span><br><span class="line">[2, 6, 10, 14, 18]</span><br><span class="line"></span><br><span class="line">iex&gt; Enum.map list, fn elem -&gt; elem * elem end</span><br><span class="line">[1, 9, 25, 49, 81]</span><br><span class="line"></span><br><span class="line">iex&gt; Enum.map list, fn elem -&gt; elem &gt; 6 end</span><br><span class="line">[false, false, false, true, true]</span><br></pre></td></tr></table></figure>

<h3 id="amp-运算符"><a href="#amp-运算符" class="headerlink" title="&amp;运算符"></a>&amp;运算符</h3><p><code>add_one = &amp;(&amp;1 + &amp;2)</code>相当于<code>add_one = fn (a,b) -&gt; a + b end</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; Enum.map [1,2,3,4], &amp;(&amp;1 + 1)</span><br><span class="line">[2, 3, 4, 5]</span><br><span class="line">iex&gt; Enum.map [1,2,3,4], &amp;(&amp;1 * &amp;1)</span><br><span class="line">[1, 4, 9, 16]</span><br><span class="line">iex&gt; Enum.map [1,2,3,4], &amp;(&amp;1 &lt; 3)</span><br><span class="line">[true, true, false, false]</span><br></pre></td></tr></table></figure>

<h2 id="模块与函数"><a href="#模块与函数" class="headerlink" title="模块与函数"></a>模块与函数</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">defmodule Times do</span><br><span class="line">  def double(n) do</span><br><span class="line">    n * 2</span><br><span class="line">  end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>定义了Times模块,只有一个函数,这样调用<code>Times.double 4</code></p>
<h3 id="函数体是代码块"><a href="#函数体是代码块" class="headerlink" title="函数体是代码块"></a>函数体是代码块</h3><p>do…end不是真实的底层语法,真实的底层语法是<code>def double(n), do: n*2</code>,所以通常在单行代码中使用do:语法,多行使用do…end语法</p>
<h3 id="函数调用与模式匹配"><a href="#函数调用与模式匹配" class="headerlink" title="函数调用与模式匹配"></a>函数调用与模式匹配</h3><p>当调用一个命名函数的时候,elixir会尝试匹配第一个子句定义的参数列表,而过不匹配,尝试函数的下一个参数列表直到匹配完成(用来递归,你懂得,记住形参数量必须相等)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">defmodule Factorial do</span><br><span class="line">  def of(0), do: 1 # of是函数名</span><br><span class="line">  def of(n), do: n * of(n - 1)</span><br></pre></td></tr></table></figure>

<p>之后可以用尾递归改进这个实现,还有传入负数,会死循环,下节改</p>
<h3 id="哨兵子句"><a href="#哨兵子句" class="headerlink" title="哨兵子句"></a>哨兵子句</h3><p>由when紧接在函数定义之后的断言</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">defmodule Guard do</span><br><span class="line">  def what_is(x) when is_number(x) do</span><br><span class="line">    IO.puts &quot;#&#123;x&#125; is a number&quot;</span><br><span class="line">  end</span><br><span class="line">  </span><br><span class="line">  def what_is(x) when is_list(x) do</span><br><span class="line">    IO.puts &quot;#&#123;inspect(x)&#125; is a list&quot;</span><br><span class="line">  end</span><br><span class="line">  </span><br><span class="line">  def what_is(x) when is_atom(x) do</span><br><span class="line">    IO.puts &quot;#&#123;x&#125; is an atom&quot;</span><br><span class="line">  end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>改进factorial的死循环</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">defmodule Factorial do</span><br><span class="line">  def of(0), do: 1</span><br><span class="line">  def of(n) when n &gt; 0 do</span><br><span class="line">    n * of(n-1)</span><br><span class="line">  end</span><br><span class="line">en</span><br></pre></td></tr></table></figure>

<p>注意,哨兵子句只支持elixir表达式的一个子集,详细可以看入门指南</p>
<h3 id="默认参数"><a href="#默认参数" class="headerlink" title="默认参数"></a>默认参数</h3><p><code>param\\value</code></p>
<p>例如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def func(p1, p2 \\ 2, p3 \\ 3, p4) do</span><br><span class="line">  IO.inspect [p1, p2, p3, p4]</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h3 id="私有函数"><a href="#私有函数" class="headerlink" title="私有函数"></a>私有函数</h3><p><code>defp</code>仅能在声明它的模块被调用</p>
<h3 id="gt-管道运算符"><a href="#gt-管道运算符" class="headerlink" title="|&gt; 管道运算符"></a>|&gt; 管道运算符</h3><p>|&gt;获得左边表达式的结果,并将其作为第一个参数传给右边的函数调用.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">filing = DB.find_customers</span><br><span class="line">  |&gt; Orders.for_customers</span><br><span class="line">  |&gt; sales_tax(2016)</span><br><span class="line">  |&gt; prepare_filing</span><br></pre></td></tr></table></figure>

<p><code>val |&gt; f(a, b)</code> 等价于 <code>f(val, a, b)</code></p>
<h3 id="使用头部和尾部构造列表"><a href="#使用头部和尾部构造列表" class="headerlink" title="使用头部和尾部构造列表"></a>使用头部和尾部构造列表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def square([]), do: []</span><br><span class="line">def square([ head | tail ]), do: [ head*head | square(tail) ]</span><br></pre></td></tr></table></figure>

<p>可以看到这样的一个模式,真正工作的是第二个函数.并且会返回一个列表,把实际处理做在头部,并且处理的是head(因为tail可能还是一个列表,而head是一个离散的值),把递归调用写在尾部,以尾部作为参数来递归调用自身</p>
<h3 id="创建映射函数-Map"><a href="#创建映射函数-Map" class="headerlink" title="创建映射函数(Map)"></a>创建映射函数(Map)</h3><p>让我们对上面说的那个过程一般化,就是map函数.map函数是对一个collection的每个元素做运算.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def map([], _func), do: []</span><br><span class="line">def map([ head | tail ], func), do: [ func.(head) | map(tail, func) ]</span><br></pre></td></tr></table></figure>

<p>注意,这个head是每次处理的数据(用func),然后递归tail,tail下次就是一个完整的被做处理的.使用<code>MyList.map [1, 2, 3], fn (n) -&gt; n*n end</code></p>
<h3 id="在递归过程中跟踪值"><a href="#在递归过程中跟踪值" class="headerlink" title="在递归过程中跟踪值"></a>在递归过程中跟踪值</h3><p>将状态传入函数的参数中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">defmodule MyList do</span><br><span class="line">  def sum([], total), do: total</span><br><span class="line">  def sum([ head | tail ], total), do: sum(tail, head+total)</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>不过这样写必须传入一个0值<code>MyList.sum([1, 2, 3], 0)</code>,可以让模块只包含一个只接受一个列表的公开函数,调用私有函数来完成.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">defmodule MyList do</span><br><span class="line">  def sum(list), do: _sum(list, 0)</span><br><span class="line"></span><br><span class="line">  defp _sum([], total), do: total</span><br><span class="line">  defp _sum([ head | tail ], total), do: _sum(tail, head+total)</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h3 id="Reduce模式"><a href="#Reduce模式" class="headerlink" title="Reduce模式"></a>Reduce模式</h3><p>一个通用的函数,把一个collection规约成一个单个值(如求和,求最大,好像聚合操作)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">defmodule MyList do</span><br><span class="line">  def reduce( [], value, _ ) do</span><br><span class="line">    value</span><br><span class="line">  end</span><br><span class="line">  </span><br><span class="line">  def reduce( [head | tail], value, func ) do</span><br><span class="line">    reduce( tail, func.( head, value ), func )</span><br><span class="line">  end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>使用<code>MyList.reduce( [1, 2, 3], 0, &amp;( &amp;1 + &amp;2 ) )</code></p>
<p>求最大值(<strong>这个实现不错</strong>)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">defmodule MyList do</span><br><span class="line">  def max( [ max ] ), do: max</span><br><span class="line">  def max( [ max | [ head | tail ] ] ) when head &gt; max, do: max( [head | tail] )</span><br><span class="line">  def max( [ max | [ head | tail ] ] ) when head &lt; max, do: max( [max | tail] )</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h3 id="更复杂的列表模式"><a href="#更复杂的列表模式" class="headerlink" title="更复杂的列表模式"></a>更复杂的列表模式</h3><p><code>[ 1, 2, 3 | [ 4, 5, 6 ] ]</code>也可以,就是说<code>|</code>左边不一定只有一个元素</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">defmodule Swapper do</span><br><span class="line">  def swap([]), do: []</span><br><span class="line">  def swap([ a, b | tail ]), do: [ b, a | swap(tail) ]</span><br><span class="line">  def swap([_]), do: raise &quot;Can&#x27;t swap a list with an odd number of elements&quot;</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p><code> Swapper.swap [1,2,3,4,5,6]</code>可以调用成功,<code>Swapper.swap [1,2,3,4,5,6,7]</code>会抛出异常,因为第三个函数是处理一个参数时的情况,但是第二个参数每次要取两个值做运算,所以原始列表一定包含偶数个元素(不抛异常的话)</p>
<h3 id="列表中的列表"><a href="#列表中的列表" class="headerlink" title="列表中的列表"></a>列表中的列表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">defmodule WeatherHistory do</span><br><span class="line">  def for_location_27([]), do: []</span><br><span class="line">  def for_location_27([ [time, 27, temp, rain ] | tail]) do</span><br><span class="line">     [ [time, 27, temp, rain] | for_location_27(tail) ]</span><br><span class="line">  end</span><br><span class="line">  def for_location_27([ _ | tail]), do: for_location_27(tail)</span><br><span class="line">  </span><br><span class="line">  def test_data do</span><br><span class="line">  [</span><br><span class="line">    [1366225622, 26, 15, 0.125],</span><br><span class="line">    [1366225622, 27, 15, 0.45],</span><br><span class="line">    [1366225622, 28, 21, 0.25],</span><br><span class="line">    [1366229222, 26, 19, 0.081],</span><br><span class="line">    [1366229222, 27, 17, 0.468],</span><br><span class="line">    [1366229222, 28, 15, 0.60],</span><br><span class="line">    [1366232822, 26, 22, 0.095],</span><br><span class="line">    [1366232822, 27, 21, 0.05],</span><br><span class="line">    [1366232822, 28, 24, 0.03],</span><br><span class="line">    [1366236422, 26, 17, 0.025]</span><br><span class="line">  ]</span><br><span class="line">  end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>使用<code>iex(9)&gt; for_location_27(test_data)</code>,列表的头部必须是4个元素,第二个为27,其实就是<code>[|]</code>,|前面的规定格式,后面的是处理列表中的列表</p>
<p>有个改进的,那个27写死的不好,我们改成传入参数,再进一步的,把头部的写法也可以简略(因为过滤器只关心地区27,其他不关心),<strong>重点</strong>看<code>def for_location([ head = [_, target_loc, _, _ ] | tail], target_loc) do</code>的写法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">defmodule WeatherHistory do</span><br><span class="line">  def for_location([], _target_loc), do: []</span><br><span class="line">  def for_location([ head = [_, target_loc, _, _ ] | tail], target_loc) do</span><br><span class="line">    [ head | for_location(tail, target_loc) ]</span><br><span class="line">  end</span><br><span class="line">  def for_location([ _ | tail], target_loc), do: for_location(tail, target_loc)</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h2 id="复杂数据结构-Maps-Keyword-Lists-Sets-and-Structs"><a href="#复杂数据结构-Maps-Keyword-Lists-Sets-and-Structs" class="headerlink" title="复杂数据结构(Maps, Keyword Lists, Sets, and Structs)"></a>复杂数据结构(Maps, Keyword Lists, Sets, and Structs)</h2><ul>
<li>散列表:map,<code>%&#123; key =&gt; value, key =&gt; value &#125;%</code>,模式匹配用它</li>
<li>散列字典:HashDict,<code>&lt;[ fg: &quot;black&quot;, bg: &quot;white&quot;, font: &quot;Merriweather&quot; ]&gt;</code>,数量多用它</li>
<li>关键字列表:keyword, <code>[ fg: &quot;black&quot;, bg: &quot;white&quot;, font: &quot;Merriweather&quot; ]</code>,保证有序,只有他允许一对多(k-v)<br>这些api的文档需要花时间了解.</li>
</ul>
<h3 id="模式匹配和更新散列表"><a href="#模式匹配和更新散列表" class="headerlink" title="模式匹配和更新散列表"></a>模式匹配和更新散列表</h3><p>经常的用法:<code>person = %&#123; name: &quot;Dave&quot;, height: 1.88 &#125;</code></p>
<ol>
<li><p>是否有一个entry的key是xxx?</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; %&#123; name: a_name &#125; = person</span><br><span class="line">%&#123;height: 1.88, name: &quot;Dave&quot;&#125;</span><br><span class="line">iex&gt; a_name</span><br><span class="line">&quot;Dave&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>有key为xxx的entry吗?(有没有同时有这两个key)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; %&#123; name: _, height: _ &#125; = person</span><br><span class="line">%&#123;height: 1.88, name: &quot;Dave&quot;&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>有keyw为xx的entry的值为xxx吗?</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; %&#123; name: &quot;Dave&quot; &#125; = person</span><br><span class="line">%&#123;height: 1.88, name: &quot;Dave&quot;&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>注意</strong>,<code>%&#123; name: a_name &#125; = person</code>这个destructured散列表的方法.这种方法很常用.</p>
<h3 id="番外-类型是什么"><a href="#番外-类型是什么" class="headerlink" title="番外:类型是什么"></a>番外:类型是什么</h3><p>keyword类型是elixir的模块,却被实现成了元组列表.显然它还是一个列表,其次elixir添加了字典的行为,从某种意义上来说,这是类似鸭子类型.keyword模块没有底层原生的数据类型,它只是假定所处理的数据都是按特定方式组织的列表(schema?依赖抽象,高层指定规范底层遵守?)</p>
<h2 id="处理集合-Enum与Stream"><a href="#处理集合-Enum与Stream" class="headerlink" title="处理集合-Enum与Stream"></a>处理集合-Enum与Stream</h2><p>Enum类似批处理,Stream是流处理.Enum是贪婪的,Stream是延迟处理的<br>从技术上讲,可被遍历的类型都被为实现了Enumerable.Enum用于迭代,过滤,组合,分割等.<br>断言操作:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; Enum.all?(list, &amp;(&amp;1 &lt; 4))</span><br><span class="line">false</span><br><span class="line">iex&gt; Enum.any?(list, &amp;(&amp;1 &lt; 4))</span><br><span class="line">true</span><br><span class="line">iex&gt; Enum.member?(list, 4)</span><br><span class="line">true</span><br><span class="line">iex&gt; Enum.empty?(list)</span><br><span class="line">false</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[ 1, 2, 3, 4, 5 ]</span><br><span class="line">|&gt; Enum.map(&amp;(&amp;1*&amp;1))</span><br><span class="line">|&gt; Enum.with_index</span><br><span class="line">|&gt; Enum.map(fn &#123;value, index&#125; -&gt; value - index end)</span><br><span class="line">|&gt; IO.inspect #=&gt; [1,3,7,13,21]</span><br></pre></td></tr></table></figure>

<h1 id="Actor综述"><a href="#Actor综述" class="headerlink" title="Actor综述"></a>Actor综述</h1><p>更加面向对象! 你所有的对象,最后都想成长为一个actor.</p>
<h1 id="队列式信箱"><a href="#队列式信箱" class="headerlink" title="队列式信箱"></a>队列式信箱</h1><p>异步地发送消息是用actor模型编程的重要特性之一。消息并不是直接发送到一个actor，而是发送到一个<strong>信箱(mailbox)</strong></p>
<p>信箱可以存放许多消息(Buffered Channels?)</p>
<p>这样的设计解耦了actor之间的关系——actor都以自己的步调运行，且发送消息时不会被阻塞。</p>
<p>虽然所有actor可以同时运行，但它们都按照信箱接收消息的顺序来依次处理消息，且仅在当前消息处理完成后才会处理下一个消息，因此我们只需要关心发送消息时的并发问题即可</p>
<p>通常actor会进行无限循环，通过receive等待接收消息，并进行消息处理。为了彻底关闭一个actor，需要满足两个条件。第一个是需要告诉actor在完成消息处理后就关闭；第二个是需要知道actor何时完成关闭。</p>
<p>actor是异步发送消息的——发送者并不会被阻塞。</p>
<h1 id="管理进程"><a href="#管理进程" class="headerlink" title="管理进程"></a>管理进程</h1><p>任何时候都可以用Process.link()在两个进程之间建立连接,这样可以检测到某一个进程的终止.连接是双向的。建立了从pid1到pid2的连接的同时，也就建立了从pid2到pid1的连接——所以如果其中一个进程终止，那么两个进程就都终止了.进程正常终止(:normal)是不会让连接的另一个进程终止的.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">defmodule LinkTest do</span><br><span class="line">     def loop do</span><br><span class="line">         receive do</span><br><span class="line">             &#123;:exit_because, reason&#125; -&gt; exit(reason)</span><br><span class="line">             &#123;:link_to, pid&#125; -&gt; Process.link(pid)</span><br><span class="line">             &#123;:EXIT, pid, reason&#125; -&gt; IO.puts(&quot;#&#123;inspect(pid)&#125; exited because #&#123;reason&#125;&quot;)</span><br><span class="line">         end</span><br><span class="line">         loop()</span><br><span class="line">     end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h2 id="系统进程"><a href="#系统进程" class="headerlink" title="系统进程"></a>系统进程</h2><p>通过设置进程的:trap_exit标识，可以让一个进程捕获另一个进程的终止消息。用专业术语来说，这是将进程转化为系统进程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def loop_system do</span><br><span class="line">  Process.flag(:trap_exit, true)</span><br><span class="line">  loop</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>实现一个进程管理者CacheSupervisor（也就是一个系统进程），它管理着若干个工作进程，当工作进程崩溃时进行干预。</p>
<p>CacheSupervisor负责创建Cache实例,如果缓存崩溃,会自动重启(虽然会丢失之前的数据),但至少得到了一个崩溃后可以继续使用的缓存</p>
<p>after是为receive增加超时机制,比如的活可能会有死锁</p>
<ol>
<li><p>进程1向缓存发送:put消息；</p>
</li>
<li><p>进程2向缓存发送:get消息；</p>
</li>
<li><p>缓存在处理进程1的消息时崩溃了；</p>
</li>
<li><p>管理者将缓存重启，但进程2的消息丢失了；</p>
</li>
<li><p>进程2在receive处陷入死锁，一直在等待消息的回复，但这个回复永远不会发送。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">defmodule Cache do</span><br><span class="line">  def start_link do</span><br><span class="line">    pid = spawn_link(__MODULE__, :loop, [HashDict.new, 0])</span><br><span class="line">    Process.register(pid, :cache)</span><br><span class="line">    pid</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def put(url, page) do</span><br><span class="line">    send(:cache, &#123;:put, url, page&#125;)</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def get(url) do</span><br><span class="line">    ref = make_ref()</span><br><span class="line">    send(:cache, &#123;:get, self(), ref, url&#125;)</span><br><span class="line">    receive do</span><br><span class="line">      &#123;:ok, ^ref, page&#125; -&gt; page</span><br><span class="line">      after 1000 -&gt; nil</span><br><span class="line">    end</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def size do</span><br><span class="line">    ref = make_ref()</span><br><span class="line">    send(:cache, &#123;:size, self(), ref&#125;)</span><br><span class="line">    receive do</span><br><span class="line">      &#123;:ok, ^ref, s&#125; -&gt; s</span><br><span class="line">      after 1000 -&gt; nil</span><br><span class="line">    end</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def terminate do</span><br><span class="line">    send(:cache, &#123;:terminate&#125;)</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def loop(pages, size) do</span><br><span class="line">    receive do</span><br><span class="line">      &#123;:put, url, page&#125; -&gt;</span><br><span class="line">        new_pages = Dict.put(pages, url, page)</span><br><span class="line">        new_size = size + byte_size(page)</span><br><span class="line">        loop(new_pages, new_size)</span><br><span class="line"></span><br><span class="line">      &#123;:get, sender, ref, url&#125; -&gt;</span><br><span class="line">        send(sender, &#123;:ok, ref, pages[url]&#125;)</span><br><span class="line">        loop(pages, size)</span><br><span class="line"></span><br><span class="line">      &#123;:size, sender, ref&#125; -&gt;</span><br><span class="line">        send(sender, &#123;:ok, ref, size&#125;)</span><br><span class="line">        loop(pages, size)</span><br><span class="line"></span><br><span class="line">      &#123;:terminate&#125; -&gt; # Terminate request - don&#x27;t recurse</span><br><span class="line">    end</span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">defmodule CacheSupervisor do</span><br><span class="line">    def start do</span><br><span class="line">        spawn(__MODULE__, :loop_system, [])</span><br><span class="line">    end</span><br><span class="line">    def loop do</span><br><span class="line">        pid = Cache.start_link</span><br><span class="line">         receive do</span><br><span class="line">             &#123;:EXIT, ^pid, :normal&#125; -&gt; </span><br><span class="line">                 IO.puts(&quot;Cache exited normally&quot;)</span><br><span class="line">                 :ok</span><br><span class="line">             &#123;:EXIT, ^pid, reason&#125; -&gt; </span><br><span class="line">                 IO.puts(&quot;#&#123;inspect(pid)&#125; exited because #&#123;reason&#125; - restarting it&quot;)</span><br><span class="line">            loop()</span><br><span class="line">        end</span><br><span class="line">     end</span><br><span class="line">     def loop_system do</span><br><span class="line">         Process.flag(:trap_exit, true)</span><br><span class="line">         loop()</span><br><span class="line">     end</span><br><span class="line">end</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="错误处理内核模式-error-kernel"><a href="#错误处理内核模式-error-kernel" class="headerlink" title="错误处理内核模式(error-kernel)"></a>错误处理内核模式(error-kernel)</h1><blockquote>
<p>软件设计有两种方式：一种方式是，使软件过于简单，明显地没有缺陷；另一种方式是，使软件过于复杂，没有明显的缺陷。</p>
</blockquote>
<p>Elixir有两个规则：</p>
<ul>
<li>如果没有异常发生，消息一定能被送达并被处理；</li>
<li>如果某个环节出现异常，异常一定会通知到使用者（假设使用者已经连接到或正在管理发生异常的进程）<br>第二条规则是Elixir提供容错性的基石。</li>
</ul>
<p>一个软件系统如果应用了错误处理内核模式，那么该系统正确运行的前提是其错误处理内核必须正确运行。成熟的程序通常使用尽可能小而简单的错误处理内核——小而简单到明显地没有缺陷。</p>
<p>对于一个使用actor模型的程序，其错误处理内核是顶层的管理者，管理着子进程——对子进程进行启动、停止、重启等操作。</p>
<p>程序的每个模块都有自己的错误处理内核——模块正确运行的前提是其错误处理内核必须正确运行。子模块也会有自己的错误处理内核，以此类推。这就构成了错误处理内核的层级树，较危险的操作都会被下放给底层的actor执行(一棵树,高层的都是错误处理内核,是管理者,风险最低;子树是工作进程风险高)</p>
<h2 id="任其崩溃"><a href="#任其崩溃" class="headerlink" title="任其崩溃"></a>任其崩溃</h2><p>使用actor模型的程序并不进行防御式编程，而是遵循“任其崩溃”的哲学，<strong>让actor的管理者来处理这些问题</strong>。这样做有几个好处，比如：</p>
<ol>
<li>代码会变得更加简洁且容易理解，可以清晰区分出“一帆风顺”的代码和容错代码；</li>
<li>多个actor之间是相互独立的，并不共享状态，因此一个actor的崩溃不太会殃及到其他actor。尤其重要的是一个actor的崩溃不会影响到其管理者，这样管理者才能正确处理此次崩溃；</li>
<li>管理者也可以选择不处理崩溃，而是记录崩溃的原因，这样我们就会得到崩溃通知并进行后续处理。</li>
</ol>
<h2 id="小总结"><a href="#小总结" class="headerlink" title="小总结"></a>小总结</h2><p>Elixir通过创建管理者并使用进程的连接来进行容错：</p>
<ol>
<li>连接是双向的——如果进程a连接到进程b，那么进程b也连接到进程a；</li>
<li>连接可以传递错误——如果两个进程已经连接，其中一个进程异常终止，那么另一个进程也会异常终止；</li>
<li>如果进程被转化成系统进程，当其连接的进程异常终止时，系统进程不会终止，而是会收到:EXIT消息。</li>
</ol>
<h1 id="OTP"><a href="#OTP" class="headerlink" title="OTP"></a>OTP</h1><p>这段代码中Cache声明自己实现了一个行为（GenServer.Behaviour）和两个函数（handle_cast()和handle_call()）。这里所说的“行为”非常类似于Java中的接口——其定义了一个函数集。模块使用use来声明自己实现了行为.</p>
<p><code>handle_cast()</code>可以处理消息但并不回复消息。其接受两个参数：收到的消息、actor的当前状态。返回值是一个二元组<code>&#123;:noreply, new_state&#125;</code>。本例中实现了一个<code>handle_cast()</code>来处理:put消息。</p>
<p><code>handle_call()</code>可以处理消息且回复消息。其接受三个参数：收到的消息、发送者标识、actor的当前状态。返回值是一个三元组<code>&#123;:reply, reply_value, new_state&#125;</code>。本例中实现了两个<code>handle_call()</code>，一个负责处理:get消息，另一个负责处理:size消息。类似于Clojure，Elixir用下划线<code>（_）</code>开头的变量名来表示该变量不被使用——比如<code>_from</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">defmodule Cache do</span><br><span class="line">  use GenServer.Behaviour</span><br><span class="line">  #####</span><br><span class="line">  # External API</span><br><span class="line"></span><br><span class="line">  def start_link do</span><br><span class="line">    :gen_server.start_link(&#123;:local, :cache&#125;, __MODULE__, &#123;HashDict.new, 0&#125;, [])</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def put(url, page) do</span><br><span class="line">    :gen_server.cast(:cache, &#123;:put, url, page&#125;)</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def get(url) do</span><br><span class="line">    :gen_server.call(:cache, &#123;:get, url&#125;)</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def size do</span><br><span class="line">    :gen_server.call(:cache, &#123;:size&#125;)</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  #####</span><br><span class="line">  # GenServer implementation</span><br><span class="line"></span><br><span class="line">  def handle_cast(&#123;:put, url, page&#125;, &#123;pages, size&#125;) do</span><br><span class="line">    new_pages = Dict.put(pages, url, page)</span><br><span class="line">    new_size = size + byte_size(page)</span><br><span class="line">    &#123;:noreply, &#123;new_pages, new_size&#125;&#125;</span><br><span class="line">  end</span><br><span class="line">  def handle_call(&#123;:get, url&#125;, _from, &#123;pages, size&#125;) do</span><br><span class="line">    &#123;:reply, pages[url], &#123;pages, size&#125;&#125;</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def handle_call(&#123;:size&#125;, _from, &#123;pages, size&#125;) do</span><br><span class="line">    &#123;:reply, size, &#123;pages, size&#125;&#125;</span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">defmodule CacheSupervisor do</span><br><span class="line">  use Supervisor.Behaviour</span><br><span class="line"></span><br><span class="line">  def start_link do</span><br><span class="line">    :supervisor.start_link(__MODULE__, []) </span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def init(_args) do</span><br><span class="line">    workers = [worker(Cache, [])]</span><br><span class="line">    supervise(workers, strategy: :one_for_one)</span><br><span class="line">  end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h2 id="重启策略"><a href="#重启策略" class="headerlink" title="重启策略"></a>重启策略</h2><p>OTP管理者行为支持多种不同的重启策略，最常用的是one-for-all和one-for-one。</p>
<p>如果一个工作进程崩溃，使用one-for-all策略的管理者将重启所有工作进程（包括那些没有崩溃的工作进程）。使用one-for-one策略的管理者仅重启已经崩溃的工作进程。</p>
<p>用OTP实现的服务器和管理者有着更多的功能，其中包括以下几点。</p>
<ol>
<li>更好的重启逻辑： 之前我们自己实现的简单管理者使用非常草率的重启策略——如果工作线程崩溃，就将其重启。如果工作线程在启动时很快就崩溃，那么管理者会一直重启工作线程。而OTP提供的管理者可以设定最大重启频率，如果重启超过这个频率，管理者将会异常终止。</li>
<li>调试与日志：通过调整OTP服务器的参数，可以开启调试和日志功能，这对开发很重要。</li>
<li>代码热升级：OTP服务器不需要停止整个系统就可以进行升级。</li>
<li>还有许多：发布管理、故障切换、自动扩容，等等。</li>
</ol>
<h2 id="复习"><a href="#复习" class="headerlink" title="复习"></a>复习</h2><blockquote>
<p>很久以前，我在描述“面向对象编程”时使用了“对象”这个概念。很抱歉这个概念让许多人误入歧途，他们将学习的重心放在了“对象”这个次要的方面。<br>真正主要的方面是“消息”……日文中有一个词ma，表示“间隔”，与其最为相近的英文或许是“ interstitial”。创建一个规模宏大且可生长的系统的关键在于其模块之间应该如何交流，而不在于其内部的属性和行为应该如何表现。</p>
</blockquote>
<p>actor模型精心设计了消息传输和封装的机制，强调了面向对象的精髓，可以说actor模型非常“面向对象”。actor模型的重点在于参与交流的实体，而CSP模型的重点在于用于交流的通道。</p>
<h1 id="运用多线程"><a href="#运用多线程" class="headerlink" title="运用多线程"></a>运用多线程</h1><p><strong>所有你的对象最后都想成长成actor</strong></p>
<p>elixir的一个特点就是将代码打包成可独立和并发运行的小块,它使用actor并发模型,actor是一个无依赖的进程,他不于其他进程共享任何东西.你可以spawn新进程,send发送消息,用receive接收消息,仅此而已.elixir里创建进程就和java中创建对象一样自然.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">defmodule SpawnBasic do</span><br><span class="line">  def greet do</span><br><span class="line">    IO.puts &quot;Hello&quot;</span><br><span class="line">  end</span><br><span class="line">en</span><br></pre></td></tr></table></figure>

<p>在独立的进程中运行,<code>:greet</code>是函数名,<code>[]</code>是参数列表;返回一个pid(整个是世界中唯一),先返回hello还是先返回PID是不确定的,要用消息同步进程的活动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iex&gt; spawn(SpawnBasic, :greet, [])</span><br><span class="line">Hello</span><br><span class="line">#PID&lt;0.42.0&gt;</span><br></pre></td></tr></table></figure>

<h3 id="在进程间发送消息"><a href="#在进程间发送消息" class="headerlink" title="在进程间发送消息"></a>在进程间发送消息</h3><p><code>send</code>接受一个PID和右边发送的消息(也叫term),一般都是原子和元组;等待消息用<code>receive</code>,用法像case,消息体作为参数,可以指定模式.</p>
<p>下面的会输出Hello, world,<code>&#123; sender, msg &#125;</code>是receive接受的参数,sender发过去的时候要带着.当发送第二个消息,不过greet函数仅能处理单条消息,处理完receive(Spaen1)就退出了.所以下面模式匹配的receive救护挂起,用after做一个超时机制</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">defmodule Spawn1 do</span><br><span class="line">  def greet do</span><br><span class="line">    receive do</span><br><span class="line">      &#123; sender, msg &#125; -&gt;</span><br><span class="line">        send sender, &#123; :ok, &quot;Hello, #&#123;msg&#125; &quot;&#125;</span><br><span class="line">    end</span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"># 客户端代码</span><br><span class="line">pid = spawn( Spawn1, :greet, [] )</span><br><span class="line">send pid, &#123; self(), &quot;world&quot; &#125; # send self(), &quot;world&quot;,&#123; self, &quot;world&quot; &#125;对应上面receive的&#123; sender, msg &#125;</span><br><span class="line"></span><br><span class="line"># 一个模式匹配,提取出message</span><br><span class="line">receive do</span><br><span class="line">  &#123; :ok, message &#125; -&gt;</span><br><span class="line">    IO.puts message</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"># 发送第二个消息,不过greet函数仅能处理单条消息,处理完receive(Spaen1)就退出了.所以下面模式匹配的receive救护挂起,用after做一个超时机制</span><br><span class="line">send pid, &#123; self(), &quot;Aron&quot; &#125;</span><br><span class="line"></span><br><span class="line">receive do</span><br><span class="line">  &#123; :ok, message &#125; -&gt;</span><br><span class="line">    IO.puts message</span><br><span class="line">  after 500 -&gt;</span><br><span class="line">    IO.puts &quot;The greeter has gone away&quot;</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>让greet函数处理多条消息.函数体就这么写,一个递归</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def greet do</span><br><span class="line">  receive do</span><br><span class="line">    &#123;sender, msg&#125; -&gt;</span><br><span class="line">      send sender, &#123; :ok, &quot;Hello, #&#123;msg&#125;&quot; &#125;</span><br><span class="line">      greet</span><br><span class="line">  end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h3 id="循环递归和栈"><a href="#循环递归和栈" class="headerlink" title="循环递归和栈"></a>循环递归和栈</h3><p><strong>尾递归优化:</strong> 函数的最后一件事是调用自己,就没有必要调用,只需要简单的跳到函数开始的地方,如果函数调用有参数,就把原始参数替换掉.,但是递归调用必须是最后一个执行的,下面的方式就不行.因为他还要将之后的结果*n.注释下面的是可以进行尾递归优化的(把乘法移到了递归里面),要增加一个累加器.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def factorial(0), do: 1</span><br><span class="line">def factorial(n), do: n * factorial(n-1)</span><br><span class="line">#############################</span><br><span class="line">def factorial(n), do: _fact(n, 1)</span><br><span class="line">defp _fact(0, acc), do: acc</span><br><span class="line">defp _fact(n, acc), do: _fact(n-1, acc*n)</span><br></pre></td></tr></table></figure>

<h3 id="进程开销"><a href="#进程开销" class="headerlink" title="进程开销"></a>进程开销</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">defmodule Chain do</span><br><span class="line">  def counter( next_pid ) do</span><br><span class="line">    receive do</span><br><span class="line">     n -&gt;</span><br><span class="line">       send next_pid, n+1</span><br><span class="line">     end</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def create_processes( n ) do</span><br><span class="line">    last = Enum.reduce 1..n, self,</span><br><span class="line">                  fn ( _, send_to ) -&gt;</span><br><span class="line">                    spawn(Chain, :counter, [ send_to ])</span><br><span class="line">                  end</span><br><span class="line"></span><br><span class="line">    send last, 0</span><br><span class="line"></span><br><span class="line">    recevie do</span><br><span class="line">      final_answer when is_integer( final_answer ) -&gt;</span><br><span class="line">        &quot;Result is #&#123; inspect( final_answer ) &#125;&quot;</span><br><span class="line">    end</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  def run( n ) do</span><br><span class="line">    IO.puts inspect :timer.tc(Chain, :create_processes, [n])</span><br><span class="line">  end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/07/19/Spark-Catalyst-Deep-Dive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/07/19/Spark-Catalyst-Deep-Dive/" class="post-title-link" itemprop="url">Spark Catalyst Deep Dive</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-07-19 18:15:40" itemprop="dateCreated datePublished" datetime="2018-07-19T18:15:40+08:00">2018-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:08:05" itemprop="dateModified" datetime="2021-05-08T11:08:05+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Spark-Catalyst-的实现分析"><a href="#Spark-Catalyst-的实现分析" class="headerlink" title="Spark Catalyst 的实现分析"></a>Spark Catalyst 的实现分析</h1><p><strong>转载自:</strong><a target="_blank" rel="noopener" href="https://github.com/ColZer/DigAndBuried/blob/master/spark/spark-catalyst.md#spark-catalyst%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90">Spark Catalyst 的实现分析</a></p>
<p>Spark SQL 和 Catalyst 分别对应了 SQL 执行期以及解析期的优化工作，因此 Catalyst 的理解是 Spark SQL 的第一步。在一些 Catalyst 的介绍以及讲座中，下面一张图是必出现，它描述了从 SQL 语句到最后执行 Plan 的生成过程中，除了 Spark SQL，其他 SQL 引擎的工作原理也基本一致，比如 Hive 之类的。</p>
<p><img src="/2018/07/19/Spark-Catalyst-Deep-Dive/Catalyst-Optimizer-diagram.png"></p>
<p>本文核心也是介绍 Catalyst 内部的实现，但是不是按照这张图的步骤来介绍 Catalyst 的实现原理，而是按照 SQL 给人最直接几个概念，比如 Row，Expression，Plan 来逐步介绍它们的内部实现。</p>
<p>看过 Spark SQL 或者 Catalyst 人都知道，相比 Spark Core 的代码，这一块真的挺复杂了，各种算法逻辑，复杂的 Scala 语法元素，以及各种性能优化，代码自动生成，可能得需要几倍的时间，反复的琢磨，才能梳理清楚。</p>
<h2 id="1-Row"><a href="#1-Row" class="headerlink" title="1. Row"></a>1. Row</h2><p>Spark SQL 中处理的数据与传统 RDD 最大区别在处理的每行数据的类型表示，传统 RDD 不对每行数据进行类型要求，可以任何复杂数据结构，比如 Map, 或者自己自定义的类之类的，而 Spark SQL 中为<code>Row</code>。</p>
<p><code>Row</code>的概念是针对之前版本中的 DataFrame 而言的，在 1.6 版本中提出的 Dataset 其实也是有<code>Row</code>的概念，只是会被隐式转换掉而已，在 Catalyst 中，处理的对象为<code>InternalRow</code>，注意<code>InternalRow</code>和<code>Row</code>是有本质区别的，<code>Row</code>是 API 层面的概念，专门针对 DataFrame，而<code>InternalRow</code>为内部概念。</p>
<p><code>Row</code>类为 DataFrame 中核心概念，它表示一行结构化数据的输入，<code>Row</code>本身可以有 Scheme，即支持设置每个字段的类型，支持 GetInt 之类的操作；它也可以无 Scheme，即<code>Scheme=NULL</code>，支持泛化的 Get 操作，返回 Any 类型数据。<code>Row</code>本身就有点像<code>Array，Tuple</code>,它的子类<code>GenericRow</code>和<code>GenericRowWithSchema</code>都是基于<code>Array[Any]</code>来实现，它的每个字段的类型为<code>Any</code>，因此<code>Row</code>可以理解为是无类型约束的，虽然它内部有<code>Scheme</code>，但是<code>Array[Any]</code>到<code>Row</code>的转换，以及泛化的 Get 操作都是不受 Scheme 的限制，因此它是类型不安全的。</p>
<p>在 Spark 1.6 版本以后，提出了 Dataset 概念，它也是 Spark 从 RDD，SchemeRDD，DataFrame 演化过来最为标准化一个概念，相信在未来会是 Spark 中最为核心概念，<code>In the long run, we expect Datasets to become a powerful way to write more efficient Spark applications.</code></p>
<p>Dataset 是 Spark SQL 中概念，在 API 层面上来说，它没有像 DataFrame 一样，强制要求输入数据为<code>Row</code>，或者依赖<code>Row.fromSeq</code>等功能函数将输入其他类型的数据强制转换为 Row，但是 Dataset 实际也是要求它处理的数据是<code>Row</code> 相似的类型，即为 Catalyst 内部的**<code>InternalRow</code><strong>和</strong><code>UnsafeRow</code>**的概念。当我们执行<code>seq(1,2,3).toDs 或者 Rdd.toDs</code>时候，实际上对每一行记录在内部做了很多转换。DataFrame 也被转换为 Dataset 进行包装，即将 DataFame 表示为<code>Dataset[Row]</code>。</p>
<p>Dataset 核心概念是为<code>Encoder</code>，它就是我们上面说隐式转换的幕后工具，如下所示 Encoder 内部的 toRow 函数，支持将输入 T 类型转换为<code>InternalRow</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def toRow(t: T): InternalRow = try &#123;</span><br><span class="line">    inputRow(0) = t</span><br><span class="line">    extractProjection(inputRow)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>它支持将一个外部的数据类型，转换为 Dataset 内部的<code>InternalRow</code>，比如上面的<code>seq(1,2,3)</code>就是需要将每个输入的 Int 数字转换为一个<code>InternalRow</code>，<code>DataFrame</code>同理需要将<code>Row</code>转换为<code>InternalRow</code>。在转换的过程中，是<strong>有类型的检查</strong>，这也是与 DataFrame 最大的区别。</p>
<p>另外 Dataset 对外最为自豪一点就是序列化以及反序列化的性能，用官方话说:<code>Datasets also leverage Tungsten’s fast in-memory encoding，an order of magnitude faster than the default Java serializer</code>，那么它是这么实现呢？其实他就是上面谈到的<code>Encoder</code>的功能，上面虽然我们说它返回的是一个<code>InternalRow</code>，实际上，<strong>它返回的是<code>InternalRow</code>一个子类，即<code>UnsafeRow</code></strong></p>
<p><code>UnsafeRow</code>内部是基于 Java 的 unsafe 包来实现（Tungsten 的功能），对 Row 中每个字段的操作都转换为字节的操作，换句话说它底层实际存储结构是 byte[]，而且支持 Kryo 序列化，相比使用 Java 序列化工具来序列化数组/Row 之类的复杂数据结构，它的性能肯定要好很多！</p>
<p>另外<code>InternalRow</code>还有一个子类，即<code>MutableRow</code>，而且<code>UnsafeRow</code>也是<code>MutableRow</code>的子类，它即为可修改的<code>InternalRow</code>，在很多地方都会出现这个，原理很简单，支持 set 等操作而已。</p>
<h2 id="2-Expression"><a href="#2-Expression" class="headerlink" title="2. Expression"></a>2. Expression</h2><p>在 SQL 语句中，除了 SELECT FROM 等关键字以外，其他大部分元素都可以理解为 Expression，比如<code>SELECT sum(a), a</code>，其中<code>sum(a)</code>和<code>a</code>都为 Expression；</p>
<p>从 SQL 语句功能来说，Expression 的功能可以划分为 Projection，Predicate，Ordering，Aggregate；</p>
<p>其中 Projection 功能就是 input 数据进行加工，输出为 InternalRow；Predicate 而是对输入的 InternalRow 进行加工输出为 Bool，Ordering 和 Aggregate 则是针对 Sortby/Groupby 专用的两类表达式；你可能会说，那么 substr 之类的函数表达式不在上面归类？substr 在 sql 语句中肯定不会单独存在，他们都是作为子表达式出现在上面的几类当中，比如用于判断或者输出数据的加工。</p>
<ul>
<li><p>Expression 是一个 Tree 结构，即可以通过多级的 Child Expression 来组合成复杂的 Expression，比如前面<code>sum(a)</code>就是由<code>sum</code> 和<code>a</code> 两个简单的 Expression 组合而成，比如更复杂的 Expression 有<code>max(sum(a), sum(b))</code>;</p>
</li>
<li><p>Expression 基本的功能是求值，比如<code>abs(a)</code>，<code>IfNull(A,B)</code>之类的操作，他们都是对输入 Row 进行加工，并输出处理结果，即 Expression 需要实现<code>def eval(input: InternalRow = null): Any</code>函数来实现它的功能。</p>
</li>
<li><p>既然 Expression 的功能是求值，那么它就有输入和输出类型的限制。每个 Expression 都有<code>def dataType: DataType</code>类型变量来表示它的输出类型，以及<code>def checkInputDataTypes(): TypeCheckResult</code>函数来校验当前 Expression 的输入（为 Tree 结构，那么它的输入即为 Child Expression 输出）是否符合类型要求。</p>
</li>
<li><p>Expression 功能是针对 Row 进行加工，但是可以把加工方法分为以下几种</p>
<ul>
<li>原生的<code>def eval(input: InternalRow = null): Any</code>函数；</li>
<li><strong>对于 Non Child Expression，Expression 的计算是基于 Child Expression 计算结果进行二次加工的，因此对于 Non Child Expression，对 Eval 进行默认实现，子类只需要实现函数<code>def nullSafeEval(input: Any): Any</code>即可以，如下所示为只有一个 Child 的 Expression 的实现：</strong></li>
</ul>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">override def eval(input: InternalRow): Any = &#123;</span><br><span class="line">    val value = child.eval(input)</span><br><span class="line">    if (value == null) &#123;</span><br><span class="line">        null</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        nullSafeEval(value)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">//比如 ExpressionAbs，就是利用子 Expression 的结果结果之上，进行一个 math.abs 加工。</span><br></pre></td></tr></table></figure>

<ul>
<li><code>Projection</code>类型，它本身不是 Expression，但是它可以根据 N 个 Expression，对输入 row 的 N 个字段分别进行加工，输出一个新的 Row，即 Expression 的容器。</li>
</ul>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">abstract class Projection extends (InternalRow =&gt; InternalRow)</span><br><span class="line">def apply(input: InternalRow): InternalRow = &#123;</span><br><span class="line">    val outputArray = new Array(exprArray.length)</span><br><span class="line">    var i = 0</span><br><span class="line">    while (i &lt; exprArray.length) &#123;</span><br><span class="line">      outputArray(i) = exprArray(i).eval(input)</span><br><span class="line">      i += 1</span><br><span class="line">    &#125;</span><br><span class="line">    new GenericInternalRow(outputArray)</span><br><span class="line">  &#125;</span><br><span class="line">//比如 row 序列化操作，可以把一个 row 序列化为 unsaferow，相当于一个 Projection</span><br></pre></td></tr></table></figure>

<ul>
<li>Expression 也可能是不支持 eval 的，即<code>Unevaluable</code>类型的 Expression，一般有三种情况：1) 是真的无法求值，比如处于 Unresolved 状态的 Expression；2) 是不支持通过 eval 进行求值，而需要通过 gen code 的方式来实现 Expression 功能; 3) Expression 为<code>RuntimeReplaceable</code>类型，它仅仅是在 parser 阶段一种临时 Expression，在优化阶段，会被替换为别的 Expression，因此它本身不需要有执行逻辑，但是得有替换相关的逻辑。</li>
</ul>
</li>
</ul>
<h3 id="Expression-大体归类"><a href="#Expression-大体归类" class="headerlink" title="Expression 大体归类"></a>Expression 大体归类</h3><table>
<thead>
<tr>
<th>Name</th>
<th>归类</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>Expression</td>
<td></td>
<td>Expression为Tree结构，中间节点都为加工类型表单，而叶子节点即为数据产生节点</td>
</tr>
<tr>
<td>Attribute</td>
<td></td>
<td>Catalyst里面最为重要的概念，可以理解为表的属性，在sql处理各个阶段会有不同的形态，比如UnresolvedAttribute-&gt;AttributeReference-&gt;BoundReference，后面会具体分析</td>
</tr>
<tr>
<td>Literal</td>
<td></td>
<td>常量，支持各种类型的常量输入</td>
</tr>
<tr>
<td>datetimeExpressions</td>
<td></td>
<td>返回当前时间类型的常量，<code>CurrentDate</code>,<code>CurrentTimestamp</code></td>
</tr>
<tr>
<td>randomExpressions</td>
<td></td>
<td>支持生成一些随机数</td>
</tr>
<tr>
<td>其他一些输入</td>
<td></td>
<td>比如获取sql计算过程中的任务对应的InputFileName，SparkPartitionID</td>
</tr>
<tr>
<td>基本计算功能：</td>
<td></td>
<td></td>
</tr>
<tr>
<td>arithmetic</td>
<td>nullSafeEval</td>
<td>数学Expression，支持<code>-</code>,<code>+</code>,<code>abs</code>, <code>+</code>,<code>-</code>,<code>*</code>,<code>/</code>,<code>%</code>,<code>max</code>,<code>min</code>,<code>pmod</code>数学运算符</td>
</tr>
<tr>
<td>bitwiseExpressions</td>
<td>nullSafeEval</td>
<td>位运算数，支持IntegralType类型的<code>and</code>,<code>or</code>,<code>not</code>,<code>xor</code>位运算</td>
</tr>
<tr>
<td>mathExpressions</td>
<td>nullSafeEval</td>
<td>数学函数，支持<code>cos</code>,<code>Sqrt</code>之类30多种,相当于Math包</td>
</tr>
<tr>
<td>stringExpressions</td>
<td>nullSafeEval</td>
<td>字符串函数，支持<code>Substring</code>,<code>Length</code>之类30多种，相当于String包</td>
</tr>
<tr>
<td>decimalExpressions</td>
<td>nullSafeEval</td>
<td>Decimal类型的支持，支持<code>Unscaled</code>,<code>MakeDecimal</code>操作</td>
</tr>
<tr>
<td>datetimeExpressions</td>
<td>nullSafeEval</td>
<td>时间类型的运算</td>
</tr>
<tr>
<td>collectionOperations</td>
<td>nullSafeEval</td>
<td>容器的操作，支持容器<code>Contains</code>,<code>Sort</code>,<code>Size</code>三种操作</td>
</tr>
<tr>
<td>cast</td>
<td>nullSafeEval</td>
<td>支持数据类型的转换</td>
</tr>
<tr>
<td>misc</td>
<td>nullSafeEval</td>
<td>功能函数包，支持MD5，crc32之类的函数功能</td>
</tr>
<tr>
<td>基本逻辑计算功能：</td>
<td></td>
<td></td>
</tr>
<tr>
<td>predicates</td>
<td>eval/nullSafeEval类型</td>
<td>支持子Expression之间的逻辑运算，比如<code>AND</code>,<code>In</code>,<code>Or</code>，输出Bool类型</td>
</tr>
<tr>
<td>regexpExpressions</td>
<td>nullSafeEval</td>
<td>支持LIKE相关操作</td>
</tr>
<tr>
<td>conditionalExpressions</td>
<td>eval</td>
<td>支持case，if，great，least四种逻辑判断运算</td>
</tr>
<tr>
<td>nullExpressions</td>
<td>eval/RuntimeReplaceable</td>
<td>与NULL/NA相关的判断或者IF判断功能，大部分都为RuntimeReplaceable，会被进行优化处理</td>
</tr>
<tr>
<td>其他类型：</td>
<td></td>
<td></td>
</tr>
<tr>
<td>complexTypeCreator</td>
<td>eval</td>
<td>SparkSql是支持复杂数据结构，比如Array，Map，Struct，这类Expression支持在sql语句上生成它们，比如select array</td>
</tr>
<tr>
<td>Generator</td>
<td>eval</td>
<td>支持flatmap类似的操作，即将Row转变为多个Row，支持Explode和自定义UserDefinedGenerator两种，其中Explode支持将数组和map拆开为多个Row。</td>
</tr>
</tbody></table>
<h3 id="2-1-Attribute-详解"><a href="#2-1-Attribute-详解" class="headerlink" title="2.1 Attribute 详解"></a>2.1 Attribute 详解</h3><p>Attribute 直译为属性，在 SQL 中，可以简单理解为输入的 Table 中的字段，Attribute 通过 Name 字段来进行命名。SQL 语句通过 Parse 生成 AST 以后，SQL 语句中的每个字段都会解析为 UnresolvedAttribute，它是属于 Attribute 的一个子类，比如<code>SELECT a</code>中的<code>a</code>就表示为<code>UnresolvedAttribute(&quot;a&quot;)</code>，还有一个特殊的 UnresolvedAttribute，既为 SQL 语句中的<code>＊</code>，它表示为<code>Star</code>，属于 UnresolvedAttribute 类型的子类。</p>
<p>Analyser 需要对 AST 进行进一步的加工，其中有一个很重要的操作就是把整个 AST 中所有 Unresolved 的 Attribute 都转变为 resolved 状态，比如根据输入 Table 将 <code>Star</code> 进行 expand 操作，对应的<code>Rule</code>名称为<code>ResolveReferences</code>，具体实现细节这里就不展开。</p>
<p>对于 resolve 操作除了将 Star 进行展开以外，它的主要功能就是关联 SQL 语句所有位置用到的 Attribute，即在 Attribute 的 name 基础上，指定一个 ID 进行唯一标示，如果一个 Attribute 在两处被多处被引用，ID 即为同一个 (怎么实现的？Attribute Resolve 操作时从底到顶来遍历整改 AST，每一步都是根据底部已经 resloved 的 Attribute 来给顶部的 Attribute 赋值，从而保证如果两个 Attribute 是指向同一个，它们的 ID 肯定是一样的；对于处于叶子节点 Attribute 是优先进行 resolve 的，比如<code>Rule:ResolveRelations</code>对处于底部的 Relation 进行 ResolveRelatition)。可以这么理解，做这些事情都是为了优化，物理存储的 Table 可能有很多 Attribute，而通过 resolve 操作，就指定整个计算过程中需要使用到 Attribute，即可以只从物理存储中读取相应字段，上层各种 Expression 对这些字段都转变为引用，因此 resolve 以后的 Attribute 不是叫做<code>resolvedAttribute</code>,而是叫做<code>AttributeReference</code>。</p>
<p>对于一个中间节点的 Expression，如果它对一个 Attribute 有引用，比如求一个字段值的长度<code>length(a)</code>，这里<code>a</code>经过了 UnresolvedAttribute 到 AttributeReference 的转化，但是针对一个输入的 Row，进行<code>length</code>Expression 计算时，还是无法从<code>AttributeReference</code>中读取相应在 Row 中的值，为什么？虽然<code>AttributeReference</code>也是 Expression，但是它是 Unevaluable，为了获取属性在输入 Row 中对应的值，需要对<code>AttributeReference</code>再进行一次<code>BindReferences</code>的转化，生成<code>BoundReference</code>，这个操作本质就是将 Expression 和一个输入 Scheme 进行关联，Scheme 有一组<code>AttributeReference</code>，它们之间是有顺序的，通过获取 AttributeReference 在<code>AttributeReference</code>组中的 Index，并生成 BoundReference，在对 BoundReference 进行 eval 时候，即可以使用该 index 获取它在相应 Row 中的值。</p>
<p>下面是 SQL 语句中 Attribute 经过的多次转化的过程：</p>
<p>SQL—parser—-&gt;UnresolvedAttribute—-Analyser—&gt;AttributeReference(resolved) —-Bind—-&gt;BoundReference</p>
<h3 id="2-2-Expression-Codegen"><a href="#2-2-Expression-Codegen" class="headerlink" title="2.2 Expression Codegen"></a>2.2 Expression Codegen</h3><p>Spark Sql 是支持 Expression 层面代码生成，首先第一个疑问是我们知道 Expression 有 eval 方法来暴露 Expression 的功能，为什么还要搞一个 Codegen 呢？原因大体上有两个，一是提高性能，二是支持一些不能通过 eval 来表达的功能。这里主要解释了第一个，即提高性能，ps：我还没有去详细的测试这一块对性能影响大小，只是从感官上做一个结论：</p>
<p>基于 eval，Expression 执行过程中应该是这样的 e1.eval(e2.eval(e3.eval(e4.eval(…))))，随着 Expression 的复杂度上升，这个执行深度应该会很大，而且每一个操作之间是基于参数和返回进行传递，在操作系统系统层面是存在开销的；其次如果一个 Expression 计算结果会被多次利用，它的结果是没有办法被二次利用。</p>
<p>那么代码生成是这么解决这个问题的呢？</p>
<p>对于一个完整的 SQL 的执行，其中所有 Expression 只是对应一个最终执行代码的一个片段，注意是代码片段，而不是函数，更加不是一个类，每个代码片段由 ExprCode 来表示</p>
<p><code>case class ExprCode(var code: String, var isNull: String, var value: String)</code></p>
<p>code 为代码片段的计算代码，和 eval 函数功能一样，这个片段核心功能是对一个 row 进行处理，并把处理结果的值写到一个变量中，这个变量名称即为 value 表示，isNull 为 false 或者 true 字符串来表示这个代码片段生成的 value 对应的值是否为 Null。</p>
<pre><code>如果由 3 个 Expression，分别为
exp1:ExprCode(code1，isnull1，value1)
exp2:ExprCode(code2，isNull2，value2)
exp2:ExprCode(code3，isNull3，value3)
在 SQL 执行过程中，针对一个 Row 会生成下面的组合代码
funtion(row) &#123;
    //定义三个变量
    exp1.dataType value1 = defauleValue(exp1.dataType)
    exp2.dataType value2 = defauleValue(exp1.dataType)
    exp3.dataType value3 = defauleValue(exp1.dataType)
    //
    exp1.code1
    exp2.code2//可以使用 value1 变量的值
    exp3.code3//可以使用 value1 和 value2 的值
    ....
&#125;
</code></pre>
<p>Expression 层面的 Codegen 的实现其实很简单，这里就是不详细去描述，后面会重新针对 codegen，包括 Whole Stage Codegen 一起做一次大大专题进行分析。</p>
<h2 id="3-LogicalPlan"><a href="#3-LogicalPlan" class="headerlink" title="3. LogicalPlan"></a>3. LogicalPlan</h2><p>如上所言，在 SQL 语句中，除了 SELECT FROM 等关键字以外，其他大部分元素都可以理解为 Expression，那么用什么来表示剩下的 SELECT FROM 这些关键字呢？毕竟 Expression 只是一些 Eval 功能函数或者代码片段，需要一个东西来串联这些片段，这个东西就是<code>Plan</code>，注意，我这里说的是<code>Plan</code>，是一个统称，而不是仅指向<code>LogicalPlan</code>。如开言图所示，在 SQL 解析以及优化，到最后过程中，都是以<code>Plan</code>而存在，只是每一步<code>Plan</code>功能不同而已。</p>
<p><code>Plan</code>表现形式也是 Tree，节点之间的关系可以理解为一种操作次序，比如 Plan 叶子节点表示从磁盘读取 DB 文件，而 Root 节点表示最终数据的输出；下面是<code>Plan</code>最常见的实例截图。</p>
<p><img src="/2018/07/19/Spark-Catalyst-Deep-Dive/project-20210508110606340.png" alt="Alt text"></p>
<p>用 SQL 语句来表示这个 Plan 即为:<code>SELECT project FROM table, table WHERE filter</code>。</p>
<p>Expression 功能是对输入 Row 进行加工，输出可能是 Any 数据类型。而 Plan 输出类型为<code>def output: Seq[Attribute]</code>表示的一组 Attribute，比如上面的<code>Project</code>和<code>Table</code>肯定是输出一个由<code>Seq[Attribute]</code>类型表示的 Row，<code>Filter</code>感觉是输出 Ture/False，但是这里说的 Plan，而不是 Filter 类型的 Expreesion，Filter 类型的 Plan 会在内部根据 Expression 计算结果来判断是否返回 Row，但是 Row 返回的类型肯定也是由<code>Seq[Attribute]</code>表示的。</p>
<p>另外 Expression 与 Plan 关系是被包含，比如<code>Filter</code>肯定是基于一个<code>Expression</code>计算结果进行判断，<code>Project</code>内部元素要么是直接为<code>Star</code>，或者为<code>Attribute</code>，或者为复杂的<code>Expression</code>，比如<code>SUM</code>。</p>
<p>下面我开始分析<code>LogicalPlan</code>，它是 SQL 语句经过 Parse 以后第一步展现形式。基于 ANTLR 实现的 SQL AST 分析过程即为 AST 树遍历过程，Catalyst 也是对 AST 树遍历过程中，完成 LogicalPlan 和所有依赖的 Expression 的构建，相关逻辑在<code>org.apache.spark.sql.catalyst.parser.AstBuilder</code>以及相关子类中，如果对 ANTLR 有一点基础，这一块代码还是比较容易看懂，就不细化分析，我们着重放在 LogicalPlan 上面。</p>
<p>在上面示意图中，我们看到<code>LogicalPlan</code>是由一些节点组成，在 Spark SQL 中，节点大体分为两种类型：Operator 和 Command。其中我们上面看到的<code>Filter</code>都可以理解为 Operator，而我们在 SQL Cli 中执行<code>set a=b</code>以及<code>addjar a</code>,它们都是 Command 类型的 Plan，当然相比由很多 Operator 组成的多级复杂 Plan，Command 组成的 Plan 可能是单节点而存在，要简单一些，下面我们对 Operator 做一些归类。</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>Project</code>(projectList: Seq[NamedExpression], child: LogicalPlan)</td>
<td>SELECT 语句输出操作，其中 projectList 为输出对象，每一个都为一个 Expression，它们可能是 Star，或者很复杂的 Expression</td>
</tr>
<tr>
<td><code>Filter</code>(condition: Expression, child: LogicalPlan)</td>
<td>根据 condition 来对 Child 输入的 Rows 进行过滤</td>
</tr>
<tr>
<td><code>Join</code>(left: LogicalPlan,right: LogicalPlan,joinType: JoinType,condition: Option[Expression])</td>
<td>left 和 right 的输出结果进行 join 操作</td>
</tr>
<tr>
<td><code>Intersect</code>(left: LogicalPlan, right: LogicalPlan)</td>
<td>left 和 right 两个 Plan 输出的 rows 进行取交集运算。</td>
</tr>
<tr>
<td><code>Except</code>(left: LogicalPlan, right: LogicalPlan)</td>
<td>在 left 计算结果中剔除掉 right 中的计算结果</td>
</tr>
<tr>
<td><code>Union</code>(children: Seq[LogicalPlan])</td>
<td>将一组 Childs 的计算结果进行 Union 联合</td>
</tr>
<tr>
<td><code>Sort</code>(order: Seq[SortOrder],global: Boolean, child: LogicalPlan)</td>
<td>对 child 的输出进行 sort 排序</td>
</tr>
<tr>
<td><code>Repartition</code>(numPartitions: Int, shuffle: Boolean, child: LogicalPlan)</td>
<td>对 child 输出的数据进行重新分区操作</td>
</tr>
<tr>
<td><code>InsertIntoTable</code>(table: LogicalPlan,child: LogicalPlan,…)</td>
<td>将 child 输出的 rows 输出到 table 中</td>
</tr>
<tr>
<td><code>Distinct</code>(child: LogicalPlan)</td>
<td>对 child 输出的 rows 取重操作</td>
</tr>
<tr>
<td><code>GlobalLimit</code>(limitExpr: Expression, child: LogicalPlan)</td>
<td>对 Child 输出的数据进行 Limit 限制</td>
</tr>
<tr>
<td><code>Sample</code>(child: LogicalPlan,….)</td>
<td>根据一些参数，从 child 输出的 Rows 进行一定比例的取样</td>
</tr>
<tr>
<td><code>Aggregate</code>(groupingExpressions: Seq[Expression],aggregateExpressions: Seq[NamedExpression],child: LogicalPlan)</td>
<td>对 child 输出 row 进行 aggregate 操作，比如 groupby 之类的操作</td>
</tr>
</tbody></table>
<p>这些 Operator 共同组成 SELECT SQL 语句中各种核心语言要素，而且 Catatyst 后面的所有优化逻辑都是针对 SELECT 语句进行优化。对于譬如 CREATE TABLE 以及 SET 之类的 SQL 语言元素，它们都是 Command 存在，相比 SELECT，Command 组成的 Plan 要简单很多，不过它的种类倒是真的不少！</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>DataBase</code>操作类</td>
<td>支持 ShowDatabase 以及 UseDatabase 以及 Create 等操作</td>
</tr>
<tr>
<td><code>Table</code>操作类</td>
<td>多达 13 种，比如 Create，Show，Alter 等</td>
</tr>
<tr>
<td><code>View</code>操作类</td>
<td>CreateViewCommand 支持 View 的创建</td>
</tr>
<tr>
<td><code>Partition</code>操作类</td>
<td>支持 Partition 新增删除等操作</td>
</tr>
<tr>
<td><code>Resources</code>操作类</td>
<td>比如 AddJar 之类的资源操作</td>
</tr>
<tr>
<td><code>Functions</code>操作类</td>
<td>支持新增函数，删除函数等操作</td>
</tr>
<tr>
<td><code>Cache</code>操作类</td>
<td>支持对 Table 进行 cache 和 uncache 操作</td>
</tr>
<tr>
<td><code>Set</code>操作</td>
<td>通过 SetCommand 执行对参数进行临时修改</td>
</tr>
</tbody></table>
<p>由 Operator 组成的 Plan，仅仅只是一组描述形式的而存在，毕竟只是<code>LogicalPlan</code>，它们需要被转换为最终的<code>PhysicalPlan</code>才能真正具有可执行的能力，而这些 Command 类型的 Plan 都是以<code>def run(sparkSession: SparkSession): Seq[Row]</code>函数暴露给 Spark SQL，比如通过调用 Table 的 run 函数完成 Table 的创建等操作。因此我们可以肯定的是：Plan 优化都是针对以 Operator 组成的 Plan。</p>
<h2 id="4-Expression-和-Plan-的-Tree-支持的操作"><a href="#4-Expression-和-Plan-的-Tree-支持的操作" class="headerlink" title="4. Expression 和 Plan 的 Tree 支持的操作"></a>4. Expression 和 Plan 的 Tree 支持的操作</h2><p>在 Catalyst 中，Expression 和 Plan 都为 Tree 类型的数据结构，无论是从 SQL 语句中解析出 Plan 或者 Expression，或针对 Plan 或 Expression 进行 Analy 以及 Optimize 操作，都需要针对 Tree 数据结构进行遍历，其中经典 Tree 遍历算法有先序和后序遍历。</p>
<p>另外由于 TreeNode 节点本身类型为<code>Product</code>（何为<code>Product</code>？在 Scala 中<code>Product</code>是最基本数据类型之一，如果一个<code>Case Class</code> 继承<code>Product</code>，那么即可以通过<code>productElement</code>函数或者<code>productIterator</code>迭代器对<code>Case Class</code>的参数信息进行索引和遍历），并且所有 Expression 和 Plan 都是属于<code>Product</code>类型，因此可以通过 TreeNode 内部定义的<code>mapProductIterator</code>函数对节点参数进行遍历。在遍历过程中，可以针对参数类型进行 Case 过滤，从而有选择的处理本次希望处理的数据，比如对一个 Expression 中所有类型为 Expression 的子表达式进行操作，而可以忽略其他类型的参数。</p>
<p>对 Plan 或 Expression 进行遍历的目的：首先是为了收集一些信息，比如针对 Tree 进行 map/foreach 操作；其次是为了对 Tree 节点内部的信息进行修改，比如对 PlanTree 中每个 Plan 节点内部引用的 Attribute 进行 Revole 操作；最后就是为对 Tree 的数据结构进行修改，比如删除 Tree 的子节点，以及与子节点进行合并，比如 Catasylt Optitimze 就有大量 Tree 结构的修改。</p>
<p>Catalyst 在实现 Tree 的操作上，代码很是优雅的主要原因：它是基于 Scala 来实现。Scala 的偏函数 PartialFunction（偏函数是对函数定义域的一个子集进行定义的函数。 scala 中用 scala.PartialFunction[-T, +S] 类来表示）可以清晰的描述操作目的，比如<code>PartialFunction[LogicalPlan, LogicalPlan]</code>是针对 Plan 类型的节点进行操作，而<code>PartialFunction[Expression, Expression]</code>是针对 Expression 进行操作；其次 Scala 强大的 Case 正则匹配，让在对 Tree 进行遍历过程，可以清晰确定这一次需要操作的对象，如果用别的语言来实现下面 TypeFilter 合并，其代码将会是何等的苦涩。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">case t1 @ TypedFilter(_, _, t2 @ TypedFilter(_, _, child))</span><br><span class="line">    if t1.deserializer.dataType == t2.deserializer.dataType =&gt;</span><br><span class="line">    TypedFilter(combineFilterFunction(t2.func, t1.func), t1.deserializer, child)</span><br><span class="line">//优雅 Case 结构匹配语句，以及简洁的 CaseIF 判断，让快速定位相应的逻辑节点，并对节点数据进行修改变的如此简单。</span><br></pre></td></tr></table></figure>

<p>同时无论是对 Expression 进行操作还是对 Plan 进行操作，Catalyst 将他们都抽象为<code>Rule</code>，它的 apply 函数通过传入一个 TreeType 类型的元素，并输出一个 TreeType 类型的元素。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">abstract class Rule[TreeType &lt;: TreeNode[_]] extends Logging &#123;</span><br><span class="line">  val ruleName: String</span><br><span class="line">  def apply(plan: TreeType): TreeType</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在对一个元素进行<code>rule.apply</code>操作以后，可以针对前后的元素是否相等<code>curPlan.fastEquals(lastPlan)</code>，来确定该<code>rule</code>是否对该元素有效，其中无效可能是因为该 rule 没有 case 匹配到相应信息，首先可能是真的没有，其次<code>rule</code>是可以反复的应用到一个节点，直到节点状态趋向稳定，即<code>rule</code>已经应用多次，已经找不到匹配的信息了。</p>
<p>另外可以将一组<code>Rule</code>组合为一个<code>Batch(name: String,rules: Rule[TreeType]*)</code>并把它封装在<code>RuleExecutor</code>中，从而通过<code>RuleExecutor</code>将该组<code>Rule</code>的可执行接口提供给外部使用，比如大名顶顶的 Optimize 策略，就是一堆堆的 Batch 组成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">abstract class Optimizerextends RuleExecutor[LogicalPlan] &#123;</span><br><span class="line">    def batches: Seq[Batch] = &#123;</span><br><span class="line">        Batch(&quot;Finish Analysis&quot;, Once,</span><br><span class="line">        EliminateSubqueryAliases,</span><br><span class="line">        ReplaceExpressions,</span><br><span class="line">        ComputeCurrentTime,</span><br><span class="line">        GetCurrentDatabase(sessionCatalog),</span><br><span class="line">        RewriteDistinctAggregates) ::</span><br><span class="line">    ....</span><br></pre></td></tr></table></figure>

<p>如上所言，<code>Rule</code>是通过反复的应用在一个节点，直到节点状态趋向稳定。但是如优化类型的<code>Rule</code>，很多时候，优化是没有止境了，优化的越深，优化开销就越大。因此我也需要一定的手段来控制<code>Batch</code>应用到何种程度，比如<code>Once extends Strategy</code>表示该<code>Batch</code>只允许应用一次；而<code>FixedPoint extends Strategy</code>表示该<code>Batch</code>最多只允许应用 N 次，当然如果<code>batch</code>在运行过程中，节点已经<code>稳定</code>，会立即退出尝试的。</p>
<p>Spark SQL 对 Plan Tree 或者内部 Expression Tree 的遍历分为几个阶段：对 AST 进行 Parse 操作，生成 Unresolve Plan；对 Unresolve Plan 进行 Analy(包括 Resolve) 操作，生成 Logical Plan；对 Logical Plan 进行 Optimize 操作，生成 Optimized Logical Plan；以及最后进行 Planning 操作，生成 Physical Plan。这里面的每一阶段都可以简述为应用一组 BatchRule 来对 plan 进行加工，但是里面每一个 Rule 都是很值得去细节学习和分析的，实话，我也没有一个一个去看！！！</p>
<blockquote>
<p>本文主要是针对 catalyst 内部实现做了一些简洁的分析，注重分析与 catalyst 相关的三个概念<code>Row，Expression，Plan</code>，因为对三个概念的理解与否，将决定是否可以看懂 spark sql 后面相关细节。<br>同时，Spark SQL 真的很复杂，如果想真的完全理解 Spark SQL 内部的具体细节，这条路还是很长！fighting！</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2018/07/19/Java-Streaming-Deep-Dive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="陶加涛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/07/19/Java-Streaming-Deep-Dive/" class="post-title-link" itemprop="url">Java Streaming Deep Dive</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-07-19 18:12:38" itemprop="dateCreated datePublished" datetime="2018-07-19T18:12:38+08:00">2018-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-08 11:03:26" itemprop="dateModified" datetime="2021-05-08T11:03:26+08:00">2021-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Memory-map-file"><a href="#Memory-map-file" class="headerlink" title="Memory map file"></a>Memory map file</h1><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><p>写到文件:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">File f = File.createTempFile(<span class="string">&quot;dict&quot;</span>, <span class="string">&quot;.dict&quot;</span>);</span><br><span class="line">System.out.println(f.getAbsolutePath());</span><br><span class="line">f.deleteOnExit();</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> (DataOutputStream out = <span class="keyword">new</span> DataOutputStream(<span class="keyword">new</span> FileOutputStream(f))) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++) &#123;</span><br><span class="line">        out.writeInt(i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RandomAccessFile raf = <span class="keyword">new</span> RandomAccessFile(path, <span class="string">&quot;r&quot;</span>);</span><br><span class="line">File Channel fc = raf.getChannel();</span><br><span class="line">MappedByteBuffer byteBuffer = fc.map(FileChannel.MapMode.READ_ONLY, <span class="number">0</span>, fc.size());</span><br></pre></td></tr></table></figure>

<h2 id="unmap"><a href="#unmap" class="headerlink" title="unmap"></a>unmap</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    fc.close();</span><br><span class="line">    raf.close();</span><br><span class="line">    Method m = FileChannelImpl.class.getDeclaredMethod(<span class="string">&quot;unmap&quot;</span>, MappedByteBuffer.class);</span><br><span class="line">    m.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">    m.invoke(FileChannelImpl.class, byteBuffer);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Can not release file mapping memory.&quot;</span>, e);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="DirectByteBuffer"><a href="#DirectByteBuffer" class="headerlink" title="DirectByteBuffer"></a>DirectByteBuffer</h1><p>ByteBuffer的所有的get/put都有两套, 一套用绝对地址, 一套用相对地址</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Relative &lt;i&gt;get&lt;/i&gt; method for reading an int value.</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* &lt;p&gt; Reads the next four bytes at this buffer&#x27;s current position,</span></span><br><span class="line"><span class="comment">* composing them into an int value according to the current byte order,</span></span><br><span class="line"><span class="comment">* and then increments the position by four.  &lt;/p&gt;</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* <span class="doctag">@return</span>  The int value at the buffer&#x27;s current position</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* <span class="doctag">@throws</span>  BufferUnderflowException</span></span><br><span class="line"><span class="comment">*          If there are fewer than four bytes</span></span><br><span class="line"><span class="comment">*          remaining in this buffer</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">int</span> <span class="title">getInt</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Absolute &lt;i&gt;get&lt;/i&gt; method for reading an int value.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt; Reads four bytes at the given index, composing them into a</span></span><br><span class="line"><span class="comment"> * int value according to the current byte order.  &lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span>  index</span></span><br><span class="line"><span class="comment"> *         The index from which the bytes will be read</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span>  The int value at the given index</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span>  IndexOutOfBoundsException</span></span><br><span class="line"><span class="comment"> *          If &lt;tt&gt;index&lt;/tt&gt; is negative</span></span><br><span class="line"><span class="comment"> *          or not smaller than the buffer&#x27;s limit,</span></span><br><span class="line"><span class="comment"> *          minus three</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">int</span> <span class="title">getInt</span><span class="params">(<span class="keyword">int</span> index)</span></span>;</span><br></pre></td></tr></table></figure>

<p>可以看到有个参数<code>JNI_COPY_TO_ARRAY_THRESHOLD</code>, 默认是6, 如果大于这个值, 会走 <code>unsafe.copyMemory</code> 的方式来把内存的值放到 array 中. 否则</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// These numbers represent the point at which we have empirically determined that the average cost of a JNI call exceeds the expense of an element by element copy.These numbers may change over time.</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> JNI_COPY_TO_ARRAY_THRESHOLD   = <span class="number">6</span>;</span><br><span class="line"></span><br><span class="line">A limit is imposed to allow <span class="keyword">for</span> safepoint polling during a large copy</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> UNSAFE_COPY_THRESHOLD = <span class="number">1024L</span> * <span class="number">1024L</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> ByteBuffer <span class="title">get</span><span class="params">(<span class="keyword">byte</span>[] dst, <span class="keyword">int</span> offset, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (((<span class="keyword">long</span>)length &lt;&lt; <span class="number">0</span>) &gt; Bits.JNI_COPY_TO_ARRAY_THRESHOLD) &#123;</span><br><span class="line">        checkBounds(offset, length, dst.length);</span><br><span class="line">        <span class="keyword">int</span> pos = position();</span><br><span class="line">        <span class="keyword">int</span> lim = limit();</span><br><span class="line">        <span class="keyword">assert</span> (pos &lt;= lim);</span><br><span class="line">        <span class="keyword">int</span> rem = (pos &lt;= lim ? lim - pos : <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span> (length &gt; rem)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> BufferUnderflowException();</span><br><span class="line">            Bits.copyToArray(ix(pos), dst, arrayBaseOffset,</span><br><span class="line">                             (<span class="keyword">long</span>)offset &lt;&lt; <span class="number">0</span>,</span><br><span class="line">                             (<span class="keyword">long</span>)length &lt;&lt; <span class="number">0</span>);</span><br><span class="line">        position(pos + length);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">super</span>.get(dst, offset, length);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">copyToArray</span><span class="params">(<span class="keyword">long</span> srcAddr, Object dst, <span class="keyword">long</span> dstBaseOffset, <span class="keyword">long</span> dstPos,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">long</span> length)</span></span>&#123;</span><br><span class="line">    <span class="keyword">long</span> offset = dstBaseOffset + dstPos;</span><br><span class="line">    <span class="keyword">while</span> (length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">long</span> size = (length &gt; UNSAFE_COPY_THRESHOLD) ? UNSAFE_COPY_THRESHOLD : length;</span><br><span class="line">        unsafe.copyMemory(<span class="keyword">null</span>, srcAddr, dst, offset, size);</span><br><span class="line">        length -= size;</span><br><span class="line">        srcAddr += size;</span><br><span class="line">        offset += size;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> ByteBuffer <span class="title">get</span><span class="params">(<span class="keyword">byte</span>[] dst, <span class="keyword">int</span> offset, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    checkBounds(offset, length, dst.length);</span><br><span class="line">    <span class="keyword">if</span> (length &gt; remaining())</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> BufferUnderflowException();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> end = offset + length;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = offset; i &lt; end; i++) &#123;</span><br><span class="line">        dst[i] = get();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="unsafe-操作"><a href="#unsafe-操作" class="headerlink" title="unsafe 操作"></a>unsafe 操作</h1><p>Spark 有一套Platform可以很方便的操作(占个坑先, 日后研究发).</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Field address = Buffer.class.getDeclaredField(&quot;address&quot;);</span><br><span class="line">address.setAccessible(true);</span><br><span class="line">long baseAddress = (long) address.get(buffer);</span><br><span class="line"></span><br><span class="line">public void copyMemory(int pos, byte[] dst, int offset, int length) &#123;</span><br><span class="line">    Platform.copyMemory(null, ix(pos), dst,</span><br><span class="line">            BYTE_ARRAY_OFFSET + ((long) offset &lt;&lt; 0),</span><br><span class="line">            (long) length &lt;&lt; 0);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private long ix(int i) &#123;</span><br><span class="line">    return baseAddress + ((long) i &lt;&lt; 0);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="番外-Delete-on-exit"><a href="#番外-Delete-on-exit" class="headerlink" title="番外 : Delete on exit"></a>番外 : Delete on exit</h1><p>deleteOnExit 不一定会成功 如果文件有流还没关闭.</p>
<p>当通过kill -15 pid,kill -2 pid(中断, ctrl+c) 结束该Java进程时ShutdownHook会被调用。但Kill -9 pid不会触发ShutdownHook调用</p>
<p>FileInputStream 类都是操作一个文件的接口，注意到在创建一个 FileInputStream 对象时，会创建一个 FileDescriptor 对象，其实这个对象就是真正代表一个存在的文件对象的描述，当我们在操作一个文件对象时可以通过 getFD() 方法获取真正操作的与底层操作系统关联的文件描述。例如可以调用 FileDescriptor.sync() 方法将操作系统缓存中的数据强制刷新到物理磁盘中。</p>
<p><img src="/2018/07/19/Java-Streaming-Deep-Dive/image015-20210508110323870.jpg"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">陶加涛</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
