<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Kyligence">
<meta property="og:type" content="website">
<meta property="og:title" content="Jiatao Tao&#39;s blog">
<meta property="og:url" content="https://tttmelody.github.io/index.html">
<meta property="og:site_name" content="Jiatao Tao&#39;s blog">
<meta property="og:description" content="Kyligence">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jiatao Tao&#39;s blog">
<meta name="twitter:description" content="Kyligence">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://tttmelody.github.io/"/>





  <title>Jiatao Tao's blog</title>
  








  
  <script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
  
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiatao Tao's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tttmelody.github.io/2018/10/24/寻找-spark-executor-日志/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aron">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://gitee.com/Meldoy/image/raw/master/life/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/24/寻找-spark-executor-日志/" itemprop="url">寻找 spark executor 日志</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-24T22:21:28+08:00">
                2018-10-24
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/24/寻找-spark-executor-日志/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/10/24/寻找-spark-executor-日志/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/10/24/寻找-spark-executor-日志/" class="leancloud_visitors" data-flag-title="寻找 spark executor 日志">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>spark on yarn 应用在运行时和完成后日志的存放位置是不同的，一般运行时是存放在各个运行节点，完成后会归集到 hdfs。无论哪种情况，都可以通过 spark 的页面跳转找到 executor 的日志，但是在大多数的生产环境中，对端口的开放是有严格的限制，也就是说根本无法正常跳转到日志页面进行查看的，这种情况下，就需要通过后台查询。</p>
<h2 id="运行时"><a href="#运行时" class="headerlink" title="运行时"></a>运行时</h2><p>spark on yarn 模式下一个 executor 对应 yarn 的一个 container，所以在 executor 的节点运行<code>ps -ef|grep spark.yarn.app.container.log.dir</code>，如果这个节点上可能运行多个 application，那么再通过 application id 进一步过滤。上面的命令会查到 executor 的进程信息，并且包含了日志路径，例如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> -Djava.io.tmpdir=/data1/hadoop/yarn/local/usercache/ocdp/appcache/application_1521424748238_0051/container_e07_1521424748238_0051_01_000002/tmp &apos;</span><br><span class="line">-Dspark.history.ui.port=18080&apos; &apos;-Dspark.driver.port=59555&apos; </span><br><span class="line">-Dspark.yarn.app.container.log.dir=/data1/hadoop/yarn/log/application_1521424748238_0051/container_e07_1521424748238_0051_01_000002</span><br></pre></td></tr></table></figure>
<p>也就是说这个 executor 的日志就在<code>/data1/hadoop/yarn/log/application_1521424748238_0051/container_e07_1521424748238_0051_01_000002</code>目录里。至此，我们就找到了运行时的 executor 日志。</p>
<h2 id="完成后"><a href="#完成后" class="headerlink" title="完成后"></a>完成后</h2><p>当这个 application 正常或者由于某种原因异常结束后，yarn 默认会将所有日志归集到 hdfs 上，所以 yarn 也提供了一个查询已结束 application 日志的方法，即<br> <code>yarn logs -applicationId application_1521424748238_0057</code>，结果里面会包含所有 executor 的日志，可能会比较多，建议将结果重定向到一个文件再详细查看。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>无论对于 spark 应用程序的开发者还是运维人员，日志对于排查问题是至关重要的，所以本文介绍了找到日志的方法。</p>
<h2 id="画外-log4j-配置-spark-on-yarn-client-mode"><a href="#画外-log4j-配置-spark-on-yarn-client-mode" class="headerlink" title="画外:log4j 配置 - spark on yarn client mode"></a>画外:log4j 配置 - spark on yarn client mode</h2><p>spark streaming 的程序如果运行方式是 yarn <strong><em>client</em></strong> mode，那么如何指定 driver 和 executor 的 log4j 配置文件？</p>
<h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>添加参数<code>--driver-java-options</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --driver-java-options &quot;-Dlog4j.configuration=file:/data1/conf/log4j-driver.properties&quot;</span><br></pre></td></tr></table></figure>
<h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>由于 executor 是运行在 yarn 的集群中的，所以先要将配置文件通过<code>--files</code>上传</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --files /data1/conf/log4j.properties --conf spark.executor.extraJavaOptions=&quot;-Dlog4j.configuration=log4j.properties&quot;</span><br></pre></td></tr></table></figure>
<p>在 log4j.properties 中要注意配置<code>spark.yarn.app.container.log.dir</code>例如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, file</span><br><span class="line">log4j.appender.file=org.apache.log4j.RollingFileAppender</span><br><span class="line">log4j.appender.file.append=true</span><br><span class="line">log4j.appender.file.file=$&#123;spark.yarn.app.container.log.dir&#125;/stdout</span><br><span class="line">log4j.appender.file.MaxFileSize=256MB</span><br><span class="line">log4j.appender.file.MaxBackupIndex=20</span><br><span class="line"></span><br><span class="line">log4j.appender.file.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.file.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %p [%t] %c&#123;1&#125;:%L - %m%n</span><br><span class="line"></span><br><span class="line"># Settings to quiet third party logs that are too verbose</span><br><span class="line">log4j.logger.org.spark-project.jetty=WARN</span><br><span class="line">log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ERROR</span><br><span class="line">log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO</span><br><span class="line">log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO</span><br></pre></td></tr></table></figure>
<p>这样就可以在 spark 的 Web UI 中直接查看日志:executor tab 下的 Logs.</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>如果是通过<code>java -cp</code>命令运行自己的 jar 包，可以通过下面的方式添加 log4j 的配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -cp -Dlog4j.configuration=file:$&#123;APP_HOME&#125;/conf/log4j.properties</span><br></pre></td></tr></table></figure>
<p>作者：Woople, 链接：<a href="https://www.jianshu.com/p/06a630618f19" target="_blank" rel="noopener">https://www.jianshu.com/p/06a630618f19</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tttmelody.github.io/2018/10/24/Shuffle/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aron">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://gitee.com/Meldoy/image/raw/master/life/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/24/Shuffle/" itemprop="url">Shuffle</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-24T14:09:16+08:00">
                2018-10-24
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/24/Shuffle/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/10/24/Shuffle/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/10/24/Shuffle/" class="leancloud_visitors" data-flag-title="Shuffle">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Shuffle-过程介绍"><a href="#Shuffle-过程介绍" class="headerlink" title="Shuffle 过程介绍"></a><strong>Shuffle 过程介绍</strong></h1><h2 id="MapReduce-的-Shuffle-过程介绍"><a href="#MapReduce-的-Shuffle-过程介绍" class="headerlink" title="MapReduce 的 Shuffle 过程介绍"></a><strong>MapReduce 的 Shuffle 过程介绍</strong></h2><p>Shuffle 的本义是洗牌、混洗，把一组有一定规则的数据尽量转换成一组无规则的数据，越随机越好。MapReduce 中的 shuffle 更像是洗牌的逆过程，把一组无规则的数据尽量转换成一组具有一定规则的数据。</p>
<p>为什么 MapReduce 计算模型需要 shuffle 过程？我们都知道 MapReduce 计算模型一般包括两个重要的阶段：map 是映射，负责数据的过滤分发；reduce 是规约，负责数据的计算归并。Reduce 的数据来源于 map，map 的输出即是 reduce 的输入，reduce 需要通过 shuffle 来获取数据。</p>
<p>从 map 输出到 reduce 输入的整个过程可以广义地称为 shuffle。Shuffle 横跨 map 端和 reduce 端，在 map 端包括 spill 过程，在 reduce 端包括 copy 和 sort 过程，如图所示：</p>
<p><img src="http://data.qq.com/resource/imgcache/uploads/2014/07/image001.jpg" alt="image001"></p>
<h3 id="Map-Shuffle-Spill-过程"><a href="#Map-Shuffle-Spill-过程" class="headerlink" title="Map Shuffle (Spill 过程)"></a>Map Shuffle (Spill 过程)</h3><p>Spill 过程包括输出、排序、溢写、合并等步骤，如图所示：</p>
<p><img src="http://data.qq.com/resource/imgcache/uploads/2014/07/image002.png" alt="image002"></p>
<h4 id="Collect"><a href="#Collect" class="headerlink" title="Collect"></a><strong>Collect</strong></h4><p>每个 Map 任务不断地以 &lt;key, value&gt; 对的形式把数据输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内存空间，在内存中放置尽可能多的数据。</p>
<p>这个数据结构其实就是个字节数组，叫 kvbuffer，名如其义，但是这里面不光放置了 &lt;key, value&gt; 数据，还放置了一些索引数据，给放置索引数据的区域起了一个 kvmeta 的别名，在 kvbuffer 的一块区域上穿了一个 IntBuffer（字节序采用的是平台自身的字节序）的马甲。&lt;key, value&gt; 数据区域和索引数据区域在 kvbuffer 中是相邻不重叠的两个区域，用一个分界点来划分两者，分界点不是亘古不变的，而是每次 spill 之后都会更新一次。初始的分界点是 0，&lt;key, value&gt; 数据的存储方向是向上增长，索引数据的存储方向是向下增长，如图所示：</p>
<p><img src="http://data.qq.com/resource/imgcache/uploads/2014/07/image003.png" alt="image003"></p>
<p>Kvbuffer 的存放指针 bufindex 是一直闷着头地向上增长，比如 bufindex 初始值为 0，一个 Int 型的 key 写完之后，bufindex 增长为 4，一个 Int 型的 value 写完之后，bufindex 增长为 8。</p>
<p>索引是对 &lt;key, value&gt; 在 kvbuffer 中的索引，是个四元组，包括：value 的起始位置、key 的起始位置、partition 值、value 的长度，占用四个 Int 长度，kvmeta 的存放指针 kvindex 每次都是向下跳四个 “格子”，然后再向上一个格子一个格子地填充四元组的数据。比如 kvindex 初始位置是 - 4，当第一个 &lt;key, value&gt; 写完之后，(kvindex+0) 的位置存放 value 的起始位置、(kvindex+1)的位置存放 key 的起始位置、(kvindex+2)的位置存放 partition 的值、(kvindex+3)的位置存放 value 的长度，然后 kvindex 跳到 - 8 位置，等第二个 &lt;key, value&gt; 和索引写完之后，kvindex 跳到 - 32 位置。</p>
<p>Kvbuffer 的大小虽然可以通过参数设置，但是总共就那么大，&lt;key, value&gt; 和索引不断地增加，加着加着，kvbuffer 总有不够用的那天，那怎么办？把数据从内存刷到磁盘上再接着往内存写数据，把 kvbuffer 中的数据刷到磁盘上的过程就叫 spill，多么明了的叫法，内存中的数据满了就自动地 spill 到具有更大空间的磁盘。</p>
<p>关于 spill 触发的条件，也就是 kvbuffer 用到什么程度开始 spill，还是要讲究一下的。如果把 kvbuffer 用得死死得，一点缝都不剩的时候再开始 spill，那 map 任务就需要等 spill 完成腾出空间之后才能继续写数据；如果 kvbuffer 只是满到一定程度，比如 80% 的时候就开始 spill，那在 spill 的同时，map 任务还能继续写数据，如果 spill 够快，map 可能都不需要为空闲空间而发愁。两利相衡取其大，一般选择后者。</p>
<p>Spill 这个重要的过程是由 spill 线程承担，spill 线程从 map 任务接到 “命令” 之后就开始正式干活，干的活叫 sortAndSpill，原来不仅仅是 spill，在 spill 之前还有个颇具争议性的 sort。</p>
<h4 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a><strong>Sort</strong></h4><p>先把 kvbuffer 中的数据按照 partition 值和 key 两个关键字升序排序，移动的只是索引数据，排序结果是 kvmeta 中数据按照 partition 为单位聚集在一起，同一 partition 内的按照 key 有序。</p>
<h4 id="Spill"><a href="#Spill" class="headerlink" title="Spill"></a><strong>Spill</strong></h4><p>Spill 线程为这次 spill 过程创建一个磁盘文件：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于 “spill12.out” 的文件。Spill 线程根据排过序的 kvmeta 挨个 partition 的把 &lt;key, value&gt; 数据吐到这个文件中，一个 partition 对应的数据吐完之后顺序地吐下个 partition，直到把所有的 partition 遍历完。一个 partition 在文件中对应的数据也叫段(segment)。</p>
<p>所有的 partition 对应的数据都放在这个文件里，虽然是顺序存放的，但是怎么直接知道某个 partition 在这个文件中存放的起始位置呢？强大的索引又出场了。有一个三元组记录某个 partition 对应的数据在这个文件中的索引：起始位置、原始数据长度、压缩之后的数据长度，一个 partition 对应一个三元组。然后把这些索引信息存放在内存中，如果内存中放不下了，后续的索引信息就需要写到磁盘文件中了：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于 “spill12.out.index” 的文件，文件中不光存储了索引数据，还存储了 crc32 的校验数据。(spill12.out.index 不一定在磁盘上创建，如果内存（默认 1M 空间）中能放得下就放在内存中，即使在磁盘上创建了，和 spill12.out 文件也不一定在同一个目录下。)</p>
<p>每一次 spill 过程就会最少生成一个 out 文件，有时还会生成 index 文件，spill 的次数也烙印在文件名中。索引文件和数据文件的对应关系如下图所示：</p>
<p><img src="http://data.qq.com/resource/imgcache/uploads/2014/07/image004.png" alt="image004"></p>
<p>话分两端，在 spill 线程如火如荼的进行 sortAndSpill 工作的同时，map 任务不会因此而停歇，而是一无既往地进行着数据输出。Map 还是把数据写到 kvbuffer 中，那问题就来了：&lt;key, value&gt; 只顾着闷头按照 bufindex 指针向上增长，kvmeta 只顾着按照 kvindex 向下增长，是保持指针起始位置不变继续跑呢，还是另谋它路？如果保持指针起始位置不变，很快 bufindex 和 kvindex 就碰头了，碰头之后再重新开始或者移动内存都比较麻烦，不可取。Map 取 kvbuffer 中剩余空间的中间位置，用这个位置设置为新的分界点，bufindex 指针移动到这个分界点，kvindex 移动到这个分界点的 - 16 位置，然后两者就可以和谐地按照自己既定的轨迹放置数据了，当 spill 完成，空间腾出之后，不需要做任何改动继续前进。分界点的转换如下图所示：</p>
<p><img src="http://data.qq.com/resource/imgcache/uploads/2014/07/image005.png" alt="image005"></p>
<p>Map 任务总要把输出的数据写到磁盘上，即使输出数据量很小在内存中全部能装得下，在最后也会把数据刷到磁盘上。</p>
<h4 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a><strong>Merge</strong></h4><p>Map 任务如果输出数据量很大，可能会进行好几次 spill，out 文件和 index 文件会产生很多，分布在不同的磁盘上。最后把这些文件进行合并的 merge 过程闪亮登场。</p>
<p>Merge 过程怎么知道产生的 spill 文件都在哪了呢？从所有的本地目录上扫描得到产生的 spill 文件，然后把路径存储在一个数组里。Merge 过程又怎么知道 spill 的索引信息呢？没错，也是从所有的本地目录上扫描得到 index 文件，然后把索引信息存储在一个列表里。到这里，又遇到了一个值得纳闷的地方。在之前 spill 过程中的时候为什么不直接把这些信息存储在内存中呢，何必又多了这步扫描的操作？特别是 spill 的索引数据，之前当内存超限之后就把数据写到磁盘，现在又要从磁盘把这些数据读出来，还是需要装到更多的内存中。之所以多此一举，是因为这时 kvbuffer 这个内存大户已经不再使用可以回收，有内存空间来装这些数据了。（对于内存空间较大的土豪来说，用内存来省却这两个 io 步骤还是值得考虑的。）</p>
<p>然后为 merge 过程创建一个叫 file.out 的文件和一个叫 file.out.index 的文件用来存储最终的输出和索引。</p>
<p>一个 partition 一个 partition 的进行合并输出。对于某个 partition 来说，从索引列表中查询这个 partition 对应的所有索引信息，每个对应一个段插入到段列表中。也就是这个 partition 对应一个段列表，记录所有的 spill 文件中对应的这个 partition 那段数据的文件名、起始位置、长度等等。</p>
<p>然后对这个 partition 对应的所有的 segment 进行合并，目标是合并成一个 segment。当这个 partition 对应很多个 segment 时，会分批地进行合并：先从 segment 列表中把第一批取出来，以 key 为关键字放置成最小堆，然后从最小堆中每次取出最小的 &lt;key, value&gt; 输出到一个临时文件中，这样就把这一批段合并成一个临时的段，把它加回到 segment 列表中；再从 segment 列表中把第二批取出来合并输出到一个临时 segment，把其加入到列表中；这样往复执行，直到剩下的段是一批，输出到最终的文件中。</p>
<p>最终的索引数据仍然输出到 index 文件中。</p>
<p><img src="http://data.qq.com/resource/imgcache/uploads/2014/07/image006.png" alt="image006"></p>
<p>Map 端的 shuffle 过程到此结束。</p>
<h3 id="Reduce-shuffle"><a href="#Reduce-shuffle" class="headerlink" title="Reduce shuffle"></a>Reduce shuffle</h3><h4 id="Copy"><a href="#Copy" class="headerlink" title="Copy"></a><strong>Copy</strong></h4><p>Reduce 任务通过 http 向各个 map 任务拖取它所需要的数据。每个节点都会启动一个常驻的 http server，其中一项服务就是响应 reduce 拖取 map 数据。当有 mapOutput 的 http 请求过来的时候，http server 就读取相应的 map 输出文件中对应这个 reduce 部分的数据通过网络流输出给 reduce。</p>
<p>Reduce 任务拖取某个 map 对应的数据，如果在内存中能放得下这次数据的话就直接把数据写到内存中。Reduce 要向每个 map 去拖取数据，在内存中每个 map 对应一块数据，当内存中存储的 map 数据占用空间达到一定程度的时候，开始启动内存中 merge，把内存中的数据 merge 输出到磁盘上一个文件中。</p>
<p>如果在内存中不能放得下这个 map 的数据的话，直接把 map 数据写到磁盘上，在本地目录创建一个文件，从 http 流中读取数据然后写到磁盘，使用的缓存区大小是 64K。拖一个 map 数据过来就会创建一个文件，当文件数量达到一定阈值时，开始启动磁盘文件 merge，把这些文件合并输出到一个文件。</p>
<p>有些 map 的数据较小是可以放在内存中的，有些 map 的数据较大需要放在磁盘上，这样最后 reduce 任务拖过来的数据有些放在内存中了有些放在磁盘上，最后会对这些来一个全局合并。</p>
<h4 id="Merge-Sort"><a href="#Merge-Sort" class="headerlink" title="Merge Sort"></a><strong>Merge Sort</strong></h4><p>这里使用的 merge 和 map 端使用的 merge 过程一样。Map 的输出数据已经是有序的，merge 进行一次合并排序，所谓 reduce 端的 sort 过程就是这个合并的过程。一般 reduce 是一边 copy 一边 sort，即 copy 和 sort 两个阶段是重叠而不是完全分开的。</p>
<p>Reduce 端的 shuffle 过程至此结束。</p>
<h2 id="Spark-的-Shuffle-过程介绍"><a href="#Spark-的-Shuffle-过程介绍" class="headerlink" title="Spark 的 Shuffle 过程介绍"></a><strong>Spark 的 Shuffle 过程介绍</strong></h2><h3 id="Shuffle-Writer"><a href="#Shuffle-Writer" class="headerlink" title="Shuffle Writer"></a><strong>Shuffle Writer</strong></h3><p>Spark 丰富了任务类型，有些任务之间数据流转不需要通过 shuffle，但是有些任务之间还是需要通过 shuffle 来传递数据，比如 wide dependency 的 group by key。</p>
<p>Spark 中需要 shuffle 输出的 map 任务会为每个 reduce 创建对应的 bucket，map 产生的结果会根据设置的 partitioner 得到对应的 bucketId，然后填充到相应的 bucket 中去。每个 map 的输出结果可能包含所有的 reduce 所需要的数据，所以每个 map 会创建 R 个 bucket（R 是 reduce 的个数），M 个 map 总共会创建 M*R 个 bucket。</p>
<p>Map 创建的 bucket 其实对应磁盘上的一个文件，map 的结果写到每个 bucket 中其实就是写到那个磁盘文件中，这个文件也被称为 blockFile，是 DiskBlockManager 管理器通过文件名的 hash 值对应到本地目录的子目录中创建的。每个 map 要在节点上创建 R 个磁盘文件用于结果输出，map 的结果是直接输出到磁盘文件上的，100KB 的内存缓冲是用来创建 FastBufferedOutputStream 输出流。这种方式一个问题就是 shuffle 文件过多。</p>
<p><img src="http://data.qq.com/resource/imgcache/uploads/2014/07/image007.png" alt="image007"></p>
<p>针对上述 shuffle 过程产生的文件过多问题，Spark 有另外一种改进的 shuffle 过程：consolidation shuffle，以期显著减少 shuffle 文件的数量。在 consolidation shuffle 中每个 bucket 并非对应一个文件，而是对应文件中的一个 segment 部分。Job 的 map 在某个节点上第一次执行，为每个 reduce 创建 bucket 对应的输出文件，把这些文件组织成 ShuffleFileGroup，当这次 map 执行完之后，这个 ShuffleFileGroup 可以释放为下次循环利用；当又有 map 在这个节点上执行时，不需要创建新的 bucket 文件，而是在上次的 ShuffleFileGroup 中取得已经创建的文件继续追加写一个 segment；当前次 map 还没执行完，ShuffleFileGroup 还没有释放，这时如果有新的 map 在这个节点上执行，无法循环利用这个 ShuffleFileGroup，而是只能创建新的 bucket 文件组成新的 ShuffleFileGroup 来写输出。</p>
<p><img src="http://data.qq.com/resource/imgcache/uploads/2014/07/image008.png" alt="image008"></p>
<p>比如一个 job 有 3 个 map 和 2 个 reduce：(1) 如果此时集群有 3 个节点有空槽，每个节点空闲了一个 core，则 3 个 map 会调度到这 3 个节点上执行，每个 map 都会创建 2 个 shuffle 文件，总共创建 6 个 shuffle 文件；(2) 如果此时集群有 2 个节点有空槽，每个节点空闲了一个 core，则 2 个 map 先调度到这 2 个节点上执行，每个 map 都会创建 2 个 shuffle 文件，然后其中一个节点执行完 map 之后又调度执行另一个 map，则这个 map 不会创建新的 shuffle 文件，而是把结果输出追加到之前 map 创建的 shuffle 文件中；总共创建 4 个 shuffle 文件；(3) 如果此时集群有 2 个节点有空槽，一个节点有 2 个空 core 一个节点有 1 个空 core，则一个节点调度 2 个 map 一个节点调度 1 个 map，调度 2 个 map 的节点上，一个 map 创建了 shuffle 文件，后面的 map 还是会创建新的 shuffle 文件，因为上一个 map 还正在写，它创建的 ShuffleFileGroup 还没有释放；总共创建 6 个 shuffle 文件。</p>
<h3 id="Shuffle-Fetcher"><a href="#Shuffle-Fetcher" class="headerlink" title="Shuffle Fetcher"></a><strong>Shuffle Fetcher</strong></h3><p>Reduce 去拖 map 的输出数据，Spark 提供了两套不同的拉取数据框架：通过 socket 连接去取数据；使用 netty 框架去取数据。</p>
<p>每个节点的 Executor 会创建一个 BlockManager，其中会创建一个 BlockManagerWorker 用于响应请求。当 reduce 的 GET_BLOCK 的请求过来时，读取本地文件将这个 blockId 的数据返回给 reduce。如果使用的是 Netty 框架，BlockManager 会创建 ShuffleSender 用于发送 shuffle 数据。</p>
<p>并不是所有的数据都是通过网络读取，对于在本节点的 map 数据，reduce 直接去磁盘上读取而不再通过网络框架。</p>
<p>Reduce 拖过来数据之后以什么方式存储呢？Spark map 输出的数据没有经过排序，spark shuffle 过来的数据也不会进行排序，spark 认为 shuffle 过程中的排序不是必须的，并不是所有类型的 reduce 需要的数据都需要排序，强制地进行排序只会增加 shuffle 的负担。Reduce 拖过来的数据会放在一个 HashMap 中，HashMap 中存储的也是 &lt;key, value&gt; 对，key 是 map 输出的 key，map 输出对应这个 key 的所有 value 组成 HashMap 的 value。Spark 将 shuffle 取过来的每一个 &lt;key, value&gt; 对插入或者更新到 HashMap 中，来一个处理一个。HashMap 全部放在内存中。</p>
<p>Shuffle 取过来的数据全部存放在内存中，对于数据量比较小或者已经在 map 端做过合并处理的 shuffle 数据，占用内存空间不会太大，但是对于比如 group by key 这样的操作，reduce 需要得到 key 对应的所有 value，并将这些 value 组一个数组放在内存中，这样当数据量较大时，就需要较多内存。</p>
<p>当内存不够时，要不就失败，要不就用老办法把内存中的数据移到磁盘上放着。Spark 意识到在处理数据规模远远大于内存空间时所带来的不足，引入了一个具有外部排序的方案。Shuffle 过来的数据先放在内存中，当内存中存储的 &lt;key, value&gt; 对超过 1000 并且内存使用超过 70% 时，判断节点上可用内存如果还足够，则把内存缓冲区大小翻倍，如果可用内存不再够了，则把内存中的 &lt;key, value&gt; 对排序然后写到磁盘文件中。最后把内存缓冲区中的数据排序之后和那些磁盘文件组成一个最小堆，每次从最小堆中读取最小的数据，这个和 MapReduce 中的 merge 过程类似。</p>
<h2 id="MapReduce-和-Spark-的-Shuffle-过程对比"><a href="#MapReduce-和-Spark-的-Shuffle-过程对比" class="headerlink" title="MapReduce 和 Spark 的 Shuffle 过程对比"></a><strong>MapReduce 和 Spark 的 Shuffle 过程对比</strong></h2><table>
<thead>
<tr>
<th></th>
<th>MapReduce</th>
<th>Spark</th>
</tr>
</thead>
<tbody>
<tr>
<td>collect</td>
<td>在内存中构造了一块数据结构用于 map 输出的缓冲</td>
<td>没有在内存中构造一块数据结构用于 map 输出的缓冲，而是直接把输出写到磁盘文件</td>
</tr>
<tr>
<td>sort</td>
<td>map 输出的数据有排序</td>
<td>map 输出的数据没有排序</td>
</tr>
<tr>
<td>merge</td>
<td>对磁盘上的多个 spill 文件最后进行合并成一个输出文件</td>
<td>在 map 端没有 merge 过程，在输出时直接是对应一个 reduce 的数据写到一个文件中，这些文件同时存在并发写，最后不需要合并成一个</td>
</tr>
<tr>
<td>copy 框架</td>
<td>jetty</td>
<td>netty 或者直接 socket 流</td>
</tr>
<tr>
<td>对于本节点上的文件</td>
<td>仍然是通过网络框架拖取数据</td>
<td>不通过网络框架，对于在本节点上的 map 输出文件，采用本地读取的方式</td>
</tr>
<tr>
<td>copy 过来的数据存放位置</td>
<td>先放在内存，内存放不下时写到磁盘</td>
<td>一种方式全部放在内存；另一种方式先放在内存</td>
</tr>
<tr>
<td>merge sort</td>
<td>最后会对磁盘文件和内存中的数据进行合并排序</td>
<td>对于采用另一种方式时也会有合并排序的过程</td>
</tr>
</tbody>
</table>
<h2 id="Shuffle-后续优化方向"><a href="#Shuffle-后续优化方向" class="headerlink" title="Shuffle 后续优化方向"></a><strong>Shuffle 后续优化方向</strong></h2><p>通过上面的介绍，我们了解到，shuffle 过程的主要存储介质是磁盘，尽量的减少 io 是 shuffle 的主要优化方向。我们脑海中都有那个经典的存储金字塔体系，shuffle 过程为什么把结果都放在磁盘上，那是因为现在内存再大也大不过磁盘，内存就那么大，还这么多张嘴吃，当然是分配给最需要的了。如果具有 “土豪” 内存节点，减少 shuffle io 的最有效方式无疑是尽量把数据放在内存中。下面列举一些现在看可以优化的方面，期待经过我们不断的努力，TDW 计算引擎运行地更好。</p>
<h3 id="MapReduce-Shuffle-后续优化方向"><a href="#MapReduce-Shuffle-后续优化方向" class="headerlink" title="MapReduce Shuffle 后续优化方向"></a><strong>MapReduce Shuffle 后续优化方向</strong></h3><ul>
<li><p>压缩：对数据进行压缩，减少写读数据量；</p>
</li>
<li><p>减少不必要的排序：并不是所有类型的 reduce 需要的数据都是需要排序的，排序这个 nb 的过程如果不需要最好还是不要的好；</p>
</li>
<li><p>内存化：shuffle 的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招；</p>
</li>
<li><p>网络框架：netty 的性能据说要占优了；</p>
</li>
<li><p>本节点上的数据不走网络框架：对于本节点上的 map 输出，reduce 直接去读吧，不需要绕道网络框架。</p>
</li>
</ul>
<h3 id="Spark-Shuffle-后续优化方向"><a href="#Spark-Shuffle-后续优化方向" class="headerlink" title="Spark Shuffle 后续优化方向"></a><strong>Spark Shuffle 后续优化方向</strong></h3><p>Spark 作为 MapReduce 的进阶架构，对于 shuffle 过程已经是优化了的，特别是对于那些具有争议的步骤已经做了优化，但是 Spark 的 shuffle 对于我们来说在一些方面还是需要优化的。</p>
<ul>
<li><p>压缩：对数据进行压缩，减少写读数据量；</p>
</li>
<li><p>内存化：Spark 历史版本中是有这样设计的：map 写数据先把数据全部写到内存中，写完之后再把数据刷到磁盘上；考虑内存是紧缺资源，后来修改成把数据直接写到磁盘了；对于具有较大内存的集群来讲，还是尽量地往内存上写吧，内存放不下了再放磁盘。</p>
</li>
</ul>
<p>摘自: <a href="http://data.qq.com/article?id=543" target="_blank" rel="noopener">腾讯大数据博客</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tttmelody.github.io/2018/10/22/Spark-Parquet-file-split/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aron">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://gitee.com/Meldoy/image/raw/master/life/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/22/Spark-Parquet-file-split/" itemprop="url">Spark Parquet file split</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-22T20:14:43+08:00">
                2018-10-22
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/22/Spark-Parquet-file-split/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/10/22/Spark-Parquet-file-split/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/10/22/Spark-Parquet-file-split/" class="leancloud_visitors" data-flag-title="Spark Parquet file split">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>在实际使用 Spark + Parquet 的时候, 遇到了两个不解的地方:</p>
<ol>
<li>我们只有一个 Parquet 文件(小于 HDFS block size), 但是 Spark 在某个 stage 生成了4个 tasks 来处理.</li>
<li>4个 tasks 中只有一个 task 处理了所有数据, 其他几个都没有在处理数据.</li>
</ol>
<p>这两个问题牵涉到对于 Parquet, Spark 是如何来进行切分 partitions, 以及每个 partition 要处理哪部分数据的.</p>
<p><strong>先说结论</strong>, Spark 中, Parquet 是 splitable 的, 代码见<code>ParquetFileFormat#isSplitable</code>. 那会不会把数据切碎? 答案是不会, 因为是以 Spark row group 为最小单位切分 Parquet 的, 这也会导致一些 partitions 会没有数据, 极端情况下, 如果只有一个 row group 的话, partitions 再多, 也只会一个 partition 有数据.</p>
<p>接下来开始我们的源码之旅:</p>
<h3 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h3><h4 id="1-根据-Parquet-按文件大小切块生成-partitions"><a href="#1-根据-Parquet-按文件大小切块生成-partitions" class="headerlink" title="1. 根据 Parquet 按文件大小切块生成 partitions:"></a>1. 根据 Parquet 按文件大小切块生成 partitions:</h4><p>在 <code>FileSourceScanExec#createNonBucketedReadRDD</code> 中, 如果文件是 splitable 的, 会按照 maxSplitBytes 把文件切分, 最后生成的数量, 就是 RDD partition 的数量, 这个解释了<strong>不解1</strong>, 代码如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxSplitBytes = <span class="type">Math</span>.min(defaultMaxSplitBytes, <span class="type">Math</span>.max(openCostInBytes, bytesPerCore))</span><br><span class="line">logInfo(<span class="string">s"Planning scan with bin packing, max size: <span class="subst">$maxSplitBytes</span> bytes, "</span> +</span><br><span class="line">      <span class="string">s"open cost is considered as scanning <span class="subst">$openCostInBytes</span> bytes."</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> splitFiles = selectedPartitions.flatMap &#123; partition =&gt;</span><br><span class="line">  partition.files.flatMap &#123; file =&gt;</span><br><span class="line">    <span class="keyword">val</span> blockLocations = getBlockLocations(file)</span><br><span class="line">    <span class="keyword">if</span> (fsRelation.fileFormat.isSplitable(</span><br><span class="line">        fsRelation.sparkSession, fsRelation.options, file.getPath)) &#123;</span><br><span class="line">      (<span class="number">0</span>L until file.getLen by maxSplitBytes).map &#123; offset =&gt;</span><br><span class="line">        <span class="keyword">val</span> remaining = file.getLen - offset</span><br><span class="line">        <span class="keyword">val</span> size = <span class="keyword">if</span> (remaining &gt; maxSplitBytes) maxSplitBytes <span class="keyword">else</span> remaining</span><br><span class="line">        <span class="keyword">val</span> hosts = getBlockHosts(blockLocations, offset, size)</span><br><span class="line">        <span class="type">PartitionedFile</span>(</span><br><span class="line">          partition.values, file.getPath.toUri.toString, offset, size, hosts)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> hosts = getBlockHosts(blockLocations, <span class="number">0</span>, file.getLen)</span><br><span class="line">      <span class="type">Seq</span>(<span class="type">PartitionedFile</span>(</span><br><span class="line">        partition.values, file.getPath.toUri.toString, <span class="number">0</span>, file.getLen, hosts))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;.toArray.sortBy(_.length)(implicitly[<span class="type">Ordering</span>[<span class="type">Long</span>]].reverse)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> partitions = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">FilePartition</span>]</span><br><span class="line"><span class="keyword">val</span> currentFiles = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">PartitionedFile</span>]</span><br><span class="line"><span class="keyword">var</span> currentSize = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Close the current partition and move to the next. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">closePartition</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (currentFiles.nonEmpty) &#123;</span><br><span class="line">    <span class="keyword">val</span> newPartition =</span><br><span class="line">      <span class="type">FilePartition</span>(</span><br><span class="line">        partitions.size,</span><br><span class="line">        currentFiles.toArray.toSeq) <span class="comment">// Copy to a new Array.</span></span><br><span class="line">    partitions += newPartition</span><br><span class="line">  &#125;</span><br><span class="line">  currentFiles.clear()</span><br><span class="line">  currentSize = <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Assign files to partitions using "First Fit Decreasing" (FFD)</span></span><br><span class="line">splitFiles.foreach &#123; file =&gt;</span><br><span class="line">  <span class="keyword">if</span> (currentSize + file.length &gt; maxSplitBytes) &#123;</span><br><span class="line">    closePartition()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Add the given file to the current partition.</span></span><br><span class="line">  currentSize += file.length + openCostInBytes</span><br><span class="line">  currentFiles += file</span><br><span class="line">&#125;</span><br><span class="line">closePartition()</span><br><span class="line"></span><br><span class="line"><span class="keyword">new</span> <span class="type">FileScanRDD</span>(fsRelation.sparkSession, readFile, partitions)</span><br></pre></td></tr></table></figure>
<h4 id="2-使用-ParquetInputSplit-构造-reader"><a href="#2-使用-ParquetInputSplit-构造-reader" class="headerlink" title="2. 使用 ParquetInputSplit 构造 reader:"></a>2. 使用 ParquetInputSplit 构造 reader:</h4><p>在 <code>ParquetFileFormat#buildReaderWithPartitionValues</code> 实现中, 会使用 split 来初始化 reader, 并且根据配置可以把 reader 分为否是 vectorized 的, 关于 vectorized, 我会在日后再开一篇博文来介绍:</p>
<ul>
<li><code>vectorizedReader.initialize(split, hadoopAttemptContext)</code></li>
<li><code>reader.initialize(split, hadoopAttemptContext)</code></li>
</ul>
<p>关于 步骤2 在画外中还有更详细的代码, 但与本文的主流程关系不大, 这里先不表.</p>
<h4 id="3-划分-Parquet-的-row-groups-到不同的Spark-partitions-中去"><a href="#3-划分-Parquet-的-row-groups-到不同的Spark-partitions-中去" class="headerlink" title="3. 划分 Parquet 的 row groups 到不同的Spark partitions 中去"></a>3. 划分 Parquet 的 row groups 到不同的Spark partitions 中去</h4><p>在 步骤1 中根据文件大小均分了一些 partitions, 但不是所有这些 partitions 最后都会有数据. </p>
<p>接回 步骤2 中的 init, 在 <code>SpecificParquetRecordReaderBase#initialize</code> 中, 会在 <code>readFooter</code> 的时候传入一个 <code>RangeMetadataFilter</code>, 这个 filter 的range 是根据你的 split 的边界来的, 最后会用这个 range 来划定 row group 的归属:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    footer = readFooter(configuration, file, range(inputSplit.getStart(), inputSplit.getEnd()));</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Parquet 的<code>ParquetFileReader#readFooter</code>方法会用到<code>ParquetMetadataConverter#converter.readParquetMetadata(f, filter);</code>, 这个<code>readParquetMetadata</code> 使用了一个访问者模式, 而其中对于<code>RangeMetadataFilter</code>的处理是:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FileMetaData <span class="title">visit</span><span class="params">(RangeMetadataFilter filter)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> filterFileMetaDataByMidpoint(readFileMetaData(from), filter);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>终于到了最关键的切分的地方, 最关键的就是这一段, 谁拥有这个 row group的中点, 谁就可以处理这个 row group. </p>
<p>现在假设我们有一个40m 的文件, 只有一个 row group, 10m 一分, 那么将会有4个 partitions, 但是只有一个 partition 会占有这个 row group 的中点, 所以也只有这一个 partition 会有数据.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> midPoint = startIndex + totalSize / <span class="number">2</span>;</span><br><span class="line"><span class="keyword">if</span> (filter.contains(midPoint)) &#123;</span><br><span class="line">  newRowGroups.add(rowGroup);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>完整代码如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> FileMetaData <span class="title">filterFileMetaDataByMidpoint</span><span class="params">(FileMetaData metaData, RangeMetadataFilter filter)</span> </span>&#123;</span><br><span class="line">  List&lt;RowGroup&gt; rowGroups = metaData.getRow_groups();</span><br><span class="line">  List&lt;RowGroup&gt; newRowGroups = <span class="keyword">new</span> ArrayList&lt;RowGroup&gt;();</span><br><span class="line">  <span class="keyword">for</span> (RowGroup rowGroup : rowGroups) &#123;</span><br><span class="line">    <span class="keyword">long</span> totalSize = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">long</span> startIndex = getOffset(rowGroup.getColumns().get(<span class="number">0</span>));</span><br><span class="line">    <span class="keyword">for</span> (ColumnChunk col : rowGroup.getColumns()) &#123;</span><br><span class="line">      totalSize += col.getMeta_data().getTotal_compressed_size();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">long</span> midPoint = startIndex + totalSize / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">if</span> (filter.contains(midPoint)) &#123;</span><br><span class="line">      newRowGroups.add(rowGroup);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  metaData.setRow_groups(newRowGroups);</span><br><span class="line">  <span class="keyword">return</span> metaData;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="画外"><a href="#画外" class="headerlink" title="画外:"></a>画外:</h4><p>步骤2 中的代码其实是 spark 正儿八经如何读文件的代码, 最后返回一个<code>FileScanRDD</code>, 也很值得顺路看一下, 完整代码如下:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">(file: <span class="type">PartitionedFile</span>) =&gt; &#123;</span><br><span class="line">  assert(file.partitionValues.numFields == partitionSchema.size)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fileSplit =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">FileSplit</span>(<span class="keyword">new</span> <span class="type">Path</span>(<span class="keyword">new</span> <span class="type">URI</span>(file.filePath)), file.start, file.length, <span class="type">Array</span>.empty)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> split =</span><br><span class="line">    <span class="keyword">new</span> org.apache.parquet.hadoop.<span class="type">ParquetInputSplit</span>(</span><br><span class="line">      fileSplit.getPath,</span><br><span class="line">      fileSplit.getStart,</span><br><span class="line">      fileSplit.getStart + fileSplit.getLength,</span><br><span class="line">      fileSplit.getLength,</span><br><span class="line">      fileSplit.getLocations,</span><br><span class="line">      <span class="literal">null</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> attemptId = <span class="keyword">new</span> <span class="type">TaskAttemptID</span>(<span class="keyword">new</span> <span class="type">TaskID</span>(<span class="keyword">new</span> <span class="type">JobID</span>(), <span class="type">TaskType</span>.<span class="type">MAP</span>, <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">  <span class="keyword">val</span> hadoopAttemptContext =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">TaskAttemptContextImpl</span>(broadcastedHadoopConf.value.value, attemptId)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Try to push down filters when filter push-down is enabled.</span></span><br><span class="line">  <span class="comment">// Notice: This push-down is RowGroups level, not individual records.</span></span><br><span class="line">  <span class="keyword">if</span> (pushed.isDefined) &#123;</span><br><span class="line">    <span class="type">ParquetInputFormat</span>.setFilterPredicate(hadoopAttemptContext.getConfiguration, pushed.get)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> parquetReader = <span class="keyword">if</span> (enableVectorizedReader) &#123;</span><br><span class="line">    <span class="keyword">val</span> vectorizedReader = <span class="keyword">new</span> <span class="type">VectorizedParquetRecordReader</span>()</span><br><span class="line">    vectorizedReader.initialize(split, hadoopAttemptContext)</span><br><span class="line">    logDebug(<span class="string">s"Appending <span class="subst">$partitionSchema</span> <span class="subst">$&#123;file.partitionValues&#125;</span>"</span>)</span><br><span class="line">    vectorizedReader.initBatch(partitionSchema, file.partitionValues)</span><br><span class="line">    <span class="keyword">if</span> (returningBatch) &#123;</span><br><span class="line">      vectorizedReader.enableReturningBatches()</span><br><span class="line">    &#125;</span><br><span class="line">    vectorizedReader</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    logDebug(<span class="string">s"Falling back to parquet-mr"</span>)</span><br><span class="line">    <span class="comment">// ParquetRecordReader returns UnsafeRow</span></span><br><span class="line">    <span class="keyword">val</span> reader = pushed <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(filter) =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ParquetRecordReader</span>[<span class="type">UnsafeRow</span>](</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ParquetReadSupport</span>,</span><br><span class="line">          <span class="type">FilterCompat</span>.get(filter, <span class="literal">null</span>))</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ParquetRecordReader</span>[<span class="type">UnsafeRow</span>](<span class="keyword">new</span> <span class="type">ParquetReadSupport</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    reader.initialize(split, hadoopAttemptContext)</span><br><span class="line">    reader</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> iter = <span class="keyword">new</span> <span class="type">RecordReaderIterator</span>(parquetReader)</span><br><span class="line">  <span class="type">Option</span>(<span class="type">TaskContext</span>.get()).foreach(_.addTaskCompletionListener(_ =&gt; iter.close()))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// UnsafeRowParquetRecordReader appends the columns internally to avoid another copy.</span></span><br><span class="line">  <span class="keyword">if</span> (parquetReader.isInstanceOf[<span class="type">VectorizedParquetRecordReader</span>] &amp;&amp;</span><br><span class="line">      enableVectorizedReader) &#123;</span><br><span class="line">    iter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">InternalRow</span>]]</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> fullSchema = requiredSchema.toAttributes ++ partitionSchema.toAttributes</span><br><span class="line">    <span class="keyword">val</span> joinedRow = <span class="keyword">new</span> <span class="type">JoinedRow</span>()</span><br><span class="line">    <span class="keyword">val</span> appendPartitionColumns = <span class="type">GenerateUnsafeProjection</span>.generate(fullSchema, fullSchema)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// This is a horrible erasure hack...  if we type the iterator above, then it actually check</span></span><br><span class="line">    <span class="comment">// the type in next() and we get a class cast exception.  If we make that function return</span></span><br><span class="line">    <span class="comment">// Object, then we can defer the cast until later!</span></span><br><span class="line">    <span class="keyword">if</span> (partitionSchema.length == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// There is no partition columns</span></span><br><span class="line">      iter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">InternalRow</span>]]</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      iter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">InternalRow</span>]]</span><br><span class="line">        .map(d =&gt; appendPartitionColumns(joinedRow(d, file.partitionValues)))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这个返回的<code>(PartitionedFile) =&gt; Iterator[InternalRow]</code>, 是在<code>FileSourceScanExec#inputRDD</code>用的<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">lazy</span> <span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> readFile: (<span class="type">PartitionedFile</span>) =&gt; <span class="type">Iterator</span>[<span class="type">InternalRow</span>] =</span><br><span class="line">    relation.fileFormat.buildReaderWithPartitionValues(</span><br><span class="line">      sparkSession = relation.sparkSession,</span><br><span class="line">      dataSchema = relation.dataSchema,</span><br><span class="line">      partitionSchema = relation.partitionSchema,</span><br><span class="line">      requiredSchema = requiredSchema,</span><br><span class="line">      filters = pushedDownFilters,</span><br><span class="line">      options = relation.options,</span><br><span class="line">      hadoopConf = relation.sparkSession.sessionState.newHadoopConfWithOptions(relation.options))</span><br><span class="line"></span><br><span class="line">  relation.bucketSpec <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(bucketing) <span class="keyword">if</span> relation.sparkSession.sessionState.conf.bucketingEnabled =&gt;</span><br><span class="line">      createBucketedReadRDD(bucketing, readFile, selectedPartitions, relation)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      createNonBucketedReadRDD(readFile, selectedPartitions, relation)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>FileScanRDD<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FileScanRDD</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    @transient private val sparkSession: <span class="type">SparkSession</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    readFunction: (<span class="type">PartitionedFile</span></span>) <span class="title">=&gt;</span> <span class="title">Iterator</span>[<span class="type">InternalRow</span>],</span></span><br><span class="line"><span class="class">    <span class="title">@transient</span> <span class="title">val</span> <span class="title">filePartitions</span></span>: <span class="type">Seq</span>[<span class="type">FilePartition</span>])</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">RDD</span>[<span class="type">InternalRow</span>](sparkSession.sparkContext, <span class="type">Nil</span>) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">RDDPartition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> files = split.asInstanceOf[<span class="type">FilePartition</span>].files.toIterator</span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> currentFile: <span class="type">PartitionedFile</span> = <span class="literal">null</span> <span class="comment">// 根据 currentFile = files.next() 来的, 具体实现我就不贴了 有兴趣的可以自己看下.</span></span><br><span class="line">    ...</span><br><span class="line">    readFunction(currentFile)</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>提升一个 Parquet 中的 row group 中的行数阈值, 籍此提示 Spark 并行度.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tttmelody.github.io/2018/10/16/Spark-map-vs-mapPartitions/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aron">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://gitee.com/Meldoy/image/raw/master/life/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/16/Spark-map-vs-mapPartitions/" itemprop="url">Spark map vs mapPartitions</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-16T10:43:54+08:00">
                2018-10-16
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/16/Spark-map-vs-mapPartitions/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/10/16/Spark-map-vs-mapPartitions/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/10/16/Spark-map-vs-mapPartitions/" class="leancloud_visitors" data-flag-title="Spark map vs mapPartitions">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>官方定义:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new RDD by applying a function to all elements of this RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new RDD by applying a function to each partition of this RDD.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * `preservesPartitioning` indicates whether the input function preserves the partitioner, which</span></span><br><span class="line"><span class="comment"> * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(</span><br><span class="line">    <span class="keyword">this</span>,</span><br><span class="line">    (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(iter),</span><br><span class="line">    preservesPartitioning)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到的是 mapPartitions 需要的函数参数传入的是一个 iter, 返回的也是一个 iter, 而 map 仅仅是一个元素.</p>
<p>假设我们有 10k 个元素, 10个 partitions, 数据均匀分布:</p>
<ol>
<li>map:调用10k 次 map 方法</li>
<li>mapPartitions:调用10次 mapPartitions 方法, 每次传入1k个(一个 partition 的数据量)进行计算. 结果先存到 memory 中, 直到可以返回.</li>
<li>flatMap在 单个元素(map)上工作，生成结果是多个元素(mapPartitions)</li>
</ol>
<p>结论:mapPartitions 转换比 map 快，因为它调用你的函数 一次/分区，而不是 一次/元素, 像有一些高开销的 init 的时候, 如数据库连接, 使用 mapPartitions, 每个分区就只需要一次 init.</p>
<p>来看一个使用 mapPartitions 报错的例子, 运行这段代码, 会报 already closed exception, 因为 spark 的计算都是 lazy 的, 下面的 partition.map 到真的触发计算的时候, conn 已经 close 了.解决方法就是 <code>val newPartition = partition.map(...}).toList</code> 触发计算.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> newDF = myDF.mapPartitions(</span><br><span class="line">  partition =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> conn = <span class="keyword">new</span> <span class="type">DbConnection</span></span><br><span class="line">    <span class="keyword">val</span> newPartition = partition.map(record =&gt; &#123; readMatchingFromDB(record, connection) &#125;)</span><br><span class="line">    conn.close()</span><br><span class="line">    newPartition</span><br><span class="line">  &#125;).toDF()</span><br></pre></td></tr></table></figure>
<p>map mapPartitions 对外部引用的更新都没法作用出来: 下面的 flag 还是0.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> flag = <span class="number">0</span></span><br><span class="line"><span class="keyword">val</span> test = rdd.map &#123;</span><br><span class="line">  row =&gt; flag += <span class="number">1</span></span><br><span class="line">&#125;.collect()</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tttmelody.github.io/2018/10/13/Learning-Scala/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aron">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://gitee.com/Meldoy/image/raw/master/life/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/13/Learning-Scala/" itemprop="url">Learning Scala</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-13T17:22:36+08:00">
                2018-10-13
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/13/Learning-Scala/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/10/13/Learning-Scala/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/10/13/Learning-Scala/" class="leancloud_visitors" data-flag-title="Learning Scala">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h3><p>a Scala array is a mutable sequence of objects that all share the same type. 当然 array 本身是不可变的, 比如 length, 引用, 但是里面的元素可以变.</p>
<p>Scala achieves a conceptual simplicity by treating everything, from arrays to expressions, as objects with methods.</p>
<p>No special case! Moreover, this uniformity does not incur a significant performance cost. The Scala compiler uses Java arrays, primitive types, and native arithmetic where possible in the compiled code.</p>
<p>Scala has fewer special cases than Java. Arrays are simply instances of classes like any other class in Scala. When you apply parentheses surrounding one or more values to a variable, Scala will transform the code into an invocation of a method named apply on that variable. So greetStrings(i) gets transformed into greetStrings.apply(i). Thus accessing an element of an array in Scala is simply a method call like any other. This principle is not restricted to arrays: any application of an object to some arguments in parentheses will be transformed to an apply method call. Of course this will compile only if that type of object actually defines an apply method. So it’s not a special case; it’s a general rule.</p>
<p>greetStrings(0) = “Hello” &lt;=&gt; greetStrings.update(0, “Hello”)</p>
<p>val numNames = Array(“zero”, “one”, “two”) &lt;=&gt; val numNames2 = Array.apply(“zero”, “one”, “two”)</p>
<p>Array companion object. 伴生对象. 定义在 <code>object Array</code> 中的, 静态. 而具体实例的.apply() 是在<code>class Array[T]</code>中的.</p>
<h3 id="List"><a href="#List" class="headerlink" title="List"></a>List</h3><p>Array 是 mutable sequence of objects, List 是 immutable sequence of objects.</p>
<p>init:<code>val l = List(1, 2, 3)`</code></p>
<p>concatenation:<code>list1 ::: list2</code>, <code>List(1, 2) ::: List(3, 4) = List(3, 4).:::(List(1, 2)) = List(1, 2, 3, 4)</code></p>
<p>append to head:<code>1 :: List(2, 3)</code> &lt;=&gt; List(1, 2, 3)</p>
<p>If the method name ends in a colon, the method is invoked on the right operand. <code>1 :: twoThree</code> &lt;=&gt; <code>twoThree.::(1)</code>, <code>1 :: List(2, 3) = List(2, 3).::(1) = List(1, 2, 3)</code></p>
<p>可以看看这个:<code>final case class ::[B](override val head: B, private[scala] var tl: List[B]) extends List[B]</code>: A non empty list characterized by a head and a tail.</p>
<p>Class List does offer an “append” operation, because the time it takes to append to a list grows linearly with the size of the list, whereas prepending with :: takes constant time.</p>
<p>you can use a ListBuffer, a mutable list that does offer an append operation, and when you’re done call toList.</p>
<p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/11591539421591_.pic_hd.jpg" alt=""></p>
<p><code>def printArgs(args: Array[String]): Unit = { args.foreach(println) }</code> 即使这样也是有副作用的. 改成这样更 functional<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def formatArgs(args: Array[String]) = args.mkString(&quot;\n&quot;)</span><br><span class="line">println(formatArgs(args))</span><br></pre></td></tr></table></figure></p>
<p>If a function isn’t returning any interesting value(Unit), the only way that function can make a difference in the world is through some kind of side effect.</p>
<p>Every useful program is likely to have side effects of some form; otherwise, it wouldn’t be able to provide value to the outside world. 但是尽量减少side effects, 让你的代码更容易的去测试.(void 方法 没法测)</p>
<p><code>val lines = Source.fromFile(args(0)).getLines().toList</code>, getLines()返回一个 iter, toList 的话, 可以想访问几遍元素访问几遍元素, 代价是进入到内存中区.</p>
<h3 id="折叠"><a href="#折叠" class="headerlink" title="折叠"></a>折叠</h3><p>奥义:<strong>op( op( … op(x_1, x_2) …, x_{n-1}), x_n)</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fold</span><br><span class="line">foldLeft</span><br><span class="line">foldRight</span><br><span class="line">reduce</span><br><span class="line">reduceLeft</span><br><span class="line">reduceRight</span><br></pre></td></tr></table></figure>
<h4 id="fold系列"><a href="#fold系列" class="headerlink" title="fold系列"></a>fold系列</h4><p>fold 函数是不讲究折叠顺序的，适合应用在并发计算中；而 foldLeft 和 foldRight 按元素顺序折叠，一个是由左开始，一个是由右开始。</p>
<p><code>scala&gt; List(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;).foldLeft(0)(_ + _.toInt)</code>, 也可以<code>foldLeft(0)((a, b) =&gt; a + b.toInt)</code></p>
<p>坑:<code>List(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;).fold(0) {_ + _.toInt}</code> 抛错<code>value toInt is not a member of Any</code>, 因为顺序可能变成了:<code>(0 + &quot;2&quot;.toInt) + (&quot;1&quot; + &quot;3&quot;.toInt).toInt</code>, 而期望是:<code>((0 + &quot;1&quot;.toInt) + &quot;2&quot;.toInt) + &quot;3&quot;.toInt</code></p>
<p>foldLeft 和 foldRight 的速写法分别为：/:() 和:()：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(0 /: List(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;)) &#123;(sum, elem) =&gt; sum + elem.toInt&#125;</span><br><span class="line">(List(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;) :\ 0) &#123;_.toInt + _&#125;</span><br></pre></td></tr></table></figure>
<h4 id="reduce-系列"><a href="#reduce-系列" class="headerlink" title="reduce 系列"></a>reduce 系列</h4><p><strong>op( op( … op(x_1, x_2) …, x_{n-1}), x_n)</strong></p>
<p>reduce 系列与 fold 系列的区别在于：reduce 的返回值类型必须和列表的元素类型一致或是其父类，而 fold 无这种限制；fold 必须设定折叠初始值，reduce 不需。来看看：</p>
<p><code>List(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;).reduceLeft((a, b) =&gt; a.toInt + b.toInt)</code>, 报错:<code>error: type mismatch; found : Int, required: String</code>, 要改成<code>List(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;).map(_.toInt).reduceLeft(_ + _)</code></p>
<p>reduce 与 reduceLeft、reduceRight 的区别也类似于 fold 系列。reduce 的折叠结合是无序的，因此 reduce 也可以用于并行计算。可以把 reduce 当成 fold 的一个特殊版。</p>
<p>看下官方的说法:reduceLeft:Applies a binary operator to all elements of this $coll, going left to right. <code>op( op( ... op(x_1, x_2) ..., x_{n-1}), x_n)</code></p>
<h3 id="Classes-and-Objects"><a href="#Classes-and-Objects" class="headerlink" title="Classes and Objects"></a>Classes and Objects</h3><p>scala 里默认的 access level 是 public</p>
<p>方法的 args 都是 val 的.</p>
<p>不显示的用 return, This philosophy will encourage you to make methods quite small, 减少 multi return.</p>
<p>A method that is executed only for its side effects is known as a procedure.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x</span><br><span class="line">+ y</span><br><span class="line"></span><br><span class="line">会被认为是两个 statement, 可以这样写</span><br><span class="line"></span><br><span class="line">(x</span><br><span class="line">+ y)</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">x +</span><br><span class="line">y</span><br><span class="line"></span><br><span class="line">it is a common <span class="type">Scala</span> style to put the operators at the end of the line instead of the beginning:</span><br></pre></td></tr></table></figure>
<h4 id="THE-RULES-OF-SEMICOLON-INFERENCE"><a href="#THE-RULES-OF-SEMICOLON-INFERENCE" class="headerlink" title="THE RULES OF SEMICOLON INFERENCE"></a>THE RULES OF SEMICOLON INFERENCE</h4><p>In short, a line ending is treated as a semicolon unless one of the following conditions is true:</p>
<ol>
<li>The line in question ends in a word that would not be legal as the end of a statement, such as a period or an infix operator.</li>
<li>The next line begins with a word that cannot start a statement.</li>
<li>The line ends while inside parentheses (…) or brackets […], because these cannot contain multiple statements anyway.</li>
</ol>
<h4 id="SINGLETON-OBJECTS"><a href="#SINGLETON-OBJECTS" class="headerlink" title="SINGLETON OBJECTS"></a>SINGLETON OBJECTS</h4><p>Scala 不能有静态成员变量. 但是Scala has singleton objects.</p>
<p>When a singleton object shares the same name with a class, it is called that class’s companion object.</p>
<p>You must define both the class and its companion object in the same source file.</p>
<p>In particular, a singleton object is initialized the first time some code accesses it.</p>
<p>A singleton object that does not share the same name with a companion class is called a standalone object.</p>
<h3 id="Depp-in-inherit"><a href="#Depp-in-inherit" class="headerlink" title="Depp in inherit"></a>Depp in inherit</h3><p><strong>Point</strong></p>
<p>A method is abstract if it does not have an implementation (i.e., no equals sign or body). 不用显示写明是 abstract.</p>
<p>use a parameterless method whenever there are no parameters and the method accesses mutable state only by reading fields of the containing object (in particular, it does not change mutable state). This convention supports the uniform access principle,[1] which says that client code should not be affected by a decision to implement an attribute as a field or method.</p>
<p>使用 method 省内存, 使用 filed 会快, 因为类初始化的时候就算好了, and that usage profile might change over time. The point is that clients of theElement class should not be affected when its internal implementation changes.</p>
<p>Element should not need to be rewritten if a field of that class gets changed into an access function, so long as the access function is pure (i.e., it does not have any side effects and does not depend on mutable state). The client should not need to care either way.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">abstract class Element &#123;</span><br><span class="line">  def contents: Array[String]</span><br><span class="line"></span><br><span class="line">  def height: Int = contents.length</span><br><span class="line">  def width: Int = if (height == 0) 0 else contents(0).length</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">abstract class Element &#123;</span><br><span class="line">  def contents: Array[String]</span><br><span class="line"></span><br><span class="line">  val height = contents.length</span><br><span class="line">  val width = if (height == 0) 0 else contents(0).length</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You can also leave off the empty parentheses on an invocation of any function that takes no arguments.</p>
<p><strong>On the other hand, you should never define a method that has side-effects without parentheses, because invocations of that method would then look like a field selection.</strong></p>
<h4 id="OVERRIDING-METHODS-AND-FIELDS"><a href="#OVERRIDING-METHODS-AND-FIELDS" class="headerlink" title="OVERRIDING METHODS AND FIELDS"></a>OVERRIDING METHODS AND FIELDS</h4><p>This makes it possible for a field to override a parameterless method.(parameterless method 在子类里可以用 field 来 override)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class ArrayElement(conts: Array[String]) extends Element &#123; </span><br><span class="line">  val contents: Array[String] = conts </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Scala it is forbidden to define a field and method with the same name in the same class</p>
<h4 id="INVOKING-SUPERCLASS-CONSTRUCTORS"><a href="#INVOKING-SUPERCLASS-CONSTRUCTORS" class="headerlink" title="INVOKING SUPERCLASS CONSTRUCTORS"></a>INVOKING SUPERCLASS CONSTRUCTORS</h4><p><code>class LineElement(s: String) extends ArrayElement(Array(s))</code> 尼玛 这写法略丑啊…</p>
<p>if you add new members to base classes (which we usually call superclasses) in a class hierarchy, you risk breaking client code.</p>
<p>组合优于继承, 继承 suffers from the fragile base class problem, base class 会 break client 代码. 使用继承的时候 常常问 is a. 然后client代码里子类有被转成父类的需要吗.</p>
<p>If one of the two operand arrays is longer than the other, zip will drop the remaining elements, <code>Array(1, 2, 3) zip Array(&quot;a&quot;, &quot;b&quot;)</code> &lt;=&gt; <code>Array((1, &quot;a&quot;), (2, &quot;b&quot;))</code></p>
<p>当你的循环结束，将会返回被 yield 的值的一个集合。集合的类型和被迭代的集合类型一样，如果迭代的是 list，返回 list, 如果是 map 就返回 map<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; for (i &lt;- 1 to 5) yield &#123;println(i); i * 3&#125;</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">res1: scala.collection.immutable.IndexedSeq[Int] = Vector(3, 6, 9, 12, 15)</span><br></pre></td></tr></table></figure></p>
<h3 id="Type-Parameterization"><a href="#Type-Parameterization" class="headerlink" title="Type Parameterization"></a>Type Parameterization</h3><h4 id="协变-逆变-variance-annotations"><a href="#协变-逆变-variance-annotations" class="headerlink" title="协变/逆变 (variance annotations)"></a>协变/逆变 (variance annotations)</h4><p>S 是 T 的子类, 那么 Queue[S] 是 Queue[T] 的子类吗, 如果是, 你可以说 Queue 是协变的(covariant). 这样一个方法的参数是Queue[AnyRef], 你可以传入任何Queue[…].</p>
<p>scala 默认是 nonvariant/rigid subtyping. </p>
<p><code>trait Queue[+T] { ... }</code> 定义成这样是协变(covariant)</p>
<p><code>trait Queue[-T] { ... }</code> 定义成这样是逆变(contravariant)</p>
<p>逆变是如果 T 是 S 的子类, Queue[S] 是 Queue[T] 的子类</p>
<p>在 java 中, 数组被当成 covariant 对待. covariant 会有这种问题, 编译可以过, 但是运行出错, 有了泛型后, array 的 covariant 不再 necessary:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">String[] a1 = &#123; <span class="string">"abc"</span> &#125;; </span><br><span class="line">Object[] a2 = a1; </span><br><span class="line">a2[<span class="number">0</span>] = <span class="keyword">new</span> Integer(<span class="number">17</span>); </span><br><span class="line">String s = a1[<span class="number">0</span>];</span><br></pre></td></tr></table></figure>
<p>Scala 不会把数组当做可以 covariant 的.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a1 = Array(<span class="string">"abc"</span>)</span><br><span class="line">scala&gt; val a2: Array[Any] = a1 </span><br><span class="line">&lt;console&gt;:<span class="number">8</span>: error: type mismatch; </span><br><span class="line">  found : Array[String] </span><br><span class="line">  required: Array[Any] </span><br><span class="line">        val a2: Array[Any] = a1</span><br></pre></td></tr></table></figure></p>
<p>如果要用, 要这样用:<code>scala&gt; val a2: Array[Object] = a1.asInstanceOf[Array[Object]]</code></p>
<p>对于纯函数式的语言来说, 协变可能是自然的, 但是只要的你参数是泛型, 都有可能有上面的问题:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StrangeIntQueue</span> <span class="keyword">extends</span> <span class="title">Queue</span>[<span class="type">Int</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">enqueue</span></span>(x: <span class="type">Int</span>) = &#123;</span><br><span class="line">    println(math.sqrt(x))</span><br><span class="line">    <span class="keyword">super</span>.enqueue(x)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> x: <span class="type">Queue</span>[<span class="type">Any</span>] = <span class="keyword">new</span> <span class="type">StrangeIntQueue</span> </span><br><span class="line">x.enqueue(<span class="string">"abc"</span>)</span><br></pre></td></tr></table></figure></p>
<p>用 + 注解的类型不允许作用于作为 方法参数 的类型, 像 setter 方法:<code>def x_=(y:T)</code> 这个 T 类型不能是协变的</p>
<h4 id="LOWER-BOUNDS"><a href="#LOWER-BOUNDS" class="headerlink" title="LOWER BOUNDS"></a>LOWER BOUNDS</h4><p><code>U &gt;: T</code> 定义了 U 的下界为 T. 这样 U 必须是 T 的超类型. 可以用这种方式绕过上面说的 方法参数的类型不能是协变的<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Queue</span>[+<span class="type">T</span>](<span class="params">private val leading: <span class="type">List</span>[<span class="type">T</span>], private val trailing: <span class="type">List</span>[<span class="type">T</span>]</span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">enqueue</span></span>[<span class="type">U</span> &gt;: <span class="type">T</span>](x: <span class="type">U</span>) = <span class="keyword">new</span> <span class="type">Queue</span>[<span class="type">U</span>](leading, x :: trailing)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>假设我们有 Fruit 类和两个子类: Apple 和 Orange. 可以对Queue[Apple] 追加一个 Orange, 其结果是一个 Queue[Fruit]</p>
<p>超类型和子类型的关系是反身(reflexive)的, 一个类型同时是自己的超类型和子类型, 所以上面我们仍可以把一个 T 传入 enqueue.</p>
<h4 id="逆变"><a href="#逆变" class="headerlink" title="逆变"></a>逆变</h4><p>里氏代换原则:在任何需要类型 U 的值的地方, 都可以用 T 的值替换. 那么就可以安全的假定类型 T 是类型 U 的子类型.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Function1</span>[-<span class="type">S</span>, +<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(x: <span class="type">S</span>): <span class="type">T</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tttmelody.github.io/2018/10/06/Druid-Storage-原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aron">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://gitee.com/Meldoy/image/raw/master/life/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/06/Druid-Storage-原理/" itemprop="url">Druid Storage 原理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-06T20:49:18+08:00">
                2018-10-06
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/06/Druid-Storage-原理/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/10/06/Druid-Storage-原理/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/10/06/Druid-Storage-原理/" class="leancloud_visitors" data-flag-title="Druid Storage 原理">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>转载自<a href="https://blog.bcmeng.com/post/druid-storage.html" target="_blank" rel="noopener">编程小梦</a>, 该博主一系列文章质量都非常高</p>
<h3 id="What-is-Druid"><a href="#What-is-Druid" class="headerlink" title="What is Druid"></a>What is Druid</h3><p>Druid 是一个开源的实时 OLAP 系统，可以对超大规模数据提供亚秒级查询，其具有以下特点：</p>
<ol>
<li>列式存储</li>
<li>倒排索引 （基于 Bitmap 实现）</li>
<li>分布式的 Shared-Nothing 架构 （高可用，易扩展是 Druid 的设计目标）</li>
<li>实时摄入 （数据被 Druid 实时摄入后便可以立即查询）</li>
</ol>
<h3 id="Why-Druid"><a href="#Why-Druid" class="headerlink" title="Why Druid"></a>Why Druid</h3><p>为了能够提取利用大数据的商业价值，我们必然需要对数据进行分析，尤其是多维分析， 但是在几年前，整个业界并没有一款很好的 OLAP 工具，各种多维分析的方式如下图所示：</p>
<p><img src="http://aron-blog-image.oss-cn-hangzhou.aliyuncs.com/18-10-6/55283969.jpg" alt="屏幕快照 2017-10-31 下午8.27.50.png-1080.8kB"></p>
<p>其中直接基于 Hive，MR，Spark 的方式查询速度一般十分慢，并发低；而传统的关系型数据库无法支撑大规模数据；以 HBase 为代表的 NoSQL 数据库也无法提供高效的过滤，聚合能力。正因为现有工具有着各种各样的痛点，Druid 应运而生，以下几点自然是其设计目标：</p>
<ol>
<li>快速查询</li>
<li>可以支撑大规模数据集</li>
<li>高效的过滤和聚合</li>
<li>实时摄入</li>
</ol>
<h3 id="Druid-架构"><a href="#Druid-架构" class="headerlink" title="Druid 架构"></a>Druid 架构</h3><p><img src="http://aron-blog-image.oss-cn-hangzhou.aliyuncs.com/18-10-6/98456054.jpg" alt="image.png-181kB"></p>
<p>Druid 的整体架构如上图所示，其中主要有 3 条路线：</p>
<ol>
<li><p>实时摄入的过程： 实时数据会首先按行摄入 Real-time Nodes，Real-time Nodes 会先将每行的数据加入到 1 个 map 中，等达到一定的行数或者大小限制时，Real-time Nodes 就会将内存中的 map 持久化到磁盘中，Real-time Nodes 会按照 segmentGranularity 将一定时间段内的小文件 merge 为一个大文件，生成 Segment，然后将 Segment 上传到 Deep Storage（HDFS，S3）中，Coordinator 知道有 Segment 生成后，会通知相应的 Historical Node 下载对应的 Segment，并负责该 Segment 的查询。</p>
</li>
<li><p>离线摄入的过程： 离线摄入的过程比较简单，就是直接通过 MR job 生成 Segment，剩下的逻辑和实时摄入相同：</p>
</li>
<li><p>用户查询过程： 用户的查询都是直接发送到 Broker Node，Broker Node 会将查询分发到 Real-time 节点和 Historical 节点，然后将结果合并后返回给用户。</p>
</li>
</ol>
<p>各节点的主要职责如下：</p>
<h4 id="Historical-Nodes"><a href="#Historical-Nodes" class="headerlink" title="Historical Nodes"></a>Historical Nodes</h4><p>Historical 节点是整个 Druid 集群的骨干，主要负责加载不可变的 segment，并负责 Segment 的查询（注意，Segment 必须加载到 Historical 的内存中才可以提供查询）。Historical 节点是无状态的，所以可以轻易的横向扩展和快速恢复。Historical 节点 load 和 un-load segment 是依赖 ZK 的，但是即使 ZK 挂掉，Historical 依然可以对已经加载的 Segment 提供查询，只是不能再 load 新 segment，drop 旧 segment。</p>
<h4 id="Broker-Nodes"><a href="#Broker-Nodes" class="headerlink" title="Broker Nodes"></a>Broker Nodes</h4><p>Broker 节点是 Druid 查询的入口，主要负责查询的分发和 Merge。 之外，Broker 还会对不可变的 Segment 的查询结果进行 LRU 缓存。</p>
<h4 id="Coordinator-Nodes"><a href="#Coordinator-Nodes" class="headerlink" title="Coordinator Nodes"></a>Coordinator Nodes</h4><p>Coordinator 节点主要负责 Segment 的管理。Coordinator 节点会通知 Historical 节点加载新 Segment，删除旧 Segment，复制 Segment，以及 Segment 间的复杂均衡。</p>
<p>Coordinator 节点依赖 ZK 确定 Historical 的存活和集群 Segment 的分布。</p>
<h4 id="Real-time-Node"><a href="#Real-time-Node" class="headerlink" title="Real-time Node"></a>Real-time Node</h4><p>实时节点主要负责数据的实时摄入，实时数据的查询，将实时数据转为 Segment，将 Segment Hand off 给 Historical 节点。</p>
<h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h4><p>Druid 依赖 ZK 实现服务发现，数据拓扑的感知，以及 Coordinator 的选主。</p>
<h4 id="Metadata-Storage"><a href="#Metadata-Storage" class="headerlink" title="Metadata Storage"></a>Metadata Storage</h4><p>Metadata storage（Mysql） 主要用来存储 Segment 和配置的元数据。当有新 Segment 生成时，就会将 Segment 的元信息写入 metadata store, Coordinator 节点会监控 Metadata store 从而知道何时 load 新 Segment，何时 drop 旧 Segment。注意，查询时不会涉及 Metadata store。</p>
<h4 id="Deep-Storage"><a href="#Deep-Storage" class="headerlink" title="Deep Storage"></a>Deep Storage</h4><p>Deep storage (S3 and HDFS) 是作为 Segment 的永久备份，查询时同样不会涉及 Deep storage。</p>
<h3 id="Column"><a href="#Column" class="headerlink" title="Column"></a>Column</h3><p><img src="http://aron-blog-image.oss-cn-hangzhou.aliyuncs.com/18-10-6/34009295.jpg" alt="屏幕快照 2017-10-27 下午3.45.05.png-278kB"></p>
<p>Druid 中的列主要分为 3 类：时间列，维度列，指标列。Druid 在数据摄入和查询时都依赖时间列，这也是合理的，因为多维分析一般都带有时间维度。维度和指标是 OLAP 系统中常见的概念，维度主要是事件的属性，在查询时一般用来 filtering 和 group by，指标是用来聚合和计算的，一般是数值类型，像 count,sum，min，max 等。</p>
<p>Druid 中的维度列支持 String，Long，Float，不过只有 String 类型支持倒排索引；指标列支持 Long，Float，Complex， 其中 Complex 指标包含 HyperUnique，Cardinality，Histogram，Sketch 等复杂指标。强类型的好处是可以更好的对每 1 列进行编码和压缩， 也可以保证数据索引的高效性和查询性能。</p>
<h3 id="Segment"><a href="#Segment" class="headerlink" title="Segment"></a>Segment</h3><p>前面提到过，Druid 中会按时间段生成不可变的带倒排索引的列式文件，这个文件就称之为 Segment，Segment 是 Druid 中数据存储、复制、均衡、以及计算的基本单元， Segment 由 dataSource_beginTime_endTime_version_shardNumber 唯一标识，1 个 segment 一般包含 5–10 million 行记录，大小一般在 300~700mb。</p>
<h3 id="Segment-的存储格式"><a href="#Segment-的存储格式" class="headerlink" title="Segment 的存储格式"></a>Segment 的存储格式</h3><p><img src="http://aron-blog-image.oss-cn-hangzhou.aliyuncs.com/18-10-6/29577015.jpg" alt="image.png-90kB">Druid segment 的存储格式如上图所示，包含 3 部分：</p>
<ul>
<li>version 文件</li>
<li>meta 文件</li>
<li>数据文件</li>
</ul>
<p>其中 meta 文件主要包含每 1 列的文件名和文件的偏移量。（注，druid 为了减少文件描述符，将 1 个 segment 的所有列都合并到 1 个大的 smoosh 中，由于 druid 访问 segment 文件的时候采用 MMap 的方式，所以单个 smoosh 文件的大小不能超过 2G，如果超过 2G，就会写到下一个 smoosh 文件）。</p>
<p>在 smoosh 文件中，数据是按列存储中，包含时间列，维度列和指标列，其中每 1 列会包含 2 部分：ColumnDescriptor 和 binary 数据。其中 ColumnDescriptor 主要保存每 1 列的数据类型和 Serde 的方式。</p>
<p>smoosh 文件中还有 index.drd 文件和 metadata.drd 文件，其中 index.drd 主要包含该 segment 有哪些列，哪些维度，该 Segment 的时间范围以及使用哪种 bitmap；metadata.drd 主要包含是否需要聚合，指标的聚合函数，查询粒度，时间戳字段的配置等。</p>
<h3 id="指标列的存储格式"><a href="#指标列的存储格式" class="headerlink" title="指标列的存储格式"></a>指标列的存储格式</h3><p>我们先来看指标列的存储格式：</p>
<p><img src="http://aron-blog-image.oss-cn-hangzhou.aliyuncs.com/18-10-6/8107893.jpg" alt="image.png-35.9kB"></p>
<p>指标列的存储格式如上图所示：</p>
<ul>
<li>version</li>
<li>value 个数</li>
<li>每个 block 的 value 的个数（druid 对 Long 和 Float 类型会按 block 进行压缩，block 的大小是 64K）</li>
<li>压缩类型 （druid 目前主要有 LZ4 和 LZF 俩种压缩算法）</li>
<li>编码类型 （druid 对 Long 类型支持差分编码和 Table 编码两种方式，Table 编码就是将 long 值映射到 int，当指标列的基数小于 256 时，druid 会选择 Table 编码，否则会选择差分编码）</li>
<li>编码的 header （以差分编码为例，header 中会记录版本号，base value，每个 value 用几个 bit 表示）</li>
<li>每个 block 的 header （主要记录版本号，是否允许反向查找，value 的数量，列名长度和列名）</li>
<li>每 1 列具体的值</li>
</ul>
<h4 id="Long-型指标"><a href="#Long-型指标" class="headerlink" title="Long 型指标"></a>Long 型指标</h4><p>Druid 中对 Long 型指标会先进行编码，然后按 block 进行压缩。编码算法包含差分编码和 table 编码，压缩算法包含 LZ4 和 LZF。</p>
<h4 id="Float-型指标"><a href="#Float-型指标" class="headerlink" title="Float 型指标"></a>Float 型指标</h4><p>Druid 对于 Float 类型的指标不会进行编码，只会按 block 进行压缩。</p>
<h4 id="Complex-型指标"><a href="#Complex-型指标" class="headerlink" title="Complex 型指标"></a>Complex 型指标</h4><p>Druid 对于 HyperUnique，Cardinality，Histogram，Sketch 等复杂指标不会进行编码和压缩处理，每种复杂指标的 Serde 方式由每种指标自己的 ComplexMetricSerde 实现类实现。</p>
<h3 id="String-维度的存储格式"><a href="#String-维度的存储格式" class="headerlink" title="String 维度的存储格式"></a>String 维度的存储格式</h3><p><img src="http://aron-blog-image.oss-cn-hangzhou.aliyuncs.com/18-10-6/79137485.jpg" alt="image.png-81.2kB"></p>
<p>String 维度的存储格式如上图所示，前面提到过，时间列，维度列，指标列由两部分组成：ColumnDescriptor 和 binary 数据。 String 维度的 binary 数据主要由 3 部分组成：dict，字典编码后的 id 数组，用于倒排索引的 bitmap。</p>
<p>以上图中的 D2 维度列为例，总共有 4 行，前 3 行的值是 meituan，第 4 行的值是 dianing。Druid 中 dict 的实现十分简单，就是一个 hashmap。图中 dict 的内容就是将 meituan 编码为 0，dianping 编码为 1。 Id 数组的内容就是用编码后的 ID 替换掉原始值，所以就是 [1,1,1,0]。第 3 部分的倒排索引就是用 bitmap 表示某个值是否出现在某行中，如果出现了，bitmap 对应的位置就会置为 1，如图：meituan 在前 3 行中都有出现，所以倒排索引 1：[1,1,1,0] 就表示 meituan 在前 3 行中出现。</p>
<p>显然，倒排索引的大小是列的基数 * 总的行数，如果没有处理的话结果必然会很大。不过好在如果维度列如果基数很高的话，bitmap 就会比较稀疏，而稀疏的 bitmap 可以进行高效的压缩。</p>
<h3 id="Segment-生成过程"><a href="#Segment-生成过程" class="headerlink" title="Segment 生成过程"></a>Segment 生成过程</h3><ol>
<li>Add Row to Map</li>
<li>Begin persist to disk</li>
<li>Write version file</li>
<li>Merge and write dimension dict</li>
<li>Write time column</li>
<li>Write metric column</li>
<li>Write dimension column</li>
<li>Write index.drd</li>
<li>Merge and write bitmaps</li>
<li>Write metadata.drd</li>
</ol>
<h3 id="Segment-load-过程"><a href="#Segment-load-过程" class="headerlink" title="Segment load 过程"></a>Segment load 过程</h3><p><img src="http://aron-blog-image.oss-cn-hangzhou.aliyuncs.com/18-10-6/34569900.jpg" alt="meta.png-44.3kB"></p>
<ol>
<li>Read version</li>
<li>Load segment to MappedByteBuffer</li>
<li>Get column offset from meta</li>
<li>Deserialize each column from ByteBuffer</li>
</ol>
<h3 id="Segment-Query-过程"><a href="#Segment-Query-过程" class="headerlink" title="Segment Query 过程"></a>Segment Query 过程</h3><p>Druid 查询的最小单位是 Segment，Segment 在查询之前必须先 load 到内存，load 过程如上一步所述。如果没有索引的话，我们的查询过程就只能 Scan 的，遇到符合条件的行选择出来，但是所有查询都进行全表 Scan 肯定是不可行的，所以我们需要索引来快速过滤不需要的行。Druid 的 Segmenet 查询过程如下：</p>
<ol>
<li>构造 1 个 Cursor 进行迭代</li>
<li>查询之前构造出 Fliter</li>
<li>根据 Index 匹配 Fliter，得到满足条件的 Row 的 Offset</li>
<li>根据每列的 ColumnSelector 去指定 Row 读取需要的列。</li>
</ol>
<h3 id="Druid-的编码和压缩"><a href="#Druid-的编码和压缩" class="headerlink" title="Druid 的编码和压缩"></a>Druid 的编码和压缩</h3><p>前面已经提到了，Druid 对 Long 型的指标进行了差分编码和 Table 编码，Long 型和 Float 型的指标进行了 LZ4 或者 LZF 压缩。</p>
<p>其实编码和压缩本质上是一个东西，一切熵增的编码都是压缩。 在计算机领域，我们一般把针对特定类型的编码称之为编码，针对任意类型的通用编码称之为压缩。</p>
<p>编码和压缩的本质就是让每一个 bit 尽可能带有更多的信息。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tttmelody.github.io/2018/10/01/Druid-入门-benchmark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aron">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://gitee.com/Meldoy/image/raw/master/life/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/01/Druid-入门-benchmark/" itemprop="url">Druid 入门&benchmark</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-01T10:36:00+08:00">
                2018-10-01
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/01/Druid-入门-benchmark/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/10/01/Druid-入门-benchmark/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/10/01/Druid-入门-benchmark/" class="leancloud_visitors" data-flag-title="Druid 入门&benchmark">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>coordinator:管理集群状态, 8081<br>broker:查询节点, 8082<br>historical:历史节点, 管理历史数据, 8083<br>overlord: 统治节点, 管理数据写入, 8090<br>middleManager: 中间管理者, 负责写数据<br>pivot: ui, 9095</p>
<p>可以分为三种节点:</p>
<ol>
<li>Master, 包含 overlord 和 coordinator, 负责数据写入和容错.</li>
<li>Data, 数据节点, 包含 historical, middleManager, 负责数据写入和历史数据加载</li>
<li>Query, broker, 查询数据.</li>
</ol>
<p>1,3尽量选多核大内存, 2选磁盘大的机子. 机子资源少时, 可以把 master 和 query 放到一起, 在家 historical 加速热点数据查询.</p>
<hr>
<p>overloard:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">-server</span><br><span class="line">-Xms4g</span><br><span class="line">-Xmx4g</span><br><span class="line">-XX:NewSize=256m</span><br><span class="line">-XX:MaxNewSize=256m</span><br><span class="line">-XX:+UserConcMarkSweepGC</span><br><span class="line">-XX:+PrintGCDetails</span><br><span class="line">-XX:+PrintGCTimeStamps</span><br><span class="line">-Duser.timezone=UTC+0800</span><br><span class="line">-Dfile.encoding=UTF-8</span><br><span class="line">-Djava.io.tmpdir=var/tmp</span><br><span class="line">-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager</span><br><span class="line"></span><br><span class="line">druid.service=druid/overlord</span><br><span class="line">druid.host=10.1.30.101</span><br><span class="line">druid.port=8090</span><br><span class="line">druid.indexer.queue.startDelay=PT30S</span><br><span class="line">druid.indexer.runner.type=remote</span><br><span class="line">druid.indexer.storage.type=metadata</span><br></pre></td></tr></table></figure></p>
<p>broker<br>有个 query cache 的 要注意.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"> 1 -server</span><br><span class="line"> 2 -Xms24g</span><br><span class="line"> 3 -Xmx24g</span><br><span class="line"> 4 -XX:NewSize=6g</span><br><span class="line"> 5 -XX:MaxNewSize=6g</span><br><span class="line"> 6 -XX:MaxDirectMemorySize=32g</span><br><span class="line"> 7 -XX:+UserConcMarkSweepGC</span><br><span class="line"> 8 -XX:+PrintGCDetails</span><br><span class="line"> 9 -XX:+PrintGCTimeStamps</span><br><span class="line">10 -Duser.timezone=UTC+0800</span><br><span class="line">11 -Dfile.encoding=UTF-8</span><br><span class="line">12 -Djava.io.tmpdir=var/tmp</span><br><span class="line">13 -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager</span><br><span class="line"></span><br><span class="line"> 1 druid.service=druid/broker</span><br><span class="line"> 2 druid.host=10.1.30.102</span><br><span class="line"> 3 druid.port=8082</span><br><span class="line"> 4</span><br><span class="line"> 5 # HTTP server threads</span><br><span class="line"> 6 druid.broker.http.numConnections=20</span><br><span class="line"> 7 druid.broker.http.readTimeout=PT5M</span><br><span class="line"> 8 druid.server.http.numThreads=50</span><br><span class="line"> 9</span><br><span class="line">10 # Processing threads and buffers</span><br><span class="line">11 druid.processing.buffer.sizeBytes=2147483647</span><br><span class="line">12 druid.processing.numMergeBuffers=2</span><br><span class="line">13 druid.processing.numThreads=15</span><br><span class="line">14 druid.processing.tmpDir=var/druid/processing</span><br><span class="line">15</span><br><span class="line">16 # Query cache disabled -- push down caching and merging instead</span><br><span class="line">17 druid.broker.cache.useCache=false</span><br><span class="line">18 druid.broker.cache.populateCache=false</span><br><span class="line">19 # druid.cache.sizeInBytes=6000000000</span><br><span class="line">20</span><br><span class="line">21 # Query config</span><br><span class="line">22 druid.broker.balancer.type=connectionCount</span><br><span class="line">23</span><br><span class="line">24 # SQL</span><br><span class="line">25 druid.sql.enable=true</span><br></pre></td></tr></table></figure></p>
<p>historical<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"> 1 -server</span><br><span class="line"> 2 -Xms12g</span><br><span class="line"> 3 -Xmx12g</span><br><span class="line"> 4 -XX:NewSize=4g</span><br><span class="line"> 5 -XX:MaxNewSize=4g</span><br><span class="line"> 6 -XX:MaxDirectMemorySize=10g</span><br><span class="line"> 7 -XX:+UserConcMarkSweepGC</span><br><span class="line"> 8 -XX:+PrintGCDetails</span><br><span class="line"> 9 -XX:+PrintGCTimeStamps</span><br><span class="line">10 -Duser.timezone=UTC+0800</span><br><span class="line">11 -XX:MaxDirectMemorySize=8g</span><br><span class="line">12 -Dfile.encoding=UTF-8</span><br><span class="line">13 -Djava.io.tmpdir=var/tmp</span><br><span class="line">14 -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager</span><br><span class="line"></span><br><span class="line"> 1 druid.service=druid/historical</span><br><span class="line"> 2 druid.host=10.1.30.103</span><br><span class="line"> 3 druid.port=8083</span><br><span class="line"> 4</span><br><span class="line"> 5 # HTTP server threads</span><br><span class="line"> 6 druid.server.http.numThreads=50</span><br><span class="line"> 7</span><br><span class="line"> 8 # Processing threads and buffers</span><br><span class="line"> 9 druid.processing.buffer.sizeBytes=1073741824</span><br><span class="line">10 druid.processing.numMergeBuffers=2</span><br><span class="line">11 druid.processing.numThreads=15</span><br><span class="line">12 druid.processing.tmpDir=var/druid/processing</span><br><span class="line">13</span><br><span class="line">14 # Segment storage</span><br><span class="line">15 druid.segmentCache.locations=[&#123;&quot;path&quot;:&quot;var/druid/segment-cache&quot;,&quot;maxSize&quot;\:130000000000&#125;]</span><br><span class="line">16 druid.server.maxSize=130000000000</span><br><span class="line">17</span><br><span class="line">18 # Query cache (note change)</span><br><span class="line">19 druid.historical.cache.useCache=false</span><br><span class="line">20 druid.historical.cache.populateCache=false</span><br><span class="line">21 druid.cache.type=caffeine</span><br><span class="line">22 druid.cache.sizeInBytes=2000000000</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tttmelody.github.io/2018/09/14/Maven-shade/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aron">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://gitee.com/Meldoy/image/raw/master/life/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/14/Maven-shade/" itemprop="url">Maven shade</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-14T11:19:06+08:00">
                2018-09-14
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/09/14/Maven-shade/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/09/14/Maven-shade/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/09/14/Maven-shade/" class="leancloud_visitors" data-flag-title="Maven shade">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>程序里报错<code>Caused by: java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.canSerialize(Ljava/lang/Class;Ljava/util/concurrent/atomic/AtomicReference;)Z</code></p>
<p>但是无论从<code>mvn dependency:tree</code>, 还是运行时加载的 jar 包来看, 都是用了正确的 <code>jackson-databind-2.6.5.jar</code>. 问题就刁钻在它用的这个类, 其实不是 <code>jackson-databind</code> 里的, 而是其他的包里 shaed 但是又没有 relocation 的. 除非你把这个包给从依赖李去掉, 在这个包的里面的依赖里去掉, 或者最外面加正确版本的<code>jackson-databind-2.6.5.jar</code>都是没有用的, 见下图:</p>
<p><img src="http://aron-blog-image.oss-cn-hangzhou.aliyuncs.com/18-9-14/40098244.jpg" alt=""></p>
<p>所以画框里他 exclusive 也是没有用的. 解决方法就是我们做成 external 的, 并且 exclude 掉.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>external-influxdb<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>External-InfluxDB<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://kyligence.io<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Curator for KAP<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>apache<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kylin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">relativePath</span>&gt;</span>../../../pom.xml<span class="tag">&lt;/<span class="name">relativePath</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shadeBase</span>&gt;</span>org.apache.kylin.shaded.influxdb<span class="tag">&lt;/<span class="name">shadeBase</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shaded.curator.version</span>&gt;</span>2.12.0<span class="tag">&lt;/<span class="name">shaded.curator.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.influxdb<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>influxdb-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>20.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- cover log4j from parent pom--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jcl-over-slf4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--overwrite parent, need to upgrade this when upgrade grpc--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">include</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>log4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.slf4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">relocations</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>org.influxdb<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.org.influxdb<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.squareup.moshi<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.com.squareup.moshi<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>okhttp3<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.okhttp3<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>okio<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.okio<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>retrofit2<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.retrofit2<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.google<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>$&#123;shadeBase&#125;.com.google.common<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">relocations</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="画外"><a href="#画外" class="headerlink" title="画外"></a>画外</h4><p>当你使用 Maven 对项目打包时，你需要了解以下 3 个打包 plugin，它们分别是</p>
<table>
<thead>
<tr>
<th>plugin</th>
<th>function</th>
</tr>
</thead>
<tbody>
<tr>
<td>maven-jar-plugin</td>
<td>maven 默认打包插件，用来创建 project jar</td>
</tr>
<tr>
<td>maven-shade-plugin</td>
<td>用来打可执行包，executable(fat) jar</td>
</tr>
<tr>
<td>maven-assembly-plugin</td>
<td>支持定制化打包方式，例如 apache 项目的打包方式</td>
</tr>
</tbody>
</table>
<p>不管你 dependences 里的 scope 设置为什么, mvn package 出来的 你 src 的 jar 包里, 只会有你的 class 文件, 不会有所依赖的 jar 包, 可以通过 maven assembly 插件来做这个事情. 但是如果打成 war 包, 是会包含 compile scope 的依赖的. 而 provided 是要容器提供, 比如说 Tomcat, 会到 Tomcat 的 <code>$liferay-tomcat-home\webapps\ROOT\WEB-INF\lib</code> 目录下找.</p>
<p>然后再来看 maven 的不同 scope 的官方定义:</p>
<ul>
<li><p>compile</p>
<p>  This is the default scope. Compile dependencies are available in all classpaths of a project. Furthermore, those dependencies are propagated to dependent projects(会有依赖传递).</p>
</li>
<li><p>provided</p>
<p>  This is much like compile, but indicates you expect the JDK or a container to provide the dependency at runtime. For example, when building a web application for the Java Enterprise Edition, you would set the dependency on the Servlet API and related Java EE APIs to scope provided because the web container provides those classes. This scope is only available on the compilation and test classpath, and is not transitive.</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tttmelody.github.io/2018/09/12/Storage-different-strategies-about-high-cardinal-string/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aron">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://gitee.com/Meldoy/image/raw/master/life/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/12/Storage-different-strategies-about-high-cardinal-string/" itemprop="url">Storage:different strategies about high cardinal string</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-12T17:03:14+08:00">
                2018-09-12
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/09/12/Storage-different-strategies-about-high-cardinal-string/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/09/12/Storage-different-strategies-about-high-cardinal-string/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/09/12/Storage-different-strategies-about-high-cardinal-string/" class="leancloud_visitors" data-flag-title="Storage:different strategies about high cardinal string">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Keywords"><a href="#Keywords" class="headerlink" title="Keywords"></a>Keywords</h1><p>run length encoding:对数据稀疏的列有较好的压缩率和访问速度</p>
<p>字典</p>
<h1 id="Clickhouse"><a href="#Clickhouse" class="headerlink" title="Clickhouse"></a>Clickhouse</h1><p>ClickHouse doesn’t have the concept of encodings. Strings can contain an arbitrary set of bytes, which are stored and output as-is. If you need to store texts, we recommend using UTF-8 encoding. At the very least, if your terminal uses UTF-8 (as recommended), you can read and write your values without making conversions. Similarly, certain functions for working with strings have separate variations that work under the assumption that the string contains a set of bytes representing a UTF-8 encoded text. For example, the ‘length’ function calculates the string length in bytes, while the ‘lengthUTF8’ function calculates the string length in Unicode code points, assuming that the value is UTF-8 encoded.</p>
<h1 id="Druid"><a href="#Druid" class="headerlink" title="Druid"></a>Druid</h1><p>使用字典:</p>
<p>字典是 mmap 进来的, 见:<code>org.apache.druid.segment.serde.ColumnPartSerde.Deserializer#read</code><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> GenericIndexed&lt;String&gt; rDictionary = GenericIndexed.read(</span><br><span class="line">    buffer,</span><br><span class="line">    GenericIndexed.STRING_STRATEGY,</span><br><span class="line">    builder.getFileMapper()</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">DictionaryEncodedColumnSupplier dictionaryEncodedColumnSupplier = <span class="keyword">new</span> DictionaryEncodedColumnSupplier(</span><br><span class="line">    rDictionary,</span><br><span class="line">    rSingleValuedColumn,</span><br><span class="line">    rMultiValuedColumn,</span><br><span class="line">    columnConfig.columnCacheSizeBytes()</span><br><span class="line">);</span><br></pre></td></tr></table></figure></p>
<p>其中<code>GenericIndexed.read</code>具体:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">GenericIndexed&lt;T&gt; <span class="title">createGenericIndexedVersionTwo</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    ByteBuffer byteBuffer,</span></span></span><br><span class="line"><span class="function"><span class="params">    ObjectStrategy&lt;T&gt; strategy,</span></span></span><br><span class="line"><span class="function"><span class="params">    SmooshedFileMapper fileMapper</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (fileMapper == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IAE(<span class="string">"SmooshedFileMapper can not be null for version 2."</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">boolean</span> allowReverseLookup = byteBuffer.get() == REVERSE_LOOKUP_ALLOWED;</span><br><span class="line">  <span class="keyword">int</span> logBaseTwoOfElementsPerValueFile = byteBuffer.getInt();</span><br><span class="line">  <span class="keyword">int</span> numElements = byteBuffer.getInt();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    String columnName = SERIALIZER_UTILS.readString(byteBuffer);</span><br><span class="line">    <span class="keyword">int</span> elementsPerValueFile = <span class="number">1</span> &lt;&lt; logBaseTwoOfElementsPerValueFile;</span><br><span class="line">    <span class="keyword">int</span> numberOfFilesRequired = getNumberOfFilesRequired(elementsPerValueFile, numElements);</span><br><span class="line">    ByteBuffer[] valueBuffersToUse = <span class="keyword">new</span> ByteBuffer[numberOfFilesRequired];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numberOfFilesRequired; i++) &#123;</span><br><span class="line">      <span class="comment">// SmooshedFileMapper.mapFile() contract guarantees that the valueBuffer's limit equals to capacity.</span></span><br><span class="line">      ByteBuffer valueBuffer = fileMapper.mapFile(GenericIndexedWriter.generateValueFileName(columnName, i));</span><br><span class="line">      valueBuffersToUse[i] = valueBuffer.asReadOnlyBuffer();</span><br><span class="line">    &#125;</span><br><span class="line">    ByteBuffer headerBuffer = fileMapper.mapFile(GenericIndexedWriter.generateHeaderFileName(columnName));</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> GenericIndexed&lt;&gt;(</span><br><span class="line">        valueBuffersToUse,</span><br><span class="line">        headerBuffer,</span><br><span class="line">        strategy,</span><br><span class="line">        allowReverseLookup,</span><br><span class="line">        logBaseTwoOfElementsPerValueFile,</span><br><span class="line">        numElements</span><br><span class="line">    );</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"File mapping failed."</span>, e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>字典的具体表示是:<code>CachingIndexed&lt;String&gt; cachedLookups</code>, get 的时候先 get 从 cache 拿, 然后没有的话再从 mmap 中拿.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CachingIndexed</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">Indexed</span>&lt;<span class="title">T</span>&gt;, <span class="title">Closeable</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> INITIAL_CACHE_CAPACITY = <span class="number">16384</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger log = <span class="keyword">new</span> Logger(CachingIndexed.class);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> GenericIndexed&lt;T&gt;.BufferIndexed delegate;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> SizedLRUMap&lt;Integer, T&gt; cachedValues;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Creates a CachingIndexed wrapping the given GenericIndexed with a value lookup cache</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * CachingIndexed objects are not thread safe and should only be used by a single thread at a time.</span></span><br><span class="line"><span class="comment">   * CachingIndexed objects must be closed to release any underlying cache resources.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> delegate the GenericIndexed to wrap with a lookup cache.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> lookupCacheSize maximum size in bytes of the lookup cache if greater than zero</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">CachingIndexed</span><span class="params">(GenericIndexed&lt;T&gt; delegate, <span class="keyword">final</span> <span class="keyword">int</span> lookupCacheSize)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.delegate = delegate.singleThreaded();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (lookupCacheSize &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      log.debug(<span class="string">"Allocating column cache of max size[%d]"</span>, lookupCacheSize);</span><br><span class="line">      cachedValues = <span class="keyword">new</span> SizedLRUMap&lt;&gt;(INITIAL_CACHE_CAPACITY, lookupCacheSize);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      cachedValues = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> Class&lt;? extends T&gt; getClazz()</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">return</span> delegate.getClazz();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> delegate.size();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> T <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (cachedValues != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">final</span> T cached = cachedValues.getValue(index);</span><br><span class="line">      <span class="keyword">if</span> (cached != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> cached;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">final</span> T value = delegate.get(index);</span><br><span class="line">      cachedValues.put(index, value, delegate.getLastValueSize());</span><br><span class="line">      <span class="keyword">return</span> value;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> delegate.get(index);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">indexOf</span><span class="params">(T value)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> delegate.indexOf(value);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Iterator&lt;T&gt; <span class="title">iterator</span><span class="params">()</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> delegate.iterator();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (cachedValues != <span class="keyword">null</span>) &#123;</span><br><span class="line">      log.debug(<span class="string">"Closing column cache"</span>);</span><br><span class="line">      cachedValues.clear();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">inspectRuntimeShape</span><span class="params">(RuntimeShapeInspector inspector)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    inspector.visit(<span class="string">"cachedValues"</span>, cachedValues != <span class="keyword">null</span>);</span><br><span class="line">    inspector.visit(<span class="string">"delegate"</span>, delegate);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SizedLRUMap</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">LinkedHashMap</span>&lt;<span class="title">K</span>, <span class="title">Pair</span>&lt;<span class="title">Integer</span>, <span class="title">V</span>&gt;&gt;</span></span><br><span class="line"><span class="class">  </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> maxBytes;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> numBytes = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SizedLRUMap</span><span class="params">(<span class="keyword">int</span> initialCapacity, <span class="keyword">int</span> maxBytes)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">      <span class="keyword">super</span>(initialCapacity, <span class="number">0.75f</span>, <span class="keyword">true</span>);</span><br><span class="line">      <span class="keyword">this</span>.maxBytes = maxBytes;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">removeEldestEntry</span><span class="params">(Map.Entry&lt;K, Pair&lt;Integer, V&gt;&gt; eldest)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (numBytes &gt; maxBytes) &#123;</span><br><span class="line">        numBytes -= eldest.getValue().lhs;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(K key, V value, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="keyword">int</span> totalSize = size + <span class="number">48</span>; <span class="comment">// add approximate object overhead</span></span><br><span class="line">      numBytes += totalSize;</span><br><span class="line">      <span class="keyword">super</span>.put(key, <span class="keyword">new</span> Pair&lt;&gt;(totalSize, value));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> V <span class="title">getValue</span><span class="params">(Object key)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">      <span class="keyword">final</span> Pair&lt;Integer, V&gt; sizeValuePair = <span class="keyword">super</span>.get(key);</span><br><span class="line">      <span class="keyword">return</span> sizeValuePair == <span class="keyword">null</span> ? <span class="keyword">null</span> : sizeValuePair.rhs;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="IndexR"><a href="#IndexR" class="headerlink" title="IndexR"></a>IndexR</h1><h1 id="Carbondata"><a href="#Carbondata" class="headerlink" title="Carbondata"></a>Carbondata</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tttmelody.github.io/2018/08/30/Spark-mapPartitions/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aron">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://gitee.com/Meldoy/image/raw/master/life/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/30/Spark-mapPartitions/" itemprop="url">Spark:mapPartitions</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-30T22:56:55+08:00">
                2018-08-30
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/30/Spark-mapPartitions/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/08/30/Spark-mapPartitions/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/08/30/Spark-mapPartitions/" class="leancloud_visitors" data-flag-title="Spark:mapPartitions">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://gitee.com/Meldoy/image/raw/master/life/head.jpg"
                alt="Aron" />
            
              <p class="site-author-name" itemprop="name">Aron</p>
              <p class="site-description motion-element" itemprop="description">Kyligence</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">31</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Aron</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'xUIDY0rxakumlhKmQqajlnUc-gzGzoHsz',
        appKey: 'lrqs8UwcvY6z1has9clbxJWL',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("mUXETWIxo42z09pHif0vNGo2-gzGzoHsz", "Sv2QHSHPjuj5DVnTOmO5VQIj");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  

</body>
</html>
