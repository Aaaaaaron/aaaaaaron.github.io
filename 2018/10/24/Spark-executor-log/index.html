<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=" id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;spark on yarn 应用在运行时和完成后日志的存放位置是不同的，一般运行时是存放在各个运行节点，完成后会归集到 hdfs。无论哪种情况，都可以通过 spark 的页面跳转找到 executor 的日志，但是在大多数的生产环境中，对端口的开放是有严格的限制，也就是说根本无法正常跳转到日志页面进行查看的，这种情况下，就需要通过后台查询。"><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/noise.css"><title>寻找 Spark executor 日志 | Jiatao Tao's blog</title><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><meta name="generator" content="Hexo 5.4.0"></head><body><article class="wrapper"><div class="post-main"><div class="nav"><nav class="container"><a class="sidebar-nav-item active" href="/">Home</a><a class="sidebar-nav-item" href="/archives">Archives</a><a class="sidebar-nav-item" href="/about">About</a></nav><div class="container post-meta"><div class="post-tags"><a class="post-tag-none-link" href="/tags/BigData/" rel="tag">BigData</a><a class="post-tag-none-link" href="/tags/Spark/" rel="tag">Spark</a></div><div class="post-time">2018-10-24</div></div></div><div class="container post-header"><h1>寻找 Spark executor 日志</h1></div><div class="container post-toc"><details class="toc"><summary class="toc-accordion">Table of Contents</summary><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%97%B6"><span class="toc-number">2.</span> <span class="toc-text">运行时</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%88%90%E5%90%8E"><span class="toc-number">3.</span> <span class="toc-text">完成后</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%BB%E5%A4%96-log4j-%E9%85%8D%E7%BD%AE-spark-on-yarn-client-mode"><span class="toc-number">5.</span> <span class="toc-text">画外:log4j 配置 - spark on yarn client mode</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Driver"><span class="toc-number">5.1.</span> <span class="toc-text">Driver</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Executor"><span class="toc-number">5.2.</span> <span class="toc-text">Executor</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">6.</span> <span class="toc-text">其他</span></a></li></ol></details></div><div class="container post-content"><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>spark on yarn 应用在运行时和完成后日志的存放位置是不同的，一般运行时是存放在各个运行节点，完成后会归集到 hdfs。无论哪种情况，都可以通过 spark 的页面跳转找到 executor 的日志，但是在大多数的生产环境中，对端口的开放是有严格的限制，也就是说根本无法正常跳转到日志页面进行查看的，这种情况下，就需要通过后台查询。</p>
<h2 id="运行时"><a href="#运行时" class="headerlink" title="运行时"></a>运行时</h2><p>spark on yarn 模式下一个 executor 对应 yarn 的一个 container，所以在 executor 的节点运行<code>ps -ef|grep spark.yarn.app.container.log.dir</code>，如果这个节点上可能运行多个 application，那么再通过 application id 进一步过滤。上面的命令会查到 executor 的进程信息，并且包含了日志路径，例如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> -Djava.io.tmpdir=/data1/hadoop/yarn/local/usercache/ocdp/appcache/application_1521424748238_0051/container_e07_1521424748238_0051_01_000002/tmp &#x27;</span><br><span class="line">-Dspark.history.ui.port=18080&#x27; &#x27;-Dspark.driver.port=59555&#x27; </span><br><span class="line">-Dspark.yarn.app.container.log.dir=/data1/hadoop/yarn/log/application_1521424748238_0051/container_e07_1521424748238_0051_01_000002 </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>也就是说这个 executor 的日志就在<code>/data1/hadoop/yarn/log/application_1521424748238_0051/container_e07_1521424748238_0051_01_000002</code>目录里。至此，我们就找到了运行时的 executor 日志。</p>
<h2 id="完成后"><a href="#完成后" class="headerlink" title="完成后"></a>完成后</h2><p>当这个 application 正常或者由于某种原因异常结束后，yarn 默认会将所有日志归集到 hdfs 上，所以 yarn 也提供了一个查询已结束 application 日志的方法，即<br> <code>yarn logs -applicationId application_1521424748238_0057</code>，结果里面会包含所有 executor 的日志，可能会比较多，建议将结果重定向到一个文件再详细查看。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>无论对于 spark 应用程序的开发者还是运维人员，日志对于排查问题是至关重要的，所以本文介绍了找到日志的方法。</p>
<h2 id="画外-log4j-配置-spark-on-yarn-client-mode"><a href="#画外-log4j-配置-spark-on-yarn-client-mode" class="headerlink" title="画外:log4j 配置 - spark on yarn client mode"></a>画外:log4j 配置 - spark on yarn client mode</h2><p>spark streaming 的程序如果运行方式是 yarn <strong><em>client</em></strong> mode，那么如何指定 driver 和 executor 的 log4j 配置文件？</p>
<h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>添加参数<code>--driver-java-options</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --driver-java-options &quot;-Dlog4j.configuration=file:/data1/conf/log4j-driver.properties&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>由于 executor 是运行在 yarn 的集群中的，所以先要将配置文件通过<code>--files</code>上传</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --files /data1/conf/log4j.properties --conf spark.executor.extraJavaOptions=&quot;-Dlog4j.configuration=log4j.properties&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在 log4j.properties 中要注意配置<code>spark.yarn.app.container.log.dir</code>例如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, file</span><br><span class="line">log4j.appender.file=org.apache.log4j.RollingFileAppender</span><br><span class="line">log4j.appender.file.append=true</span><br><span class="line">log4j.appender.file.file=$&#123;spark.yarn.app.container.log.dir&#125;/stdout</span><br><span class="line">log4j.appender.file.MaxFileSize=256MB</span><br><span class="line">log4j.appender.file.MaxBackupIndex=20</span><br><span class="line"></span><br><span class="line">log4j.appender.file.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.file.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %p [%t] %c&#123;1&#125;:%L - %m%n</span><br><span class="line"></span><br><span class="line"># Settings to quiet third party logs that are too verbose</span><br><span class="line">log4j.logger.org.spark-project.jetty=WARN</span><br><span class="line">log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ERROR</span><br><span class="line">log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO</span><br><span class="line">log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这样就可以在 spark 的 Web UI 中直接查看日志:executor tab 下的 Logs.</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>如果是通过<code>java -cp</code>命令运行自己的 jar 包，可以通过下面的方式添加 log4j 的配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -cp -Dlog4j.configuration=file:$&#123;APP_HOME&#125;/conf/log4j.properties</span><br></pre></td></tr></table></figure>

<p>作者：Woople, 链接：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/06a630618f19">https://www.jianshu.com/p/06a630618f19</a></p>
</div></div><div class="post-main post-comment"></div></article><link rel="stylesheet" type="text/css" href="/css/font.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
  $(".fancybox").fancybox();
});
</script></body></html>