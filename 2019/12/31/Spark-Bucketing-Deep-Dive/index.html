<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="google-site-verification" content="oi9cACXaYdEyQVeGEUG_WUOEeFyJe9ey2Sj7Dcribng">
  <meta name="baidu-site-verification" content="code-W1bppEiaZ5">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://aaaaaaron.github.io').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: true,
    lazyload: false,
    pangu: false,
    comments: {"style":"buttons","active":null,"storage":true,"lazyload":true,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":false,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="0. 开篇Spark 的 bucket 原理上其实和 repartition 非常相似(其实对数据的操作都是一样的), 但是 Spark 的 repartition 是用来调整 Dataframe 的分区数, 而 bucketing 机制相比, 更多了以下的功能:  当有点查的时候, 可以 pruning 掉不必要的文件. 当 join 的双边都有 bucketBy 且满足一定条件之后, 可以进行">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark bucketing Deep Dive">
<meta property="og:url" content="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/index.html">
<meta property="og:site_name" content="Jiatao Tao&#39;s blog">
<meta property="og:description" content="0. 开篇Spark 的 bucket 原理上其实和 repartition 非常相似(其实对数据的操作都是一样的), 但是 Spark 的 repartition 是用来调整 Dataframe 的分区数, 而 bucketing 机制相比, 更多了以下的功能:  当有点查的时候, 可以 pruning 掉不必要的文件. 当 join 的双边都有 bucketBy 且满足一定条件之后, 可以进行">
<meta property="og:locale">
<meta property="og:image" content="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/78b48f23-6c82-4db0-9237-ecfcadbe8248-20210508113716354.png">
<meta property="og:image" content="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/20191019131517-20210508113251983.png">
<meta property="og:image" content="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/20191019201545-20210508113430256.png">
<meta property="og:image" content="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/2d709ebe-ee49-495a-8971-c8cf988cabf2-20210508114216719.png">
<meta property="og:image" content="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/20191019202151-20210508114058946.png">
<meta property="og:image" content="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/20191018084225-20210508113437602.png">
<meta property="og:image" content="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/20191018084345-20210508113440064.png">
<meta property="article:published_time" content="2019-12-31T09:10:27.000Z">
<meta property="article:modified_time" content="2021-05-08T03:48:40.530Z">
<meta property="article:author" content="Jiatao Tao">
<meta property="article:tag" content="BigData">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/78b48f23-6c82-4db0-9237-ecfcadbe8248-20210508113716354.png">

<link rel="canonical" href="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Spark bucketing Deep Dive | Jiatao Tao's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiatao Tao's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">λ</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-desktop"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://aaaaaaron.github.io/2019/12/31/Spark-Bucketing-Deep-Dive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Jiatao Tao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiatao Tao's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          Spark bucketing Deep Dive
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-31 17:10:27" itemprop="dateCreated datePublished" datetime="2019-12-31T17:10:27+08:00">2019-12-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-05-08 11:48:40" itemprop="dateModified" datetime="2021-05-08T11:48:40+08:00">2021-05-08</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="0-开篇"><a href="#0-开篇" class="headerlink" title="0. 开篇"></a>0. 开篇</h1><p>Spark 的 bucket 原理上其实和 repartition 非常相似(其实对数据的操作都是一样的), 但是 Spark 的 repartition 是用来调整 Dataframe 的分区数, 而 bucketing 机制相比, 更多了以下的功能:</p>
<ol>
<li>当有点查的时候, 可以 pruning 掉不必要的文件.</li>
<li>当 join 的双边都有 bucketBy 且满足一定条件之后, 可以进行 bucket join, 极大的优化 join 大-大表 join 性能(能优化掉 shuffle, 这个真的是大杀器).</li>
</ol>
<p>以下的文章讲先介绍 bucket 的原理, 然后具体展开上面的这两点优化, 最后会讲下 bucketing 机制存在的问题.</p>
<h1 id="1-基础"><a href="#1-基础" class="headerlink" title="1. 基础"></a>1. 基础</h1><p>bucketing 与 repartition 都是对数据里每条记录通过一个 Hash 函数计算 key(<code>Murmur3Hash</code>)得到一个值, 相同值的记录放到同一个分片中去. </p>
<p>bucketBy 的写入比较特殊, 不能直接 write.parquet, 因为需要记录一些信息到元数据信息, 在我们自己测试的时候, 可以这样写: <code>df.write.format(&quot;parquet&quot;).option(&quot;path&quot;, &quot;/tmp/bueket&quot;).bucketBy(3, &quot;id&quot;).saveAsTable(&quot;tbl&quot;)</code>, 我们 buketBy 的列是 id, 分了三个桶, 最后产生出来的文件会小于等于3个(如果 id 只有一个值, 只会有一个文件). 如下, 文件名中<code>_</code>后的, 就是每个文件的 buckId(也是里面记录的 hash 值).</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00000.c000.snappy.parquet</span><br><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00001.c000.snappy.parquet</span><br><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00002.c000.snappy.parquet</span><br></pre></td></tr></table></figure>

<p>写入的流程比较杂, 后续会专门讲下, 文末贴了两张调用 debug 的图, 感兴趣的读者可以自己去追下, 虽然流程很长, 但是代码还是很简单的.</p>
<p>这里提下 buckId 是怎么加到文件上的: 在 <code>DynamicPartitionDataWriter#newOutputWriter</code> 中:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 所以可以看到最多五位数的 bucketId</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bucketIdToString</span></span>(id: <span class="type">Int</span>): <span class="type">String</span> = <span class="string">f&quot;_<span class="subst">$id</span>%05d&quot;</span></span><br><span class="line"><span class="keyword">val</span> bucketIdStr = bucketId.map(<span class="type">BucketingUtils</span>.bucketIdToString).getOrElse(<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>提一句, 其实如果你是 repartition 了之后存下来的, <code>part-</code> 后的数字也就是 hash 之后的值, 这个 task attempt ID 是之前 execute task 时传入的 sparkPartitionId, 代码在 <code>FileFormatWriter#executeTask</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getFilename</span></span>(taskContext: <span class="type">TaskAttemptContext</span>, ext: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> split = taskContext.getTaskAttemptID.getTaskID.getId</span><br><span class="line">  <span class="string">f&quot;part-<span class="subst">$split</span>%05d-<span class="subst">$jobId</span><span class="subst">$ext</span>&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="2-优化-Bucket-Pruning"><a href="#2-优化-Bucket-Pruning" class="headerlink" title="2. 优化: Bucket Pruning"></a>2. 优化: Bucket Pruning</h1><p>对于点查, 还是上面 bucketBy id 的例子, 例如 id=5, 可以把id=5当做原始表中一条记录, 同样的我们可以计算出它的 hash 值, 得到它的 bucket ID, 那么我们只要扫这个 bucket ID 文件就可以了, 因为其他 bucket ID 的文件里肯定没有5这个元素.</p>
<h2 id="2-1-Strategy-部分"><a href="#2-1-Strategy-部分" class="headerlink" title="2.1 Strategy 部分"></a>2.1 Strategy 部分</h2><p>先简单带过下 Spark read parquet 的流程, 想了解更多的可以参考这篇<a href="https://aaaaaaron.github.io/2018/11/01/Spark-Read-Deep-Dive/">Spark Read Deep Dive
</a>. </p>
<p>Spark 具体读取底层数据文件的 <code>SparkStrategy</code> 叫做 <code>FileSourceStrategy</code>, 在其 <code>apply</code> 方法中我们可以看到他需要一个 <code>LogicalRelation</code> 来触发到 apply 方法中的各个逻辑: <code>l @ LogicalRelation(fsRelation: HadoopFsRelation, _, table, _)) =&gt;</code>. </p>
<p><code>LogicalRelation</code> 中最重要的是<code>HadoopFsRelation</code>, 我们也可以构造自己的 <code> HadoopFsRelation</code> 传入, 从而生成 DataFrame, 这里不展开:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseRelationToDataFrame</span></span>(baseRelation: <span class="type">BaseRelation</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="type">Dataset</span>.ofRows(self, <span class="type">LogicalRelation</span>(baseRelation))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>提一句, Spark 的 <code>partitionBy</code> 的 pruning 通过 FileIndex, FileIndex 记录在 <code>HadoopFsRelation</code> 一路传到 <code>FileSourceScanExec</code>, <code>FileSourceScanExec</code> 里调用 <code>FileIndex#listFiles</code> pruning 文件, 但是 bucket 并不走 FileIndex 这一套, 事实上我觉得这两个逻辑是类似, 不知道为啥 Spark 不实现在一起, 有知道同学可以说下. </p>
<p>在 <code>FileSourceStrategy#apply</code> 中会对于可以进行 bucket pruning 的情况(<code>bucketColumnNames.length == 1 &amp;&amp; numBuckets &gt; 1</code>), 会传给<code>FileSourceScanExec</code>一个 <code>bucketSet</code>, 它是一个bitset, 通过这个 <code>bucketSet</code> 我们就能知道有哪些文件被选中了(010就代表第二个文件被选中了, 这个用法还是有点装逼). </p>
<p>下面来看看这个 <code>bucketSet</code> 是如何返回的, 代码如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getExpressionBuckets</span></span>(</span><br><span class="line">    expr: <span class="type">Expression</span>,</span><br><span class="line">    bucketColumnName: <span class="type">String</span>,</span><br><span class="line">    numBuckets: <span class="type">Int</span>): <span class="type">BitSet</span> = &#123;</span><br><span class="line">  ...</span><br><span class="line">  expr <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">Equality</span>(a: <span class="type">Attribute</span>, <span class="type">Literal</span>(v, _)) <span class="keyword">if</span> a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromValue(a, v)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">In</span>(a: <span class="type">Attribute</span>, list)</span><br><span class="line">      <span class="keyword">if</span> list.forall(_.isInstanceOf[<span class="type">Literal</span>]) &amp;&amp; a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromIterable(a, list.map(e =&gt; e.eval(<span class="type">EmptyRow</span>)))</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">InSet</span>(a: <span class="type">Attribute</span>, hset)</span><br><span class="line">      <span class="keyword">if</span> hset.forall(_.isInstanceOf[<span class="type">Literal</span>]) &amp;&amp; a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromIterable(a, hset.map(e =&gt; expressions.<span class="type">Literal</span>(e).eval(<span class="type">EmptyRow</span>)))</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">IsNull</span>(a: <span class="type">Attribute</span>) <span class="keyword">if</span> a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromValue(a, <span class="literal">null</span>)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">And</span>(left, right) =&gt;</span><br><span class="line">      getExpressionBuckets(left, bucketColumnName, numBuckets) &amp;</span><br><span class="line">        getExpressionBuckets(right, bucketColumnName, numBuckets)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">Or</span>(left, right) =&gt;</span><br><span class="line">      getExpressionBuckets(left, bucketColumnName, numBuckets) |</span><br><span class="line">      getExpressionBuckets(right, bucketColumnName, numBuckets)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="keyword">val</span> matchedBuckets = <span class="keyword">new</span> <span class="type">BitSet</span>(numBuckets)</span><br><span class="line">      matchedBuckets.setUntil(numBuckets)</span><br><span class="line">      matchedBuckets</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>最主要的逻辑在 <code>FileSourceStrategy#getExpressionBuckets</code> 中, 可以看到 bucket 的 pruning 只支持 <code>Equality, In, InSet, IsNull</code> (And/Or 也支持, 只不过 And/Or 的 left/right 也必须是前面的类型), 其他情况是不支持 pruning 的, 直接返回所有 buckets.</p>
<p>我们来看 Equality 情况的处理, 调用了 <code>getBucketIdFromValue</code>, 里面逻辑是使用 HashPartitioning 求出 filter 里 literal 的 hash 值. 直接把这个 hash 值写入bitset 就是 最后返回的 <code>bucketSet</code> (这里还搞了 InternalRow/UnsafeProjection, 主要是为了处理各个类型的 value)</p>
<p>这里需要注意的一点是, 如果你的 filter 是 <code>cast(id as string)=&#39;1&#39;</code>, 或者等号右边的不是一个 lit, 是没法做 pruning 的.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Given bucketColumn, numBuckets and value, returns the corresponding bucketId</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBucketIdFromValue</span></span>(bucketColumn: <span class="type">Attribute</span>, numBuckets: <span class="type">Int</span>, value: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> mutableInternalRow = <span class="keyword">new</span> <span class="type">SpecificInternalRow</span>(<span class="type">Seq</span>(bucketColumn.dataType))</span><br><span class="line">  mutableInternalRow.update(<span class="number">0</span>, value)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> bucketIdGenerator = <span class="type">UnsafeProjection</span>.create(</span><br><span class="line">    <span class="type">HashPartitioning</span>(<span class="type">Seq</span>(bucketColumn), numBuckets).partitionIdExpression :: <span class="type">Nil</span>,</span><br><span class="line">    bucketColumn :: <span class="type">Nil</span>)</span><br><span class="line">  bucketIdGenerator(mutableInternalRow).getInt(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>PS. 之前我们仿照这个 bucket pruning 写了一个 file index 的时候, 还踩了一个坑, 当时觉得 <code>not</code> 也是支持的, 本来是选中那个文件, not 的话就取反一下, 变成 pruning 掉那个文件. 但是其实是有问题的, 事实上, 所有的 pruning, 只要不是精确到每个值都做 index 的, not 的情况都不能支持, 举例来说, a != 5, 但是 a 可能等于除5之外的任何值, 你把包含5的文件去掉了, 但是这个文件里除了5的记录, 还有其他值的记录.</p>
<h2 id="2-2-Exec-部分"><a href="#2-2-Exec-部分" class="headerlink" title="2.2 Exec 部分"></a>2.2 Exec 部分</h2><p>Spark 用 Strategy 来构造 Exec 的, <code>FileSourceStrategy</code> 用来构造 <code>FileSourceScanExec</code></p>
<p>在第一部分中, 我们传入了 <code>bucketSet</code> 给 <code>FileSourceScanExec</code>, 这个 bitset 告诉了我们要扫描哪些文件. </p>
<p><code>FileSourceScanExec</code> 通过 <code>inputRDD</code> 暴露出数据给上层的算子, 所以说 Spark SQL 用的也是 RDD. <code>inputRDD</code> 有两种逻辑, 如果是 bucketing 的话, 会调用 <code>FileSourceScanExec#createBucketedReadRDD</code>.</p>
<p><code>createBucketedReadRDD</code> 逻辑也很简单: </p>
<ol>
<li>找到每个 partitions 里的所有文件(如果没有用 partitionBy 机制, selectedPartitions 就只会有一个)</li>
<li>找到每个文件对应的 bucketID(文件名里记录着, 正则匹配), group by 这个 ID, 得到类似 Map[bucketID, Arrar[file]] 这样的结构</li>
<li>只取出 <code>bucketSet</code> 中记录了的 bucketID 对应的 files.</li>
</ol>
<p>到这里, 就完成了 bucket pruning 的逻辑.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> bucketedFileName = <span class="string">&quot;&quot;&quot;.*_(\d+)(?:\..*)?$&quot;&quot;&quot;</span>.r</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBucketId</span></span>(fileName: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">Int</span>] = fileName <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> bucketedFileName(bucketId) =&gt; <span class="type">Some</span>(bucketId.toInt)</span><br><span class="line">  <span class="keyword">case</span> other =&gt; <span class="type">None</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The algorithm is pretty simple: each RDD partition being returned should include all the files</span></span><br><span class="line"><span class="comment"> * with the same bucket id from all the given Hive partitions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createBucketedReadRDD</span></span>(</span><br><span class="line">    bucketSpec: <span class="type">BucketSpec</span>,</span><br><span class="line">    readFile: (<span class="type">PartitionedFile</span>) =&gt; <span class="type">Iterator</span>[<span class="type">InternalRow</span>],</span><br><span class="line">    selectedPartitions: <span class="type">Seq</span>[<span class="type">PartitionDirectory</span>],</span><br><span class="line">    fsRelation: <span class="type">HadoopFsRelation</span>): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  logInfo(<span class="string">s&quot;Planning with <span class="subst">$&#123;bucketSpec.numBuckets&#125;</span> buckets&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> filesGroupedToBuckets =</span><br><span class="line">    selectedPartitions.flatMap &#123; p =&gt;</span><br><span class="line">      p.files.map &#123; f =&gt;</span><br><span class="line">        <span class="keyword">val</span> hosts = getBlockHosts(getBlockLocations(f), <span class="number">0</span>, f.getLen)</span><br><span class="line">        <span class="type">PartitionedFile</span>(p.values, f.getPath.toUri.toString, <span class="number">0</span>, f.getLen, hosts)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.groupBy &#123; f =&gt;</span><br><span class="line">      <span class="type">BucketingUtils</span></span><br><span class="line">        .getBucketId(<span class="keyword">new</span> <span class="type">Path</span>(f.filePath).getName)</span><br><span class="line">        .getOrElse(sys.error(<span class="string">s&quot;Invalid bucket file <span class="subst">$&#123;f.filePath&#125;</span>&quot;</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> prunedFilesGroupedToBuckets = <span class="keyword">if</span> (optionalBucketSet.isDefined) &#123;</span><br><span class="line">    <span class="keyword">val</span> bucketSet = optionalBucketSet.get</span><br><span class="line">    filesGroupedToBuckets.filter &#123;</span><br><span class="line">      f =&gt; bucketSet.get(f._1)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    filesGroupedToBuckets</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> filePartitions = <span class="type">Seq</span>.tabulate(bucketSpec.numBuckets) &#123; bucketId =&gt;</span><br><span class="line">    <span class="type">FilePartition</span>(bucketId, prunedFilesGroupedToBuckets.getOrElse(bucketId, <span class="type">Nil</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">FileScanRDD</span>(fsRelation.sparkSession, readFile, filePartitions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="3-优化-Bucket-Join"><a href="#3-优化-Bucket-Join" class="headerlink" title="3. 优化: Bucket Join"></a>3. 优化: Bucket Join</h1><h2 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h2><p>Bucket 还有一个很大的用处是可以用来做 Bucket Join, 这也是我觉得非常黑魔法的一个特性. 众所周知, SQL 的 join 是非常耗费时间的, Spark 为此也做了多种策略, 可以参考我之前的这篇博客 <a href="https://aaaaaaron.github.io/2019/06/29/Spark-SQL-Join-Deep-Dive/">Spark SQL Join Deep Dive</a>, 对于 大-大表进行 join, Spark 一般会选择 <code>SortMergeJoin</code> (SMJ), 因为SMJ 对比 HashJoin 来说, 不需要把一侧分片的数据都加载到内存中去, 提升了系统稳定性, 但是缺点是慢, 见下面两个例子:</p>
<h3 id="例子一"><a href="#例子一" class="headerlink" title="例子一"></a>例子一</h3><p>这个是最简单一个例子, 两个表直接 join, 无子查询.</p>
<h4 id="优化前"><a href="#优化前" class="headerlink" title="优化前"></a>优化前</h4><img src="/2019/12/31/Spark-Bucketing-Deep-Dive/78b48f23-6c82-4db0-9237-ecfcadbe8248-20210508113716354.png" alt="img" style="zoom:50%;">

<h4 id="优化后"><a href="#优化后" class="headerlink" title="优化后"></a>优化后</h4><img src="/2019/12/31/Spark-Bucketing-Deep-Dive/20191019131517-20210508113251983.png" style="zoom:50%;">

<p>可以见到, 由于是一个 SMJ, 而我们的源表又没有任何处理, 所以 Spark 自动给执行计划上加上了几个 exchange(shuffle) 和 sort, 对大数据有点了解的同学都知道, 这两步操作会十分的耗费时间与资源. 而要是启用了 Bucket Join 之后, 执行计划图会变成什么样呢? 可以见下图, 可以见之前的 exchange 和 sort 都没有了, 整体 query 时间从 40s+ 下降到 5s(图一到图三).</p>
<h3 id="例子二"><a href="#例子二" class="headerlink" title="例子二"></a>例子二</h3><p>我们来看更复杂的一个例子:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line"><span class="keyword">FROM</span> test_kylin_fact t1</span><br><span class="line"><span class="keyword">JOIN</span></span><br><span class="line">  (<span class="keyword">SELECT</span> lstg_format_name</span><br><span class="line">   <span class="keyword">FROM</span> test_kylin_fact</span><br><span class="line">   <span class="keyword">GROUP</span> <span class="keyword">BY</span> lstg_format_name) t2 <span class="keyword">ON</span> t1.lstg_format_name <span class="operator">=</span> t2.lstg_format_name</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> t1.lstg_format_name</span><br></pre></td></tr></table></figure>

<h4 id="优化前-1"><a href="#优化前-1" class="headerlink" title="优化前"></a>优化前</h4><img src="/2019/12/31/Spark-Bucketing-Deep-Dive/20191019201545-20210508113430256.png" style="zoom:50%;">

<h4 id="优化后-1"><a href="#优化后-1" class="headerlink" title="优化后"></a>优化后</h4><img src="/2019/12/31/Spark-Bucketing-Deep-Dive/2d709ebe-ee49-495a-8971-c8cf988cabf2-20210508114216719.png" alt="img" style="zoom:50%;">

<p>有意思的是, 去掉最后一个 group by, 执行图会变成这样, 这个就留给读者朋友们自己去想了.</p>
<img src="/2019/12/31/Spark-Bucketing-Deep-Dive/20191019202151-20210508114058946.png" alt="img" style="zoom:50%;">

<h3 id="3-2-原理说明"><a href="#3-2-原理说明" class="headerlink" title="3.2 原理说明"></a>3.2 原理说明</h3><p>我们先来看下 SMJ 的原理:</p>
<ol>
<li>为了让两条记录能连接到一起, 需要将具有相同 key 的记录分发到同一个分区, 这一步会导致 shuffle(Exchange).</li>
<li>分别对两个表中每个分区里的数据按照 key 进行 sort(SortExec), 然后后续做 merge sort 操作, 这样就可以不用像 HashJoin 需要把所有数据都拉到内存中.</li>
</ol>
<p>那么 Spark 是怎么知道是否需要添加这两步操作的呢? 假如我原始数据已经按 key 进行过了 sort, 那么是不是可以省下后面的 sort? 我们来看 <code>SortMergeJoinExec</code> 的两个方法:</p>
<ol>
<li><p><strong>requiredChildDistribution: Seq[Distribution]</strong>: <code>HashClusteredDistribution(leftKeys) :: HashClusteredDistribution(rightKeys) :: Nil</code>, SMJ 要求 Join 的两张表都是 HashClusteredDistribution 的</p>
</li>
<li><p><strong>requiredChildOrdering: Seq[SortOrder]</strong>: <code>leftKeys.map(SortOrder(_, Ascending)) :: rightKeys.map(SortOrder(_, Ascending)) :: Nil</code>, SMJ 要求 Join 的两张表都是排序的</p>
</li>
</ol>
<p>会在 <code>EnsureRequirements#ensureDistributionAndOrdering</code> 中判断一个节点的子节点是否符合上述的两个require:</p>
<h5 id="1-添加-exchange-节点"><a href="#1-添加-exchange-节点" class="headerlink" title="1. 添加 exchange 节点"></a>1. 添加 exchange 节点</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">children = children.zip(requiredChildDistributions).map &#123;</span><br><span class="line">  <span class="keyword">case</span> (child, distribution) <span class="keyword">if</span> child.outputPartitioning.satisfies(distribution) =&gt;</span><br><span class="line">    child</span><br><span class="line">  <span class="keyword">case</span> (child, <span class="type">BroadcastDistribution</span>(mode)) =&gt;</span><br><span class="line">    <span class="type">BroadcastExchangeExec</span>(mode, child)</span><br><span class="line">  <span class="keyword">case</span> (child, distribution) =&gt;</span><br><span class="line">    <span class="keyword">val</span> numPartitions = distribution.requiredNumPartitions</span><br><span class="line">      .getOrElse(defaultNumPreShufflePartitions)</span><br><span class="line">    <span class="type">ShuffleExchangeExec</span>(distribution.createPartitioning(numPartitions), child)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-添加-sort-节点"><a href="#2-添加-sort-节点" class="headerlink" title="2. 添加 sort 节点"></a>2. 添加 sort 节点</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">children = children.zip(requiredChildOrderings).map &#123; <span class="keyword">case</span> (child, requiredOrdering) =&gt;</span><br><span class="line">  <span class="comment">// If child.outputOrdering already satisfies the requiredOrdering, we do not need to sort.</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="type">SortOrder</span>.orderingSatisfies(child.outputOrdering, requiredOrdering)) &#123;</span><br><span class="line">    child</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">SortExec</span>(requiredOrdering, global = <span class="literal">false</span>, child = child)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-替换-children"><a href="#2-替换-children" class="headerlink" title="2. 替换 children"></a>2. 替换 children</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">operator.withNewChildren(children)</span><br></pre></td></tr></table></figure>

<p>那么为何 bucketing 可以省略掉上面这两步呢? 答案就是使用了 bucketing 机制, <code>FileSourceScanExec</code> 会暴露 SMJ 需要的 <code>Distribution</code> 和 <code>Ordering</code>, 代码见<code>FileSourceScanExec#val (outputPartitioning, outputOrdering): (Partitioning, Seq[SortOrder])</code>:</p>
<p>需要注意的一点是, 当用了 partitionBy, 或者就是你的 DF 有多个 partition 时, 会每个 partition 都有 bucketNum 个文件 (防止write 的时候数据倾斜), 所以一个 bucketID 可能会对应多个文件, 这些文件自己是有序的, 但是合在一起是无序的, 所以 ordering 是 Nil.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">lazy</span> <span class="keyword">val</span> (outputPartitioning, outputOrdering): (<span class="type">Partitioning</span>, <span class="type">Seq</span>[<span class="type">SortOrder</span>]) = &#123;</span><br><span class="line">  <span class="keyword">val</span> bucketSpec = <span class="keyword">if</span> (relation.sparkSession.sessionState.conf.bucketingEnabled) &#123;</span><br><span class="line">    relation.bucketSpec</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">None</span></span><br><span class="line">  &#125;</span><br><span class="line">  bucketSpec <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(spec) =&gt;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">toAttribute</span></span>(colName: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">Attribute</span>] = output.find(_.name == colName)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> bucketColumns = spec.bucketColumnNames.flatMap(n =&gt; toAttribute(n))</span><br><span class="line">      <span class="keyword">if</span> (bucketColumns.size == spec.bucketColumnNames.size) &#123;</span><br><span class="line">        <span class="keyword">val</span> partitioning = <span class="type">HashPartitioning</span>(bucketColumns, spec.numBuckets)</span><br><span class="line">        <span class="keyword">val</span> sortColumns =</span><br><span class="line">          spec.sortColumnNames.map(x =&gt; toAttribute(x)).takeWhile(x =&gt; x.isDefined).map(_.get)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sortOrder = <span class="keyword">if</span> (sortColumns.nonEmpty) &#123;</span><br><span class="line">          <span class="keyword">val</span> files = selectedPartitions.flatMap(partition =&gt; partition.files)</span><br><span class="line">          <span class="keyword">val</span> bucketToFilesGrouping =</span><br><span class="line">            files.map(_.getPath.getName).groupBy(file =&gt; <span class="type">BucketingUtils</span>.getBucketId(file))</span><br><span class="line">          <span class="keyword">val</span> singleFilePartitions = bucketToFilesGrouping.forall(p =&gt; p._2.length &lt;= <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (singleFilePartitions) &#123;</span><br><span class="line">            sortColumns.map(attribute =&gt; <span class="type">SortOrder</span>(attribute, <span class="type">Ascending</span>))</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">Nil</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="type">Nil</span></span><br><span class="line">        &#125;</span><br><span class="line">        (partitioning, sortOrder)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        (<span class="type">UnknownPartitioning</span>(<span class="number">0</span>), <span class="type">Nil</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      (<span class="type">UnknownPartitioning</span>(<span class="number">0</span>), <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="优化-SMJ-条件"><a href="#优化-SMJ-条件" class="headerlink" title="优化 SMJ 条件"></a>优化 SMJ 条件</h5><ol>
<li>Join 的列需要是 bucketBy/sortBy 的列, 且两边 bucketBy 的 num 要一样</li>
<li>如果有多个分区或者用了 partitionBy, Ordering 不能去除</li>
</ol>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol>
<li>DF 的每个 partition 都会有 bucket num 个文件, 比如 <code>df.repartition(5).write.format(&quot;parquet&quot;).option(&quot;path&quot;, &quot;/tmp/bueket&quot;).bucketBy(3, &quot;id&quot;)</code> 会产生15个文件, 由于这个限制, 我们使用 repartition 重新实现了类似 bucket 的功能.</li>
</ol>
<h1 id="写入-debug-截图"><a href="#写入-debug-截图" class="headerlink" title="写入 debug 截图"></a>写入 debug 截图</h1><p>TBD, 后面写个文章介绍</p>
<img src="/2019/12/31/Spark-Bucketing-Deep-Dive/20191018084225-20210508113437602.png" style="zoom:50%;">

<img src="/2019/12/31/Spark-Bucketing-Deep-Dive/20191018084345-20210508113440064.png" style="zoom: 37%;">
    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/BigData/" rel="tag"><i class="fa fa-tag"></i> BigData</a>
              <a href="/tags/Spark/" rel="tag"><i class="fa fa-tag"></i> Spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/11/03/%E6%95%B4%E7%90%86-%E7%A3%81%E7%9B%98-I-O/" rel="prev" title="[整理] 磁盘 I/O">
      <i class="fa fa-chevron-left"></i> [整理] 磁盘 I/O
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/02/07/Kylin-query-process/" rel="next" title="Kylin 查询的基本流程">
      Kylin 查询的基本流程 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#0-%E5%BC%80%E7%AF%87"><span class="nav-number">1.</span> <span class="nav-text">0. 开篇</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E5%9F%BA%E7%A1%80"><span class="nav-number">2.</span> <span class="nav-text">1. 基础</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E4%BC%98%E5%8C%96-Bucket-Pruning"><span class="nav-number">3.</span> <span class="nav-text">2. 优化: Bucket Pruning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-Strategy-%E9%83%A8%E5%88%86"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 Strategy 部分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Exec-%E9%83%A8%E5%88%86"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 Exec 部分</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E4%BC%98%E5%8C%96-Bucket-Join"><span class="nav-number">4.</span> <span class="nav-text">3. 优化: Bucket Join</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E6%A6%82%E8%BF%B0"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90%E4%B8%80"><span class="nav-number">4.1.1.</span> <span class="nav-text">例子一</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90%E4%BA%8C"><span class="nav-number">4.1.2.</span> <span class="nav-text">例子二</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E5%8E%9F%E7%90%86%E8%AF%B4%E6%98%8E"><span class="nav-number">4.1.3.</span> <span class="nav-text">3.2 原理说明</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%97%AE%E9%A2%98"><span class="nav-number">5.</span> <span class="nav-text">问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%99%E5%85%A5-debug-%E6%88%AA%E5%9B%BE"><span class="nav-number">6.</span> <span class="nav-text">写入 debug 截图</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jiatao Tao"
      src="/images/head.png">
  <p class="site-author-name" itemprop="name">Jiatao Tao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/aaaaaaron" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;aaaaaaron" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tao@apache.org" title="E-Mail → mailto:tao@apache.org" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiatao Tao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a>
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
