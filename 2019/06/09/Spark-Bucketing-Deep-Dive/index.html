<!DOCTYPE html>
<html lang="">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="Spark bucketing 机制">




  <meta name="keywords" content="BigData,Spark,">





  <link rel="alternate" href="/default" title="Jiatao Tao's blog">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=1.1">



<link rel="canonical" href="https://aaaaaaron.github.io/2019/06/09/Spark-Bucketing-Deep-Dive/">


<meta name="description" content="0. 开篇Spark 的 bucket 原理上其实和 repartition 非常相似(其实对数据的操作都是一样的), 但是 Spark 的 repartition 是用来调整 Dataframe 的分区数, 而 bucketing 机制相比, 更多了以下的功能:  当有点查的时候, 可以 pruning 掉不必要的文件. 当 join 的双边都有 bucketBy 且满足一定条件之后, 可以进行">
<meta name="keywords" content="BigData,Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark bucketing 机制">
<meta property="og:url" content="https://aaaaaaron.github.io/2019/06/09/Spark-Bucketing-Deep-Dive/index.html">
<meta property="og:site_name" content="Jiatao Tao&#39;s blog">
<meta property="og:description" content="0. 开篇Spark 的 bucket 原理上其实和 repartition 非常相似(其实对数据的操作都是一样的), 但是 Spark 的 repartition 是用来调整 Dataframe 的分区数, 而 bucketing 机制相比, 更多了以下的功能:  当有点查的时候, 可以 pruning 掉不必要的文件. 当 join 的双边都有 bucketBy 且满足一定条件之后, 可以进行">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191018084225.png">
<meta property="og:image" content="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191018084345.png">
<meta property="og:updated_time" content="2019-10-18T15:54:07.449Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark bucketing 机制">
<meta name="twitter:description" content="0. 开篇Spark 的 bucket 原理上其实和 repartition 非常相似(其实对数据的操作都是一样的), 但是 Spark 的 repartition 是用来调整 Dataframe 的分区数, 而 bucketing 机制相比, 更多了以下的功能:  当有点查的时候, 可以 pruning 掉不必要的文件. 当 join 的双边都有 bucketBy 且满足一定条件之后, 可以进行">
<meta name="twitter:image" content="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191018084225.png">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  





  


    <title> Spark bucketing 机制 - Jiatao Tao's blog </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">Jiatao Tao's blog</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/tags">
                            
                            
                                Tags
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Spark bucketing 机制
        
      </h1>

      <time class="post-time">
          Jun 9 2019
      </time>
    </header>



    
            <div class="post-content">
            <h1 id="0-开篇"><a href="#0-开篇" class="headerlink" title="0. 开篇"></a>0. 开篇</h1><p>Spark 的 bucket 原理上其实和 repartition 非常相似(其实对数据的操作都是一样的), 但是 Spark 的 repartition 是用来调整 Dataframe 的分区数, 而 bucketing 机制相比, 更多了以下的功能:</p>
<ol>
<li>当有点查的时候, 可以 pruning 掉不必要的文件.</li>
<li>当 join 的双边都有 bucketBy 且满足一定条件之后, 可以进行 bucket join, 极大的优化 join 大-大表 join 性能(能优化掉 shuffle, 这个真的是大杀器).</li>
</ol>
<p>以下的文章讲先介绍 bucket 的原理, 然后具体展开上面的这两点优化, 最后会讲下 bucketing 机制存在的问题.</p>
<h1 id="1-基础"><a href="#1-基础" class="headerlink" title="1. 基础"></a>1. 基础</h1><p>bucketing 与 repartition 都是对数据里每条记录通过一个 Hash 函数计算 key(<code>Murmur3Hash</code>)得到一个值, 相同值的记录放到同一个分片中去. </p>
<p>bucketBy 的写入比较特殊, 不能直接 write.parquet, 因为需要记录一些信息到元数据信息, 在我们自己测试的时候, 可以这样写: <code>df.write.format(&quot;parquet&quot;).option(&quot;path&quot;, &quot;/tmp/bueket&quot;).bucketBy(3, &quot;id&quot;).saveAsTable(&quot;tbl&quot;)</code>, 我们 buketBy 的列是 id, 分了三个桶, 最后产生出来的文件会小于等于3个(如果 id 只有一个值, 只会有一个文件). 如下, 文件名中<code>_</code>后的, 就是每个文件的 buckId(也是里面记录的 hash 值).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00000.c000.snappy.parquet</span><br><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00001.c000.snappy.parquet</span><br><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00002.c000.snappy.parquet</span><br></pre></td></tr></table></figure>
<p>写入的流程比较杂, 后续会专门讲下, 这里只贴两张调用 debug 的图, 感兴趣的读者可以自己去追下, 虽然流程很长, 但是代码还是很简单的.</p>
<p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191018084225.png" alt=""></p>
<p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191018084345.png" alt=""></p>
<p>这里提下 buckId 是怎么加到文件上的: 在 <code>DynamicPartitionDataWriter#newOutputWriter</code> 中:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 所以可以看到最多五位数的 bucketId</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bucketIdToString</span></span>(id: <span class="type">Int</span>): <span class="type">String</span> = <span class="string">f"_<span class="subst">$id</span>%05d"</span></span><br><span class="line"><span class="keyword">val</span> bucketIdStr = bucketId.map(<span class="type">BucketingUtils</span>.bucketIdToString).getOrElse(<span class="string">""</span>)</span><br></pre></td></tr></table></figure>
<p>提一句, 其实如果你是 repartition 了之后存下来的, <code>part-</code> 后的数字也就是 hash 之后的值, 这个 task attempt ID 就是.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getFilename</span></span>(taskContext: <span class="type">TaskAttemptContext</span>, ext: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> split = taskContext.getTaskAttemptID.getTaskID.getId</span><br><span class="line">  <span class="string">f"part-<span class="subst">$split</span>%05d-<span class="subst">$jobId</span><span class="subst">$ext</span>"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="2-优化-Bucket-Pruning"><a href="#2-优化-Bucket-Pruning" class="headerlink" title="2. 优化: Bucket Pruning"></a>2. 优化: Bucket Pruning</h1><p>对于点查, 还是上面 bucketBy id 的例子, 例如 id=5, 可以把id=5当做原始表中一条记录, 同样的我们可以计算出它的 hash 值, 得到它的 bucket ID, 那么我们只要扫这个 bucket ID 文件就可以了, 因为其他 bucket ID 的文件里肯定没有5这个元素.</p>
<h2 id="2-1-Strategy-部分"><a href="#2-1-Strategy-部分" class="headerlink" title="2.1 Strategy 部分"></a>2.1 Strategy 部分</h2><p>先简单带过下 Spark read parquet 的流程, 想了解更多的可以参考这篇<a href="https://aaaaaaron.github.io/2018/11/01/Spark-Read-Deep-Dive/">Spark Read Deep Dive
</a>. </p>
<p>Spark 具体读取底层数据文件的 <code>SparkStrategy</code> 叫做 <code>FileSourceStrategy</code>, 在其 <code>apply</code> 方法中我们可以看到他需要一个 <code>LogicalRelation</code> 来触发到 apply 方法中的各个逻辑: <code>l @ LogicalRelation(fsRelation: HadoopFsRelation, _, table, _)) =&gt;</code>. </p>
<p><code>LogicalRelation</code> 中最重要的是<code>HadoopFsRelation</code>, 我们也可以构造自己的 <code>HadoopFsRelation</code> 传入, 从而生成 DataFrame, 这里不展开:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseRelationToDataFrame</span></span>(baseRelation: <span class="type">BaseRelation</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="type">Dataset</span>.ofRows(self, <span class="type">LogicalRelation</span>(baseRelation))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>提一句, spark 的 <code>partitionBy</code> 的 pruning 通过 FileIndex, FileIndex 记录在 <code>HadoopFsRelation</code> 一路传到 <code>FileSourceScanExec</code>, <code>FileSourceScanExec</code> 里调用 <code>FileIndex#listFiles</code> pruning 文件, 但是 bucket 并不走 FileIndex 这一套, 事实上我觉得这两个逻辑是类似, 不知道为啥 Spark 不实现在一起. </p>
<p>在 <code>FileSourceStrategy#apply</code> 中会对于可以进行 bucket pruning 的情况(<code>bucketColumnNames.length == 1 &amp;&amp; numBuckets &gt; 1</code>), 会传给<code>FileSourceScanExec</code>一个 <code>bucketSet</code>, 它是一个bitset, 通过这个 <code>bucketSet</code> 我们就能知道有哪些文件被选中了(010就代表第二个文件被选中了, 这个用法还是有点装逼). </p>
<p>下面来看看这个 <code>bucketSet</code> 是如何返回的, 代码如下:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getExpressionBuckets</span></span>(</span><br><span class="line">    expr: <span class="type">Expression</span>,</span><br><span class="line">    bucketColumnName: <span class="type">String</span>,</span><br><span class="line">    numBuckets: <span class="type">Int</span>): <span class="type">BitSet</span> = &#123;</span><br><span class="line">  ...</span><br><span class="line">  expr <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">Equality</span>(a: <span class="type">Attribute</span>, <span class="type">Literal</span>(v, _)) <span class="keyword">if</span> a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromValue(a, v)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">In</span>(a: <span class="type">Attribute</span>, list)</span><br><span class="line">      <span class="keyword">if</span> list.forall(_.isInstanceOf[<span class="type">Literal</span>]) &amp;&amp; a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromIterable(a, list.map(e =&gt; e.eval(<span class="type">EmptyRow</span>)))</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">InSet</span>(a: <span class="type">Attribute</span>, hset)</span><br><span class="line">      <span class="keyword">if</span> hset.forall(_.isInstanceOf[<span class="type">Literal</span>]) &amp;&amp; a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromIterable(a, hset.map(e =&gt; expressions.<span class="type">Literal</span>(e).eval(<span class="type">EmptyRow</span>)))</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">IsNull</span>(a: <span class="type">Attribute</span>) <span class="keyword">if</span> a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromValue(a, <span class="literal">null</span>)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">And</span>(left, right) =&gt;</span><br><span class="line">      getExpressionBuckets(left, bucketColumnName, numBuckets) &amp;</span><br><span class="line">        getExpressionBuckets(right, bucketColumnName, numBuckets)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">Or</span>(left, right) =&gt;</span><br><span class="line">      getExpressionBuckets(left, bucketColumnName, numBuckets) |</span><br><span class="line">      getExpressionBuckets(right, bucketColumnName, numBuckets)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="keyword">val</span> matchedBuckets = <span class="keyword">new</span> <span class="type">BitSet</span>(numBuckets)</span><br><span class="line">      matchedBuckets.setUntil(numBuckets)</span><br><span class="line">      matchedBuckets</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>最主要的逻辑在 <code>FileSourceStrategy#getExpressionBuckets</code> 中, 可以看到 bucket 的 pruning 只支持 <code>Equality, In, InSet, IsNull</code> (And/Or 也支持, 只不过 And/Or 的 left/right 也必须是前面的类型), 其他情况是不支持 pruning 的, 直接返回所有 buckets.</p>
<p>我们来看 Equality 情况的处理, 调用了 <code>getBucketIdFromValue</code>, 里面逻辑是使用 HashPartitioning 求出 filter 里 literal 的 hash 值. 直接把这个 hash 值写入bitset 就是 最后返回的 <code>bucketSet</code> (这里还搞了 InternalRow/UnsafeProjection, 主要是为了处理各个类型的 value)</p>
<p>这里需要注意的一点是, 如果你的 filter 是 <code>cast(id as string)=&#39;1&#39;</code>, 或者等号右边的不是 lit 的, 是没法做 pruning 的.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Given bucketColumn, numBuckets and value, returns the corresponding bucketId</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBucketIdFromValue</span></span>(bucketColumn: <span class="type">Attribute</span>, numBuckets: <span class="type">Int</span>, value: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> mutableInternalRow = <span class="keyword">new</span> <span class="type">SpecificInternalRow</span>(<span class="type">Seq</span>(bucketColumn.dataType))</span><br><span class="line">  mutableInternalRow.update(<span class="number">0</span>, value)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> bucketIdGenerator = <span class="type">UnsafeProjection</span>.create(</span><br><span class="line">    <span class="type">HashPartitioning</span>(<span class="type">Seq</span>(bucketColumn), numBuckets).partitionIdExpression :: <span class="type">Nil</span>,</span><br><span class="line">    bucketColumn :: <span class="type">Nil</span>)</span><br><span class="line">  bucketIdGenerator(mutableInternalRow).getInt(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>PS. 之前我们仿照这个 bucket pruning 写了一个 file index 的时候, 还踩了一个坑, 当时觉得 <code>not</code> 也是支持的, 本来是选中那个文件, not 的话就取反一下, 变成 pruning 掉那个文件. 但是其实是有问题的, 事实上, 所有的 pruning, 只要不是精确到每个值都做 index 的, not 的情况都不能支持, 举例来说, a != 5, 但是 a 可能等于除5之外的任何值, 你把包含5的文件去掉了, 但是这个文件里除了5的记录, 还有其他值的记录.</p>
<h2 id="2-2-Exec-部分"><a href="#2-2-Exec-部分" class="headerlink" title="2.2 Exec 部分"></a>2.2 Exec 部分</h2><p>Spark 的 Strategy 是用来构造 Exec 的, 在第一部分中, 我们得到了一个 <code>bucketSet</code>, 这个会作为一个参数传给 <code>FileSourceScanExec</code>. 如果有relation 上带了 bucket 的话, 会走到 <code>FileSourceScanExec#createBucketedReadRDD</code>, 在这里面会根据具体的文件名字去得到 bucketID <code>getBucketId</code>, 就是一个简单的正则, 那 <code>_</code> 后的值, 有了这个值, 只要找到 <code>bucketSet</code> 中与其匹配的值, 就选中了这个文件.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> bucketedFileName = <span class="string">""</span><span class="string">".*_(\d+)(?:\..*)?$"</span><span class="string">""</span>.r</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBucketId</span></span>(fileName: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">Int</span>] = fileName <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> bucketedFileName(bucketId) =&gt; <span class="type">Some</span>(bucketId.toInt)</span><br><span class="line">  <span class="keyword">case</span> other =&gt; <span class="type">None</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create an RDD for bucketed reads.</span></span><br><span class="line"><span class="comment"> * The non-bucketed variant of this function is [[createNonBucketedReadRDD]].</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The algorithm is pretty simple: each RDD partition being returned should include all the files</span></span><br><span class="line"><span class="comment"> * with the same bucket id from all the given Hive partitions.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param bucketSpec the bucketing spec.</span></span><br><span class="line"><span class="comment"> * @param readFile a function to read each (part of a) file.</span></span><br><span class="line"><span class="comment"> * @param selectedPartitions Hive-style partition that are part of the read.</span></span><br><span class="line"><span class="comment"> * @param fsRelation [[HadoopFsRelation]] associated with the read.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createBucketedReadRDD</span></span>(</span><br><span class="line">    bucketSpec: <span class="type">BucketSpec</span>,</span><br><span class="line">    readFile: (<span class="type">PartitionedFile</span>) =&gt; <span class="type">Iterator</span>[<span class="type">InternalRow</span>],</span><br><span class="line">    selectedPartitions: <span class="type">Seq</span>[<span class="type">PartitionDirectory</span>],</span><br><span class="line">    fsRelation: <span class="type">HadoopFsRelation</span>): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  logInfo(<span class="string">s"Planning with <span class="subst">$&#123;bucketSpec.numBuckets&#125;</span> buckets"</span>)</span><br><span class="line">  <span class="keyword">val</span> filesGroupedToBuckets =</span><br><span class="line">    selectedPartitions.flatMap &#123; p =&gt;</span><br><span class="line">      p.files.map &#123; f =&gt;</span><br><span class="line">        <span class="keyword">val</span> hosts = getBlockHosts(getBlockLocations(f), <span class="number">0</span>, f.getLen)</span><br><span class="line">        <span class="type">PartitionedFile</span>(p.values, f.getPath.toUri.toString, <span class="number">0</span>, f.getLen, hosts)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.groupBy &#123; f =&gt;</span><br><span class="line">      <span class="type">BucketingUtils</span></span><br><span class="line">        .getBucketId(<span class="keyword">new</span> <span class="type">Path</span>(f.filePath).getName)</span><br><span class="line">        .getOrElse(sys.error(<span class="string">s"Invalid bucket file <span class="subst">$&#123;f.filePath&#125;</span>"</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> prunedFilesGroupedToBuckets = <span class="keyword">if</span> (optionalBucketSet.isDefined) &#123;</span><br><span class="line">    <span class="keyword">val</span> bucketSet = optionalBucketSet.get</span><br><span class="line">    filesGroupedToBuckets.filter &#123;</span><br><span class="line">      f =&gt; bucketSet.get(f._1)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    filesGroupedToBuckets</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> filePartitions = <span class="type">Seq</span>.tabulate(bucketSpec.numBuckets) &#123; bucketId =&gt;</span><br><span class="line">    <span class="type">FilePartition</span>(bucketId, prunedFilesGroupedToBuckets.getOrElse(bucketId, <span class="type">Nil</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">FileScanRDD</span>(fsRelation.sparkSession, readFile, filePartitions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="3-优化-Bucket-Join"><a href="#3-优化-Bucket-Join" class="headerlink" title="3. 优化: Bucket Join"></a>3. 优化: Bucket Join</h1><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol>
<li>The bucketing implementation of spark is not honoring the specified number of bucket size. Each partitions is writing into a separate files, hence you end up with lot of files for each bucket.</li>
</ol>

            </div>
          

    
      <footer class="post-footer">
        <div class="post-tags">
          
            <a href="/tags/BigData/">BigData</a>
          
            <a href="/tags/Spark/">Spark</a>
          
        </div>

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2019/06/29/Spark-SQL-Join-Deep-Dive/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Spark SQL Join Deep Dive</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2019/05/09/Parquet-乱弹/">
        <span class="next-text nav-default">Parquet 乱弹</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2012 -
    
    2019
    <span class="footer-author">Aron Tao.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/henryhuang/hexo-theme-polarbearsimple">Polar Bear Simple</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
