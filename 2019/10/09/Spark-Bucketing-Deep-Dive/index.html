<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=" id=&quot;0-开篇&quot;&gt;&lt;a href=&quot;#0-开篇&quot; class=&quot;headerlink&quot; title=&quot;0. 开篇&quot;&gt;&lt;/a&gt;0. 开篇&lt;/h1&gt;&lt;p&gt;Spark 的 bucket 原理上其实和 repartition 非常相似(其实对数据的操作都是一样的), 但是 Spark 的 repartition 是用来调整 Dataframe 的分区数, 而 bucketing 机制相比, 更多了以下的功能:"><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/noise.css"><title>Spark bucketing Deep Dive | Jiatao Tao's blog</title><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"></head><body><article class="wrapper"><div class="post-main"><div class="nav"><nav class="container"><a class="sidebar-nav-item active" href="/">Home</a><a class="sidebar-nav-item" href="/archives">Archives</a><a class="sidebar-nav-item" href="/about">About</a></nav><div class="container post-meta"><div class="post-tags"><a class="post-tag-link" href="/tags/BigData/">BigData</a><a class="post-tag-link" href="/tags/Spark/">Spark</a></div><div class="post-time">2019-10-09</div></div></div><div class="container post-header"><h1>Spark bucketing Deep Dive</h1></div><div class="container post-toc"><details class="toc"><summary class="toc-accordion">Table of Contents</summary><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0-开篇"><span class="toc-number">1.</span> <span class="toc-text">0. 开篇</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-基础"><span class="toc-number">2.</span> <span class="toc-text">1. 基础</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-优化-Bucket-Pruning"><span class="toc-number">3.</span> <span class="toc-text">2. 优化: Bucket Pruning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Strategy-部分"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 Strategy 部分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Exec-部分"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 Exec 部分</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-优化-Bucket-Join"><span class="toc-number">4.</span> <span class="toc-text">3. 优化: Bucket Join</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-概述"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#例子一"><span class="toc-number">4.1.1.</span> <span class="toc-text">例子一</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#优化前"><span class="toc-number">4.1.1.1.</span> <span class="toc-text">优化前</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#优化后"><span class="toc-number">4.1.1.2.</span> <span class="toc-text">优化后</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#例子二"><span class="toc-number">4.1.2.</span> <span class="toc-text">例子二</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#优化前-1"><span class="toc-number">4.1.2.1.</span> <span class="toc-text">优化前</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#优化后-1"><span class="toc-number">4.1.2.2.</span> <span class="toc-text">优化后</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-原理说明"><span class="toc-number">4.1.3.</span> <span class="toc-text">3.2 原理说明</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-添加-exchange-节点"><span class="toc-number">4.1.3.0.1.</span> <span class="toc-text">1. 添加 exchange 节点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-添加-sort-节点"><span class="toc-number">4.1.3.0.2.</span> <span class="toc-text">2. 添加 sort 节点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-替换-children"><span class="toc-number">4.1.3.0.3.</span> <span class="toc-text">2. 替换 children</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#优化-SMJ-条件"><span class="toc-number">4.1.3.0.4.</span> <span class="toc-text">优化 SMJ 条件</span></a></li></ol></li></ol></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#问题"><span class="toc-number">5.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#写入-debug-截图"><span class="toc-number">6.</span> <span class="toc-text">写入 debug 截图</span></a></li></details></div><div class="container post-content"><h1 id="0-开篇"><a href="#0-开篇" class="headerlink" title="0. 开篇"></a>0. 开篇</h1><p>Spark 的 bucket 原理上其实和 repartition 非常相似(其实对数据的操作都是一样的), 但是 Spark 的 repartition 是用来调整 Dataframe 的分区数, 而 bucketing 机制相比, 更多了以下的功能:</p>
<ol>
<li>当有点查的时候, 可以 pruning 掉不必要的文件.</li>
<li>当 join 的双边都有 bucketBy 且满足一定条件之后, 可以进行 bucket join, 极大的优化 join 大-大表 join 性能(能优化掉 shuffle, 这个真的是大杀器).</li>
</ol>
<p>以下的文章讲先介绍 bucket 的原理, 然后具体展开上面的这两点优化, 最后会讲下 bucketing 机制存在的问题.</p>
<h1 id="1-基础"><a href="#1-基础" class="headerlink" title="1. 基础"></a>1. 基础</h1><p>bucketing 与 repartition 都是对数据里每条记录通过一个 Hash 函数计算 key(<code>Murmur3Hash</code>)得到一个值, 相同值的记录放到同一个分片中去. </p>
<p>bucketBy 的写入比较特殊, 不能直接 write.parquet, 因为需要记录一些信息到元数据信息, 在我们自己测试的时候, 可以这样写: <code>df.write.format(&quot;parquet&quot;).option(&quot;path&quot;, &quot;/tmp/bueket&quot;).bucketBy(3, &quot;id&quot;).saveAsTable(&quot;tbl&quot;)</code>, 我们 buketBy 的列是 id, 分了三个桶, 最后产生出来的文件会小于等于3个(如果 id 只有一个值, 只会有一个文件). 如下, 文件名中<code>_</code>后的, 就是每个文件的 buckId(也是里面记录的 hash 值).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00000.c000.snappy.parquet</span><br><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00001.c000.snappy.parquet</span><br><span class="line">part-00000-39d128dc-69a1-4b91-8931-68e81c77c4ae_00002.c000.snappy.parquet</span><br></pre></td></tr></table></figure>
<p>写入的流程比较杂, 后续会专门讲下, 文末贴了两张调用 debug 的图, 感兴趣的读者可以自己去追下, 虽然流程很长, 但是代码还是很简单的.</p>
<p>这里提下 buckId 是怎么加到文件上的: 在 <code>DynamicPartitionDataWriter#newOutputWriter</code> 中:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 所以可以看到最多五位数的 bucketId</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bucketIdToString</span></span>(id: <span class="type">Int</span>): <span class="type">String</span> = <span class="string">f"_<span class="subst">$id</span>%05d"</span></span><br><span class="line"><span class="keyword">val</span> bucketIdStr = bucketId.map(<span class="type">BucketingUtils</span>.bucketIdToString).getOrElse(<span class="string">""</span>)</span><br></pre></td></tr></table></figure>
<p>提一句, 其实如果你是 repartition 了之后存下来的, <code>part-</code> 后的数字也就是 hash 之后的值, 这个 task attempt ID 是之前 execute task 时传入的 sparkPartitionId, 代码在 <code>FileFormatWriter#executeTask</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getFilename</span></span>(taskContext: <span class="type">TaskAttemptContext</span>, ext: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> split = taskContext.getTaskAttemptID.getTaskID.getId</span><br><span class="line">  <span class="string">f"part-<span class="subst">$split</span>%05d-<span class="subst">$jobId</span><span class="subst">$ext</span>"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="2-优化-Bucket-Pruning"><a href="#2-优化-Bucket-Pruning" class="headerlink" title="2. 优化: Bucket Pruning"></a>2. 优化: Bucket Pruning</h1><p>对于点查, 还是上面 bucketBy id 的例子, 例如 id=5, 可以把id=5当做原始表中一条记录, 同样的我们可以计算出它的 hash 值, 得到它的 bucket ID, 那么我们只要扫这个 bucket ID 文件就可以了, 因为其他 bucket ID 的文件里肯定没有5这个元素.</p>
<h2 id="2-1-Strategy-部分"><a href="#2-1-Strategy-部分" class="headerlink" title="2.1 Strategy 部分"></a>2.1 Strategy 部分</h2><p>先简单带过下 Spark read parquet 的流程, 想了解更多的可以参考这篇<a href="https://aaaaaaron.github.io/2018/11/01/Spark-Read-Deep-Dive/">Spark Read Deep Dive
</a>. </p>
<p>Spark 具体读取底层数据文件的 <code>SparkStrategy</code> 叫做 <code>FileSourceStrategy</code>, 在其 <code>apply</code> 方法中我们可以看到他需要一个 <code>LogicalRelation</code> 来触发到 apply 方法中的各个逻辑: <code>l @ LogicalRelation(fsRelation: HadoopFsRelation, _, table, _)) =&gt;</code>. </p>
<p><code>LogicalRelation</code> 中最重要的是<code>HadoopFsRelation</code>, 我们也可以构造自己的 <code>HadoopFsRelation</code> 传入, 从而生成 DataFrame, 这里不展开:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseRelationToDataFrame</span></span>(baseRelation: <span class="type">BaseRelation</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="type">Dataset</span>.ofRows(self, <span class="type">LogicalRelation</span>(baseRelation))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>提一句, Spark 的 <code>partitionBy</code> 的 pruning 通过 FileIndex, FileIndex 记录在 <code>HadoopFsRelation</code> 一路传到 <code>FileSourceScanExec</code>, <code>FileSourceScanExec</code> 里调用 <code>FileIndex#listFiles</code> pruning 文件, 但是 bucket 并不走 FileIndex 这一套, 事实上我觉得这两个逻辑是类似, 不知道为啥 Spark 不实现在一起, 有知道同学可以说下. </p>
<p>在 <code>FileSourceStrategy#apply</code> 中会对于可以进行 bucket pruning 的情况(<code>bucketColumnNames.length == 1 &amp;&amp; numBuckets &gt; 1</code>), 会传给<code>FileSourceScanExec</code>一个 <code>bucketSet</code>, 它是一个bitset, 通过这个 <code>bucketSet</code> 我们就能知道有哪些文件被选中了(010就代表第二个文件被选中了, 这个用法还是有点装逼). </p>
<p>下面来看看这个 <code>bucketSet</code> 是如何返回的, 代码如下:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getExpressionBuckets</span></span>(</span><br><span class="line">    expr: <span class="type">Expression</span>,</span><br><span class="line">    bucketColumnName: <span class="type">String</span>,</span><br><span class="line">    numBuckets: <span class="type">Int</span>): <span class="type">BitSet</span> = &#123;</span><br><span class="line">  ...</span><br><span class="line">  expr <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">Equality</span>(a: <span class="type">Attribute</span>, <span class="type">Literal</span>(v, _)) <span class="keyword">if</span> a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromValue(a, v)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">In</span>(a: <span class="type">Attribute</span>, list)</span><br><span class="line">      <span class="keyword">if</span> list.forall(_.isInstanceOf[<span class="type">Literal</span>]) &amp;&amp; a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromIterable(a, list.map(e =&gt; e.eval(<span class="type">EmptyRow</span>)))</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">InSet</span>(a: <span class="type">Attribute</span>, hset)</span><br><span class="line">      <span class="keyword">if</span> hset.forall(_.isInstanceOf[<span class="type">Literal</span>]) &amp;&amp; a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromIterable(a, hset.map(e =&gt; expressions.<span class="type">Literal</span>(e).eval(<span class="type">EmptyRow</span>)))</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">IsNull</span>(a: <span class="type">Attribute</span>) <span class="keyword">if</span> a.name == bucketColumnName =&gt;</span><br><span class="line">      getBucketSetFromValue(a, <span class="literal">null</span>)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">And</span>(left, right) =&gt;</span><br><span class="line">      getExpressionBuckets(left, bucketColumnName, numBuckets) &amp;</span><br><span class="line">        getExpressionBuckets(right, bucketColumnName, numBuckets)</span><br><span class="line">    <span class="keyword">case</span> expressions.<span class="type">Or</span>(left, right) =&gt;</span><br><span class="line">      getExpressionBuckets(left, bucketColumnName, numBuckets) |</span><br><span class="line">      getExpressionBuckets(right, bucketColumnName, numBuckets)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="keyword">val</span> matchedBuckets = <span class="keyword">new</span> <span class="type">BitSet</span>(numBuckets)</span><br><span class="line">      matchedBuckets.setUntil(numBuckets)</span><br><span class="line">      matchedBuckets</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>最主要的逻辑在 <code>FileSourceStrategy#getExpressionBuckets</code> 中, 可以看到 bucket 的 pruning 只支持 <code>Equality, In, InSet, IsNull</code> (And/Or 也支持, 只不过 And/Or 的 left/right 也必须是前面的类型), 其他情况是不支持 pruning 的, 直接返回所有 buckets.</p>
<p>我们来看 Equality 情况的处理, 调用了 <code>getBucketIdFromValue</code>, 里面逻辑是使用 HashPartitioning 求出 filter 里 literal 的 hash 值. 直接把这个 hash 值写入bitset 就是 最后返回的 <code>bucketSet</code> (这里还搞了 InternalRow/UnsafeProjection, 主要是为了处理各个类型的 value)</p>
<p>这里需要注意的一点是, 如果你的 filter 是 <code>cast(id as string)=&#39;1&#39;</code>, 或者等号右边的不是一个 lit, 是没法做 pruning 的.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Given bucketColumn, numBuckets and value, returns the corresponding bucketId</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBucketIdFromValue</span></span>(bucketColumn: <span class="type">Attribute</span>, numBuckets: <span class="type">Int</span>, value: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> mutableInternalRow = <span class="keyword">new</span> <span class="type">SpecificInternalRow</span>(<span class="type">Seq</span>(bucketColumn.dataType))</span><br><span class="line">  mutableInternalRow.update(<span class="number">0</span>, value)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> bucketIdGenerator = <span class="type">UnsafeProjection</span>.create(</span><br><span class="line">    <span class="type">HashPartitioning</span>(<span class="type">Seq</span>(bucketColumn), numBuckets).partitionIdExpression :: <span class="type">Nil</span>,</span><br><span class="line">    bucketColumn :: <span class="type">Nil</span>)</span><br><span class="line">  bucketIdGenerator(mutableInternalRow).getInt(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>PS. 之前我们仿照这个 bucket pruning 写了一个 file index 的时候, 还踩了一个坑, 当时觉得 <code>not</code> 也是支持的, 本来是选中那个文件, not 的话就取反一下, 变成 pruning 掉那个文件. 但是其实是有问题的, 事实上, 所有的 pruning, 只要不是精确到每个值都做 index 的, not 的情况都不能支持, 举例来说, a != 5, 但是 a 可能等于除5之外的任何值, 你把包含5的文件去掉了, 但是这个文件里除了5的记录, 还有其他值的记录.</p>
<h2 id="2-2-Exec-部分"><a href="#2-2-Exec-部分" class="headerlink" title="2.2 Exec 部分"></a>2.2 Exec 部分</h2><p>Spark 用 Strategy 来构造 Exec 的, <code>FileSourceStrategy</code> 用来构造 <code>FileSourceScanExec</code></p>
<p>在第一部分中, 我们传入了 <code>bucketSet</code> 给 <code>FileSourceScanExec</code>, 这个 bitset 告诉了我们要扫描哪些文件. </p>
<p><code>FileSourceScanExec</code> 通过 <code>inputRDD</code> 暴露出数据给上层的算子, 所以说 Spark SQL 用的也是 RDD. <code>inputRDD</code> 有两种逻辑, 如果是 bucketing 的话, 会调用 <code>FileSourceScanExec#createBucketedReadRDD</code>.</p>
<p><code>createBucketedReadRDD</code> 逻辑也很简单: </p>
<ol>
<li>找到每个 partitions 里的所有文件(如果没有用 partitionBy 机制, selectedPartitions 就只会有一个)</li>
<li>找到每个文件对应的 bucketID(文件名里记录着, 正则匹配), group by 这个 ID, 得到类似 Map[bucketID, Arrar[file]] 这样的结构</li>
<li>只取出 <code>bucketSet</code> 中记录了的 bucketID 对应的 files.</li>
</ol>
<p>到这里, 就完成了 bucket pruning 的逻辑.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> bucketedFileName = <span class="string">""</span><span class="string">".*_(\d+)(?:\..*)?$"</span><span class="string">""</span>.r</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBucketId</span></span>(fileName: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">Int</span>] = fileName <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> bucketedFileName(bucketId) =&gt; <span class="type">Some</span>(bucketId.toInt)</span><br><span class="line">  <span class="keyword">case</span> other =&gt; <span class="type">None</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The algorithm is pretty simple: each RDD partition being returned should include all the files</span></span><br><span class="line"><span class="comment"> * with the same bucket id from all the given Hive partitions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createBucketedReadRDD</span></span>(</span><br><span class="line">    bucketSpec: <span class="type">BucketSpec</span>,</span><br><span class="line">    readFile: (<span class="type">PartitionedFile</span>) =&gt; <span class="type">Iterator</span>[<span class="type">InternalRow</span>],</span><br><span class="line">    selectedPartitions: <span class="type">Seq</span>[<span class="type">PartitionDirectory</span>],</span><br><span class="line">    fsRelation: <span class="type">HadoopFsRelation</span>): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  logInfo(<span class="string">s"Planning with <span class="subst">$&#123;bucketSpec.numBuckets&#125;</span> buckets"</span>)</span><br><span class="line">  <span class="keyword">val</span> filesGroupedToBuckets =</span><br><span class="line">    selectedPartitions.flatMap &#123; p =&gt;</span><br><span class="line">      p.files.map &#123; f =&gt;</span><br><span class="line">        <span class="keyword">val</span> hosts = getBlockHosts(getBlockLocations(f), <span class="number">0</span>, f.getLen)</span><br><span class="line">        <span class="type">PartitionedFile</span>(p.values, f.getPath.toUri.toString, <span class="number">0</span>, f.getLen, hosts)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.groupBy &#123; f =&gt;</span><br><span class="line">      <span class="type">BucketingUtils</span></span><br><span class="line">        .getBucketId(<span class="keyword">new</span> <span class="type">Path</span>(f.filePath).getName)</span><br><span class="line">        .getOrElse(sys.error(<span class="string">s"Invalid bucket file <span class="subst">$&#123;f.filePath&#125;</span>"</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> prunedFilesGroupedToBuckets = <span class="keyword">if</span> (optionalBucketSet.isDefined) &#123;</span><br><span class="line">    <span class="keyword">val</span> bucketSet = optionalBucketSet.get</span><br><span class="line">    filesGroupedToBuckets.filter &#123;</span><br><span class="line">      f =&gt; bucketSet.get(f._1)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    filesGroupedToBuckets</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> filePartitions = <span class="type">Seq</span>.tabulate(bucketSpec.numBuckets) &#123; bucketId =&gt;</span><br><span class="line">    <span class="type">FilePartition</span>(bucketId, prunedFilesGroupedToBuckets.getOrElse(bucketId, <span class="type">Nil</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">FileScanRDD</span>(fsRelation.sparkSession, readFile, filePartitions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="3-优化-Bucket-Join"><a href="#3-优化-Bucket-Join" class="headerlink" title="3. 优化: Bucket Join"></a>3. 优化: Bucket Join</h1><h2 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h2><p>Bucket 还有一个很大的用处是可以用来做 Bucket Join, 这也是我觉得非常黑魔法的一个特性. 众所周知, SQL 的 join 是非常耗费时间的, Spark 为此也做了多种策略, 可以参考我之前的这篇博客 <a href="https://aaaaaaron.github.io/2019/06/29/Spark-SQL-Join-Deep-Dive/">Spark SQL Join Deep Dive</a>, 对于 大-大表进行 join, Spark 一般会选择 <code>SortMergeJoin</code> (SMJ), 因为SMJ 对比 HashJoin 来说, 不需要把一侧分片的数据都加载到内存中去, 提升了系统稳定性, 但是缺点是慢, 见下面两个例子:</p>
<h3 id="例子一"><a href="#例子一" class="headerlink" title="例子一"></a>例子一</h3><p>这个是最简单一个例子, 两个表直接 join, 无子查询.</p>
<h4 id="优化前"><a href="#优化前" class="headerlink" title="优化前"></a>优化前</h4><p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/企业微信截图_78b48f23-6c82-4db0-9237-ecfcadbe8248.png" alt></p>
<h4 id="优化后"><a href="#优化后" class="headerlink" title="优化后"></a>优化后</h4><p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191019131517.png" alt></p>
<p>可以见到, 由于是一个 SMJ, 而我们的源表又没有任何处理, 所以 Spark 自动给执行计划上加上了几个 exchange(shuffle) 和 sort, 对大数据有点了解的同学都知道, 这两步操作会十分的耗费时间与资源. 而要是启用了 Bucket Join 之后, 执行计划图会变成什么样呢? 可以见下图, 可以见之前的 exchange 和 sort 都没有了, 整体 query 时间从 40s+ 下降到 5s(图一到图三).</p>
<h3 id="例子二"><a href="#例子二" class="headerlink" title="例子二"></a>例子二</h3><p>我们来看更复杂的一个例子:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> test_kylin_fact t1</span><br><span class="line"><span class="keyword">JOIN</span></span><br><span class="line">  (<span class="keyword">SELECT</span> lstg_format_name</span><br><span class="line">   <span class="keyword">FROM</span> test_kylin_fact</span><br><span class="line">   <span class="keyword">GROUP</span> <span class="keyword">BY</span> lstg_format_name) t2 <span class="keyword">ON</span> t1.lstg_format_name = t2.lstg_format_name</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> t1.lstg_format_name</span><br></pre></td></tr></table></figure>
<h4 id="优化前-1"><a href="#优化前-1" class="headerlink" title="优化前"></a>优化前</h4><p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191019201545.png" alt></p>
<h4 id="优化后-1"><a href="#优化后-1" class="headerlink" title="优化后"></a>优化后</h4><p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/企业微信截图_2d709ebe-ee49-495a-8971-c8cf988cabf2.png" alt></p>
<p>有意思的是, 去掉最后一个 group by, 执行图会变成这样, 这个就留给读者朋友们自己去想了.</p>
<p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191019202151.png" alt></p>
<h3 id="3-2-原理说明"><a href="#3-2-原理说明" class="headerlink" title="3.2 原理说明"></a>3.2 原理说明</h3><p>我们先来看下 SMJ 的原理:</p>
<ol>
<li>为了让两条记录能连接到一起, 需要将具有相同 key 的记录分发到同一个分区, 这一步会导致 shuffle(Exchange).</li>
<li>分别对两个表中每个分区里的数据按照 key 进行 sort(SortExec), 然后后续做 merge sort 操作, 这样就可以不用像 HashJoin 需要把所有数据都拉到内存中.</li>
</ol>
<p>那么 Spark 是怎么知道是否需要添加这两步操作的呢? 假如我原始数据已经按 key 进行过了 sort, 那么是不是可以省下后面的 sort? 我们来看 <code>SortMergeJoinExec</code> 的两个方法:</p>
<ol>
<li><p><strong>requiredChildDistribution: Seq[Distribution]</strong>: <code>HashClusteredDistribution(leftKeys) :: HashClusteredDistribution(rightKeys) :: Nil</code>, SMJ 要求 Join 的两张表都是 HashClusteredDistribution 的</p>
</li>
<li><p><strong>requiredChildOrdering: Seq[SortOrder]</strong>: <code>leftKeys.map(SortOrder(_, Ascending)) :: rightKeys.map(SortOrder(_, Ascending)) :: Nil</code>, SMJ 要求 Join 的两张表都是排序的</p>
</li>
</ol>
<p>会在 <code>EnsureRequirements#ensureDistributionAndOrdering</code> 中判断一个节点的子节点是否符合上述的两个require:</p>
<h5 id="1-添加-exchange-节点"><a href="#1-添加-exchange-节点" class="headerlink" title="1. 添加 exchange 节点"></a>1. 添加 exchange 节点</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">children = children.zip(requiredChildDistributions).map &#123;</span><br><span class="line">  <span class="keyword">case</span> (child, distribution) <span class="keyword">if</span> child.outputPartitioning.satisfies(distribution) =&gt;</span><br><span class="line">    child</span><br><span class="line">  <span class="keyword">case</span> (child, <span class="type">BroadcastDistribution</span>(mode)) =&gt;</span><br><span class="line">    <span class="type">BroadcastExchangeExec</span>(mode, child)</span><br><span class="line">  <span class="keyword">case</span> (child, distribution) =&gt;</span><br><span class="line">    <span class="keyword">val</span> numPartitions = distribution.requiredNumPartitions</span><br><span class="line">      .getOrElse(defaultNumPreShufflePartitions)</span><br><span class="line">    <span class="type">ShuffleExchangeExec</span>(distribution.createPartitioning(numPartitions), child)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="2-添加-sort-节点"><a href="#2-添加-sort-节点" class="headerlink" title="2. 添加 sort 节点"></a>2. 添加 sort 节点</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">children = children.zip(requiredChildOrderings).map &#123; <span class="keyword">case</span> (child, requiredOrdering) =&gt;</span><br><span class="line">  <span class="comment">// If child.outputOrdering already satisfies the requiredOrdering, we do not need to sort.</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="type">SortOrder</span>.orderingSatisfies(child.outputOrdering, requiredOrdering)) &#123;</span><br><span class="line">    child</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">SortExec</span>(requiredOrdering, global = <span class="literal">false</span>, child = child)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="2-替换-children"><a href="#2-替换-children" class="headerlink" title="2. 替换 children"></a>2. 替换 children</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">operator.withNewChildren(children)</span><br></pre></td></tr></table></figure>
<p>那么为何 bucketing 可以省略掉上面这两步呢? 答案就是使用了 bucketing 机制, <code>FileSourceScanExec</code> 会暴露 SMJ 需要的 <code>Distribution</code> 和 <code>Ordering</code>, 代码见<code>FileSourceScanExec#val (outputPartitioning, outputOrdering): (Partitioning, Seq[SortOrder])</code>:</p>
<p>需要注意的一点是, 当用了 partitionBy, 或者就是你的 DF 有多个 partition 时, 会每个 partition 都有 bucketNum 个文件 (防止write 的时候数据倾斜), 所以一个 bucketID 可能会对应多个文件, 这些文件自己是有序的, 但是合在一起是无序的, 所以 ordering 是 Nil.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">lazy</span> <span class="keyword">val</span> (outputPartitioning, outputOrdering): (<span class="type">Partitioning</span>, <span class="type">Seq</span>[<span class="type">SortOrder</span>]) = &#123;</span><br><span class="line">  <span class="keyword">val</span> bucketSpec = <span class="keyword">if</span> (relation.sparkSession.sessionState.conf.bucketingEnabled) &#123;</span><br><span class="line">    relation.bucketSpec</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">None</span></span><br><span class="line">  &#125;</span><br><span class="line">  bucketSpec <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(spec) =&gt;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">toAttribute</span></span>(colName: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">Attribute</span>] = output.find(_.name == colName)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> bucketColumns = spec.bucketColumnNames.flatMap(n =&gt; toAttribute(n))</span><br><span class="line">      <span class="keyword">if</span> (bucketColumns.size == spec.bucketColumnNames.size) &#123;</span><br><span class="line">        <span class="keyword">val</span> partitioning = <span class="type">HashPartitioning</span>(bucketColumns, spec.numBuckets)</span><br><span class="line">        <span class="keyword">val</span> sortColumns =</span><br><span class="line">          spec.sortColumnNames.map(x =&gt; toAttribute(x)).takeWhile(x =&gt; x.isDefined).map(_.get)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sortOrder = <span class="keyword">if</span> (sortColumns.nonEmpty) &#123;</span><br><span class="line">          <span class="keyword">val</span> files = selectedPartitions.flatMap(partition =&gt; partition.files)</span><br><span class="line">          <span class="keyword">val</span> bucketToFilesGrouping =</span><br><span class="line">            files.map(_.getPath.getName).groupBy(file =&gt; <span class="type">BucketingUtils</span>.getBucketId(file))</span><br><span class="line">          <span class="keyword">val</span> singleFilePartitions = bucketToFilesGrouping.forall(p =&gt; p._2.length &lt;= <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (singleFilePartitions) &#123;</span><br><span class="line">            sortColumns.map(attribute =&gt; <span class="type">SortOrder</span>(attribute, <span class="type">Ascending</span>))</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">Nil</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="type">Nil</span></span><br><span class="line">        &#125;</span><br><span class="line">        (partitioning, sortOrder)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        (<span class="type">UnknownPartitioning</span>(<span class="number">0</span>), <span class="type">Nil</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      (<span class="type">UnknownPartitioning</span>(<span class="number">0</span>), <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="优化-SMJ-条件"><a href="#优化-SMJ-条件" class="headerlink" title="优化 SMJ 条件"></a>优化 SMJ 条件</h5><ol>
<li>Join 的列需要是 bucketBy/sortBy 的列, 且两边 bucketBy 的 num 要一样</li>
<li>如果有多个分区或者用了 partitionBy, Ordering 不能去除</li>
</ol>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol>
<li>DF 的每个 partition 都会有 bucket num 个文件, 比如 <code>df.repartition(5).write.format(&quot;parquet&quot;).option(&quot;path&quot;, &quot;/tmp/bueket&quot;).bucketBy(3, &quot;id&quot;)</code> 会产生15个文件, 由于这个限制, 我们使用 repartition 重新实现了类似 bucket 的功能.</li>
</ol>
<h1 id="写入-debug-截图"><a href="#写入-debug-截图" class="headerlink" title="写入 debug 截图"></a>写入 debug 截图</h1><p>TBD, 后面写个文章介绍</p>
<p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191018084225.png" alt></p>
<p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191018084345.png" alt></p>
</div></div><div class="post-main post-comment"></div></article><link rel="stylesheet" type="text/css" href="/css/font.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
  $(".fancybox").fancybox();
});
</script></body></html>