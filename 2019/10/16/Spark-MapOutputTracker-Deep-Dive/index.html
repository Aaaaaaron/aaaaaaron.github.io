<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=" id=&quot;基础&quot;&gt;&lt;a href=&quot;#基础&quot; class=&quot;headerlink&quot; title=&quot;基础&quot;&gt;&lt;/a&gt;基础&lt;/h2&gt;&lt;p&gt;Shuffle writer 会将中间数据保存到 Block 里面, 然后将数据的位置发送给 &lt;code&gt;MapOutputTracker&lt;/code&gt;; Shuffle reader 通过向 &lt;code&gt;MapOutputTracker&lt;/code&gt; 获取中间数据的位置之后, 才能读取到数据."><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/noise.css"><title>Spark MapOutputTracker Deep Dive | Jiatao Tao's blog</title><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><meta name="generator" content="Hexo 5.4.0"></head><body><article class="wrapper"><div class="post-main"><div class="nav"><nav class="container"><a class="sidebar-nav-item active" href="/">Home</a><a class="sidebar-nav-item" href="/archives">Archives</a><a class="sidebar-nav-item" href="/about">About</a></nav><div class="container post-meta"><div class="post-tags"><a class="post-tag-none-link" href="/tags/BigData/" rel="tag">BigData</a><a class="post-tag-none-link" href="/tags/Spark/" rel="tag">Spark</a></div><div class="post-time">2019-10-16</div></div></div><div class="container post-header"><h1>Spark MapOutputTracker Deep Dive</h1></div><div class="container post-toc"><details class="toc"><summary class="toc-accordion">Table of Contents</summary><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80"><span class="toc-number">1.</span> <span class="toc-text">基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">流程:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Write"><span class="toc-number">2.1.</span> <span class="toc-text">Write</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Read"><span class="toc-number">2.2.</span> <span class="toc-text">Read</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98"><span class="toc-number">3.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%8B%9F"><span class="toc-number">4.</span> <span class="toc-text">模拟</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3"><span class="toc-number">5.</span> <span class="toc-text">解决</span></a></li></ol></details></div><div class="container post-content"><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><p>Shuffle writer 会将中间数据保存到 Block 里面, 然后将数据的位置发送给 <code>MapOutputTracker</code>; Shuffle reader 通过向 <code>MapOutputTracker</code> 获取中间数据的位置之后, 才能读取到数据.</p>
<p><code>MapOutputTrackerMaster</code> 启动在 driver 端, <code>MapOutputTrackerWorker</code> 启动在 executor 端.</p>
<p>ShuffleStatus 就是 <code>Map[Int, Array[MapStatus]]</code>, key 是 Shuffle 的 ID, value 数组的大小是该 ShuffleMapTask 的个数, MapStatus 会记录 stage reduce 端 task 个数的 status, 具体实现有两种: <code>CompressedMapStatus</code>/<code>HighlyCompressedMapStatus</code>, 具体实现之后分析, 当 reduce 端 task 超过 2000 (<code>SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS</code>) 的时候, 会使用 <code>HighlyCompressedMapStatus</code>, 看名字可以看出后一种压缩率更高. 因为其实这个 <code>Map[Int, Array[MapStatus]]</code> 会非常占据内存, 试想下, 假如我有10w 个 map 端的 task 和10w 个 reduce 端的 task, 那么这个 <code>Array[MapStatus]</code> 实际存了 100亿 task 的信息, 而且后面这些 status 还要序列化发给 executor, 又会占用更多的空间, 同时 Spark 这里代码写的也不是非常好, 导致内存占用会很高. 而 driver 端的内存大家一般不会设置的特别高, 这里就会导致 OOM, 而 driver 又是 Spark 的单点, 这是一个非常严重的稳定性问题. 之后我会给出具体的例子和修复.</p>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程:"></a>流程:</h2><h3 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h3><ol>
<li><code>MapOutputTrackerMaster</code> 会 <code>registerShuffle</code> 和 <code>registerMapOutput</code>. registerShuffle 是 DAGScheduler 在创建一个 <code>ShuffleMapStage</code> 时会把这个 stage 对应的 shuffle 注册进来(<code>createShuffleMapStage</code>); <code>registerMapOutput</code> 是 在一个 <code>shuffleMapTask</code> 任务完成后(<code>DAGScheduler.handleTaskCompletion</code>)，会把 <code>shuffleMapTask</code> 输出的信息(<code>MapStatus</code>)放进来.</li>
</ol>
<h3 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h3><ol>
<li><p>当 shuffle read 的时候, <code>BlockStoreShuffleReader中</code>，会调用 <code>MapOutputTrackerWorker.getMapSizesByExecutorId</code> (master 端的这个方法只在 local 用)</p>
</li>
<li><p>调用 <code>MapOutputTrackerWorker#getStatuses(shuffleID)</code>, Worker 有个 mapStatuses 缓存 <code>Map[Int, Array[MapStatus]]</code>, 当 Miss 的时候, 会去 fetching, 就有两个很重要的方法:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fetchedBytes = askTracker[<span class="type">Array</span>[<span class="type">Byte</span>]](<span class="type">GetMapOutputStatuses</span>(shuffleId))</span><br><span class="line">fetchedStatuses = <span class="type">MapOutputTracker</span>.deserializeMapStatuses(fetchedBytes) <span class="comment">// 有两种模式 direct 的和 broadcast 的</span></span><br></pre></td></tr></table></figure>
<p>Worker askTracker 向 <code>MapOutputTrackerMasterEndpoint</code> 要 statues, 这个 endpoint 会向 MapOutputTrackerMaster post 一个 <code>GetMapOutputMessage(shufflID)</code> 事件(放入 <code>LinkedBlockingQueue[GetMapOutputMessage]</code>), 且 master 会启动一个 <code>MessageLoop</code>, 会 take 这个阻塞队列的事件, 从 master 自己内存中维护的 <code>shuffleStatuses</code> 找到对应 shuffleID 的 ShuffleStatus(<code>Map[Int, Array[MapStatus]]</code>), 在 Write 中提过, 当 <code>shuffleMapTask</code> 完成的时候, 会通知 <code>DAGScheduler.handleTaskCompletion</code>, 所以 driver 有所有的 <code>MapStatues</code>.</p>
</li>
<li><p>driver 拿到对应的 shuffleStatuses 之后, 需要把它 reply 回 请求的发起方, 也就是 executor, 这是最耗费内存的一步操作, 也是外面后期性能优化的点: <code>context.reply(shuffleStatus.serializedMapStatus(broadcastManager, isLocal, minSizeForBroadcast))</code>, 这个方法会调用 <code>MapOutputTracker.serializeMapStatuses</code>. 这个方法会使用 Java 的序列化机制(ObjectOutputStream)来序列化一个 <code>Array[MapStatus]</code> (对应一个 shuffle 的所有 MapStatus 输出), 并使用 gzip 压缩, 当序列化完之后, 会有有两种通知给 executor 的模式: 当序列化后的 byte 数组大小小于 minBroadcastSize(512K) 时, 会直接返回 Array[Byte], 后续使用 Spark 的 RPC 模式返回给 executor, 否则则用 Broadcast 机制返回给 executor.</p>
</li>
</ol>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol>
<li>每个 executor 拿的对应 shuffle 的 <code>Array[MapStatus]</code> 都是全量的. 这其实没有必要, 最好每个 executor 只拿自己 task 需要的 map statues 就可以了, 但是这个实现不容易</li>
<li>当 reduce 端 task 非常多的时候, 会使用 <code>HighlyCompressedMapStatus</code>, 这里面会用一个 RoaringBitmap 存 emptyBlocks, 但是其实当 reduce 特别多的时候, 存有 block 的反而更少 </li>
<li>序列化的时候, 使用 <code>ByteArrayOutputStream</code>, 且没有设置初始化大小, 导致一直在 grow, 不断的发生 array copy. 而且 <code>ByteArrayOutputStream</code> 比较坑爹, toByteArray 还会进行一次 array copy.</li>
<li>当满足一定条件会进行 broadcast, toByteArray又生成一个 array. 且要是进行 broadcast 的话, 上面的序列化就根本没有必要, 因为 broadcast 还会进行一次序列化.</li>
</ol>
<h2 id="模拟"><a href="#模拟" class="headerlink" title="模拟"></a>模拟</h2><p>测试数据集为 tpch 50, 使用 Spark SQL 测试, 测试查询为<code>select count(*) from lineitem group by l_comment </code>, 启动 20w Map 端 task, 5w Reduce 端 task, 4g driver memory.</p>
<p>发生 oom</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">    at java.util.Arrays.copyOf(Arrays.java:3236)</span><br><span class="line">    at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)</span><br><span class="line">    at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)</span><br><span class="line">    at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)</span><br><span class="line">    at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253)</span><br><span class="line">    at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211)</span><br><span class="line">    at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145)</span><br><span class="line">    at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894)</span><br><span class="line">    at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875)</span><br><span class="line">    at java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822)</span><br><span class="line">    at java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719)</span><br><span class="line">    at java.io.ObjectOutputStream.close(ObjectOutputStream.java:740)</span><br><span class="line">    at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$2.apply$mcV$sp(MapOutputTracker.scala:804)</span><br><span class="line">    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1369)</span><br><span class="line">    at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:803)</span><br><span class="line">    at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:174)</span><br><span class="line">    at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:397)</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">    at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>

<p>查看 dump:</p>
<p><img src="/2019/10/16/Spark-MapOutputTracker-Deep-Dive/20191016230210.png"></p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>社区已经 fix 了</p>
</div></div><div class="post-main post-comment"></div></article><link rel="stylesheet" type="text/css" href="/css/font.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
  $(".fancybox").fancybox();
});
</script></body></html>