<!DOCTYPE html>
<html lang="">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="Spark MapOutputTracker Deep Dive">




  <meta name="keywords" content="BigData,Spark,">





  <link rel="alternate" href="/default" title="Jiatao Tao's blog">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=1.1">



<link rel="canonical" href="https://aaaaaaron.github.io/2019/10/16/Spark-MapOutputTracker-Deep-Dive/">


<meta name="description" content="基础Shuffle writer 会将中间数据保存到 Block 里面, 然后将数据的位置发送给 MapOutputTracker; Shuffle reader 通过向 MapOutputTracker 获取中间数据的位置之后, 才能读取到数据. MapOutputTrackerMaster 启动在 driver 端, MapOutputTrackerWorker 启动在 executor 端.">
<meta name="keywords" content="BigData,Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark MapOutputTracker Deep Dive">
<meta property="og:url" content="https://aaaaaaron.github.io/2019/10/16/Spark-MapOutputTracker-Deep-Dive/index.html">
<meta property="og:site_name" content="Jiatao Tao&#39;s blog">
<meta property="og:description" content="基础Shuffle writer 会将中间数据保存到 Block 里面, 然后将数据的位置发送给 MapOutputTracker; Shuffle reader 通过向 MapOutputTracker 获取中间数据的位置之后, 才能读取到数据. MapOutputTrackerMaster 启动在 driver 端, MapOutputTrackerWorker 启动在 executor 端.">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191016230210.png">
<meta property="og:updated_time" content="2019-10-30T14:35:10.965Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark MapOutputTracker Deep Dive">
<meta name="twitter:description" content="基础Shuffle writer 会将中间数据保存到 Block 里面, 然后将数据的位置发送给 MapOutputTracker; Shuffle reader 通过向 MapOutputTracker 获取中间数据的位置之后, 才能读取到数据. MapOutputTrackerMaster 启动在 driver 端, MapOutputTrackerWorker 启动在 executor 端.">
<meta name="twitter:image" content="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191016230210.png">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  





  


    <title> Spark MapOutputTracker Deep Dive - Jiatao Tao's blog </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">Jiatao Tao's blog</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Spark MapOutputTracker Deep Dive
        
      </h1>

      <time class="post-time">
          Oct 16 2019
      </time>
    </header>



    
            <div class="post-content">
            <h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><p>Shuffle writer 会将中间数据保存到 Block 里面, 然后将数据的位置发送给 <code>MapOutputTracker</code>; Shuffle reader 通过向 <code>MapOutputTracker</code> 获取中间数据的位置之后, 才能读取到数据.</p>
<p><code>MapOutputTrackerMaster</code> 启动在 driver 端, <code>MapOutputTrackerWorker</code> 启动在 executor 端.</p>
<p>ShuffleStatus 就是 <code>Map[Int, Array[MapStatus]]</code>, key 是 Shuffle 的 ID, value 数组的大小是该 ShuffleMapTask 的个数, MapStatus 会记录 stage reduce 端 task 个数的 status, 具体实现有两种: <code>CompressedMapStatus</code>/<code>HighlyCompressedMapStatus</code>, 具体实现之后分析, 当 reduce 端 task 超过 2000 (<code>SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS</code>) 的时候, 会使用 <code>HighlyCompressedMapStatus</code>, 看名字可以看出后一种压缩率更高. 因为其实这个 <code>Map[Int, Array[MapStatus]]</code> 会非常占据内存, 试想下, 假如我有10w 个 map 端的 task 和10w 个 reduce 端的 task, 那么这个 <code>Array[MapStatus]</code> 实际存了 100亿 task 的信息, 而且后面这些 status 还要序列化发给 executor, 又会占用更多的空间, 同时 Spark 这里代码写的也不是非常好, 导致内存占用会很高. 而 driver 端的内存大家一般不会设置的特别高, 这里就会导致 OOM, 而 driver 又是 Spark 的单点, 这是一个非常严重的稳定性问题. 之后我会给出具体的例子和修复.</p>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程:"></a>流程:</h2><h3 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h3><ol>
<li><code>MapOutputTrackerMaster</code> 会 <code>registerShuffle</code> 和 <code>registerMapOutput</code>. registerShuffle 是 DAGScheduler 在创建一个 <code>ShuffleMapStage</code> 时会把这个 stage 对应的 shuffle 注册进来(<code>createShuffleMapStage</code>); <code>registerMapOutput</code> 是 在一个 <code>shuffleMapTask</code> 任务完成后(<code>DAGScheduler.handleTaskCompletion</code>)，会把 <code>shuffleMapTask</code> 输出的信息(<code>MapStatus</code>)放进来.</li>
</ol>
<h3 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h3><ol>
<li><p>当 shuffle read 的时候, <code>BlockStoreShuffleReader中</code>，会调用 <code>MapOutputTrackerWorker.getMapSizesByExecutorId</code> (master 端的这个方法只在 local 用)</p>
</li>
<li><p>调用 <code>MapOutputTrackerWorker#getStatuses(shuffleID)</code>, Worker 有个 mapStatuses 缓存 <code>Map[Int, Array[MapStatus]]</code>, 当 Miss 的时候, 会去 fetching, 就有两个很重要的方法:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fetchedBytes = askTracker[<span class="type">Array</span>[<span class="type">Byte</span>]](<span class="type">GetMapOutputStatuses</span>(shuffleId))</span><br><span class="line">fetchedStatuses = <span class="type">MapOutputTracker</span>.deserializeMapStatuses(fetchedBytes) <span class="comment">// 有两种模式 direct 的和 broadcast 的</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>Worker askTracker 向 <code>MapOutputTrackerMasterEndpoint</code> 要 statues, 这个 endpoint 会向 MapOutputTrackerMaster post 一个 <code>GetMapOutputMessage(shufflID)</code> 事件(放入 <code>LinkedBlockingQueue[GetMapOutputMessage]</code>), 且 master 会启动一个 <code>MessageLoop</code>, 会 take 这个阻塞队列的事件, 从 master 自己内存中维护的 <code>shuffleStatuses</code> 找到对应 shuffleID 的 ShuffleStatus(<code>Map[Int, Array[MapStatus]]</code>), 在 Write 中提过, 当 <code>shuffleMapTask</code> 完成的时候, 会通知 <code>DAGScheduler.handleTaskCompletion</code>, 所以 driver 有所有的 <code>MapStatues</code>.</p>
<ol start="3">
<li>driver 拿到对应的 shuffleStatuses 之后, 需要把它 reply 回 请求的发起方, 也就是 executor, 这是最耗费内存的一步操作, 也是外面后期性能优化的点: <code>context.reply(shuffleStatus.serializedMapStatus(broadcastManager, isLocal, minSizeForBroadcast))</code>, 这个方法会调用 <code>MapOutputTracker.serializeMapStatuses</code>. 这个方法会使用 Java 的序列化机制(ObjectOutputStream)来序列化一个 <code>Array[MapStatus]</code> (对应一个 shuffle 的所有 MapStatus 输出), 并使用 gzip 压缩, 当序列化完之后, 会有有两种通知给 executor 的模式: 当序列化后的 byte 数组大小小于 minBroadcastSize(512K) 时, 会直接返回 Array[Byte], 后续使用 Spark 的 RPC 模式返回给 executor, 否则则用 Broadcast 机制返回给 executor.</li>
</ol>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol>
<li>每个 executor 拿的对应 shuffle 的 <code>Array[MapStatus]</code> 都是全量的. 这其实没有必要, 最好每个 executor 只拿自己 task 需要的 map statues 就可以了, 但是这个实现不容易</li>
<li>当 reduce 端 task 非常多的时候, 会使用 <code>HighlyCompressedMapStatus</code>, 这里面会用一个 RoaringBitmap 存 emptyBlocks, 但是其实当 reduce 特别多的时候, 存有 block 的反而更少 </li>
<li>序列化的时候, 使用 <code>ByteArrayOutputStream</code>, 且没有设置初始化大小, 导致一直在 grow, 不断的发生 array copy. 而且 <code>ByteArrayOutputStream</code> 比较坑爹, toByteArray 还会进行一次 array copy.</li>
<li>当满足一定条件会进行 broadcast, toByteArray又生成一个 array. 且要是进行 broadcast 的话, 上面的序列化就根本没有必要, 因为 broadcast 还会进行一次序列化.</li>
</ol>
<h2 id="模拟"><a href="#模拟" class="headerlink" title="模拟"></a>模拟</h2><p>测试数据集为 tpch 50, 使用 Spark SQL 测试, 测试查询为<code>select count(*) from lineitem group by l_comment</code>, 启动 20w Map 端 task, 5w Reduce 端 task, 4g driver memory.</p>
<p>发生 oom<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">    at java.util.Arrays.copyOf(Arrays.java:3236)</span><br><span class="line">    at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)</span><br><span class="line">    at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)</span><br><span class="line">    at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)</span><br><span class="line">    at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253)</span><br><span class="line">    at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211)</span><br><span class="line">    at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145)</span><br><span class="line">    at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894)</span><br><span class="line">    at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875)</span><br><span class="line">    at java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822)</span><br><span class="line">    at java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719)</span><br><span class="line">    at java.io.ObjectOutputStream.close(ObjectOutputStream.java:740)</span><br><span class="line">    at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$2.apply$mcV$sp(MapOutputTracker.scala:804)</span><br><span class="line">    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1369)</span><br><span class="line">    at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:803)</span><br><span class="line">    at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:174)</span><br><span class="line">    at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:397)</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">    at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure></p>
<p>查看 dump:</p>
<p><img src="https://aron-blog-1257818292.cos.ap-shanghai.myqcloud.com/20191016230210.png" alt=""></p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>TODO</p>

            </div>
          

    
      <footer class="post-footer">
        <div class="post-tags">
          
            <a href="/tags/BigData/">BigData</a>
          
            <a href="/tags/Spark/">Spark</a>
          
        </div>

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2019/11/02/大数据处理迷思/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">大数据处理迷思</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2019/10/09/Callback与-Coroutine-协程概念说明/">
        <span class="next-text nav-default">Callback 与 Coroutine 协程概念说明</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2012 -
    
    2019
    <span class="footer-author">陶加涛.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/henryhuang/hexo-theme-polarbearsimple">Polar Bear Simple</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
